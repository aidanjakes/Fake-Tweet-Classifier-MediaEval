{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7446f328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263046056240115712</td>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s d...</td>\n",
       "      <td>21226711</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>iAnnieM</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>262995061304852481</td>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda i...</td>\n",
       "      <td>192378571</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>CarlosVerareal</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262979898002534400</td>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a...</td>\n",
       "      <td>132303095</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>LucasPalape</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262996108400271360</td>\n",
       "      <td>Scary shit #hurricane #NY http://t.co/e4JLBUfH</td>\n",
       "      <td>241995902</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Haaaaarryyy</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263018881839411200</td>\n",
       "      <td>My fave place in the world #nyc #hurricane #sa...</td>\n",
       "      <td>250315890</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>princess__natt</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>263364439582060545</td>\n",
       "      <td>42nd #time #square #NYC #subway #hurricane htt...</td>\n",
       "      <td>163674788</td>\n",
       "      <td>sandyA_fake_23</td>\n",
       "      <td>classycg</td>\n",
       "      <td>Tue Oct 30 19:39:10 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>262927032705490944</td>\n",
       "      <td>Just in time for #halloween a photo of #hurric...</td>\n",
       "      <td>246153081</td>\n",
       "      <td>sandyA_fake_14</td>\n",
       "      <td>j_unit87</td>\n",
       "      <td>Mon Oct 29 14:41:04 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>263321078884077568</td>\n",
       "      <td>Crazy pic of #Hurricane #Sandy prayers go out ...</td>\n",
       "      <td>199565482</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>MrBlakMagik</td>\n",
       "      <td>Tue Oct 30 16:46:52 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>263111677485142017</td>\n",
       "      <td>#sandy #newyork #hurricane #statueofliberty #U...</td>\n",
       "      <td>78475739</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>safi37</td>\n",
       "      <td>Tue Oct 30 02:54:46 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>262977091983785985</td>\n",
       "      <td>#nyc #hurricane http://t.co/Gv3QxZlq</td>\n",
       "      <td>869777653</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>kingmichael03</td>\n",
       "      <td>Mon Oct 29 17:59:59 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId                                          tweetText  \\\n",
       "0  263046056240115712  ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s d...   \n",
       "1  262995061304852481  @milenagimon: Miren a Sandy en NY!  Tremenda i...   \n",
       "2  262979898002534400  Buena la foto del Hurac√°n Sandy, me recuerda a...   \n",
       "3  262996108400271360     Scary shit #hurricane #NY http://t.co/e4JLBUfH   \n",
       "4  263018881839411200  My fave place in the world #nyc #hurricane #sa...   \n",
       "5  263364439582060545  42nd #time #square #NYC #subway #hurricane htt...   \n",
       "6  262927032705490944  Just in time for #halloween a photo of #hurric...   \n",
       "7  263321078884077568  Crazy pic of #Hurricane #Sandy prayers go out ...   \n",
       "8  263111677485142017  #sandy #newyork #hurricane #statueofliberty #U...   \n",
       "9  262977091983785985               #nyc #hurricane http://t.co/Gv3QxZlq   \n",
       "\n",
       "      userId      imageId(s)        username                       timestamp  \\\n",
       "0   21226711  sandyA_fake_46         iAnnieM  Mon Oct 29 22:34:01 +0000 2012   \n",
       "1  192378571  sandyA_fake_09  CarlosVerareal  Mon Oct 29 19:11:23 +0000 2012   \n",
       "2  132303095  sandyA_fake_09     LucasPalape  Mon Oct 29 18:11:08 +0000 2012   \n",
       "3  241995902  sandyA_fake_29     Haaaaarryyy  Mon Oct 29 19:15:33 +0000 2012   \n",
       "4  250315890  sandyA_fake_15  princess__natt  Mon Oct 29 20:46:02 +0000 2012   \n",
       "5  163674788  sandyA_fake_23        classycg  Tue Oct 30 19:39:10 +0000 2012   \n",
       "6  246153081  sandyA_fake_14        j_unit87  Mon Oct 29 14:41:04 +0000 2012   \n",
       "7  199565482  sandyA_fake_29     MrBlakMagik  Tue Oct 30 16:46:52 +0000 2012   \n",
       "8   78475739  sandyA_fake_15          safi37  Tue Oct 30 02:54:46 +0000 2012   \n",
       "9  869777653  sandyA_fake_29   kingmichael03  Mon Oct 29 17:59:59 +0000 2012   \n",
       "\n",
       "  label  \n",
       "0  fake  \n",
       "1  fake  \n",
       "2  fake  \n",
       "3  fake  \n",
       "4  fake  \n",
       "5  fake  \n",
       "6  fake  \n",
       "7  fake  \n",
       "8  fake  \n",
       "9  fake  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from deep_translator import GoogleTranslator\n",
    "from autocorrect import Speller\n",
    "import re\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "#from textblob import TextBlob\n",
    "#from urlextract import URLExtract\n",
    "from langdetect import detect\n",
    "\n",
    "train_data = pd.read_csv('mediaeval-2015-trainingset.txt', sep=\"\\t\", header=0)\n",
    "test_data = pd.read_csv('mediaeval-2015-testset.txt', sep=\"\\t\", header=0)\n",
    "\n",
    "\n",
    "\n",
    "#exploring dataset\n",
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77cdd083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14277 entries, 0 to 14276\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tweetId     14277 non-null  int64 \n",
      " 1   tweetText   14277 non-null  object\n",
      " 2   userId      14277 non-null  int64 \n",
      " 3   imageId(s)  14277 non-null  object\n",
      " 4   username    14277 non-null  object\n",
      " 5   timestamp   14277 non-null  object\n",
      " 6   label       14277 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 780.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aabdb80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15c48b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real count:  4921\n",
      "fake count:  6742\n",
      "humor count:  2614\n",
      "real count:  1209\n",
      "fake count:  2546\n",
      "humor count:  0\n"
     ]
    }
   ],
   "source": [
    "#count number of real values\n",
    "num_fake = (train_data.label=='fake').sum()\n",
    "num_humor = (train_data.label=='humor').sum()\n",
    "num_real = (train_data.label=='real').sum()\n",
    "print('real count: ',num_real)\n",
    "print('fake count: ',num_fake)\n",
    "print('humor count: ',num_humor)\n",
    "\n",
    "num_fake = (test_data.label=='fake').sum()\n",
    "num_humor = (test_data.label=='humor').sum()\n",
    "num_real = (test_data.label=='real').sum()\n",
    "print('real count: ',num_real)\n",
    "print('fake count: ',num_fake)\n",
    "print('humor count: ',num_humor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3044941f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42: 1\n",
      "36: 2\n",
      "33: 1\n",
      "32: 1\n",
      "29: 1\n",
      "23: 1\n",
      "19: 1\n",
      "17: 3\n",
      "16: 1\n",
      "15: 3\n",
      "14: 4\n",
      "13: 5\n",
      "12: 7\n",
      "11: 7\n",
      "10: 8\n",
      "9: 2\n",
      "8: 7\n",
      "7: 9\n",
      "6: 13\n",
      "5: 28\n",
      "4: 36\n",
      "3: 96\n",
      "2: 627\n",
      "1: 11512\n"
     ]
    }
   ],
   "source": [
    "val_counts = train_data['tweetText'].value_counts().values\n",
    "# importing the collections module\n",
    "import collections\n",
    "# getting the elements frequencies using Counter class\n",
    "elements_count = collections.Counter(val_counts)\n",
    "# printing the element and the frequency\n",
    "for key, value in elements_count.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "#print total number of duplicated tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fd137f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                 14277\n",
       "unique                                                12376\n",
       "top       Unbelievable scene flying over #StatenIsland i...\n",
       "freq                                                     42\n",
       "Name: tweetText, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['tweetText'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2119243",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count           14277\n",
       "unique          13498\n",
       "top       SAGandAFTRA\n",
       "freq               16\n",
       "Name: username, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['username'].describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2730a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16: 1\n",
      "10: 1\n",
      "9: 1\n",
      "8: 2\n",
      "7: 2\n",
      "6: 3\n",
      "5: 11\n",
      "4: 29\n",
      "3: 74\n",
      "2: 427\n",
      "1: 12947\n",
      "551  users responsible for  1330\n"
     ]
    }
   ],
   "source": [
    "val_counts = train_data['username'].value_counts().values\n",
    "# importing the collections module\n",
    "import collections\n",
    "# getting the elements frequencies using Counter class\n",
    "elements_count = collections.Counter(val_counts)\n",
    "# printing the element and the frequency\n",
    "num_users = 0\n",
    "num_tweets = 0\n",
    "#key = number of tweets, value = num users\n",
    "for key, value in elements_count.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "    if(key > 1):\n",
    "        num_tweets += key * value\n",
    "        num_users += value\n",
    "    \n",
    "print(num_users, ' users responsible for ',num_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6696d10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of different languages\n",
    "def lang_detect(text):\n",
    "    try:\n",
    "        detected_text = detect(text)\n",
    "        return detected_text\n",
    "    except:\n",
    "        return \"error\"\n",
    "\n",
    "#train_data['language'] = train_data['tweetText'].apply(lambda x: lang_detect(x))\n",
    "#train_data['language'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54ff30a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Mon Oct 29 22:34:01 +0000 2012\n",
    "#'Tue Mar 11 03: 13: 40 +0000 2014'\n",
    "def get_date_string(string):\n",
    "    return string.split()[5] + '-' + (mon_to_num(string.split()[1]) + '-' + string.split()[2]) \n",
    "#fake_tweets = train_data.loc[train_data['label'] == ('fake' or 'humor')]\n",
    "def mon_to_num(mon):\n",
    "    if mon == 'Jan':\n",
    "        return '01'\n",
    "    elif mon == 'Feb':\n",
    "        return '02'\n",
    "    elif mon == 'Mar':\n",
    "        return '03'\n",
    "    elif mon == 'Apr':\n",
    "        return '04'\n",
    "    elif mon == 'May':\n",
    "        return '05'\n",
    "    elif mon == 'Jun':\n",
    "        return '06'\n",
    "    elif mon == 'Jul':\n",
    "        return '07'\n",
    "    elif mon == 'Aug':\n",
    "        return '08'\n",
    "    elif mon == 'Sep':\n",
    "        return '09'\n",
    "    elif mon == 'Oct':\n",
    "        return '10'\n",
    "    elif mon == 'Nov':\n",
    "        return '11'\n",
    "    elif mon == 'Dec':\n",
    "        return '12'\n",
    "    \n",
    "#time stamp will be converted to fields weekday, month, year, time, date, datetime\n",
    "train_data['weekday'] = train_data['timestamp'].apply(lambda x: x.split()[0])\n",
    "train_data['date']= train_data['timestamp'].apply(lambda x: get_date_string(x))\n",
    "    \n",
    "train_data['weekday'].value_counts().plot.bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9dd6e222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2012-10-30    4163\n",
       "2012-10-29    3348\n",
       "2012-11-01    1577\n",
       "2012-11-04     875\n",
       "2012-11-02     869\n",
       "2012-11-03     693\n",
       "2012-10-31     609\n",
       "2013-04-19     298\n",
       "2014-09-11     185\n",
       "2014-02-07     136\n",
       "2012-11-05     132\n",
       "2013-04-17     108\n",
       "2014-03-18      80\n",
       "2013-04-18      77\n",
       "2014-03-10      74\n",
       "2014-02-08      64\n",
       "2014-03-19      58\n",
       "2014-02-09      56\n",
       "2013-04-20      56\n",
       "2014-03-09      55\n",
       "2012-10-28      47\n",
       "2014-03-11      45\n",
       "2014-02-10      38\n",
       "2014-02-06      34\n",
       "2014-06-02      31\n",
       "2014-03-20      23\n",
       "2014-03-24      21\n",
       "2014-02-11      19\n",
       "2014-03-23      18\n",
       "2014-03-14      17\n",
       "2014-03-12      17\n",
       "2014-05-01      16\n",
       "2014-04-16      16\n",
       "2014-03-27      15\n",
       "2014-03-08      15\n",
       "2014-05-02      15\n",
       "2014-02-12      14\n",
       "2014-05-08      14\n",
       "2014-04-17      14\n",
       "2014-04-05      13\n",
       "2014-05-09      12\n",
       "2014-03-13      12\n",
       "2014-05-19      12\n",
       "2014-04-07      12\n",
       "2014-02-05      12\n",
       "2014-04-19      11\n",
       "2014-03-22      11\n",
       "2014-04-18      11\n",
       "2014-03-17      10\n",
       "2014-05-03      10\n",
       "2014-05-06       9\n",
       "2014-04-13       9\n",
       "2014-04-14       8\n",
       "2014-03-21       8\n",
       "2014-03-28       7\n",
       "2014-02-04       7\n",
       "2014-03-25       7\n",
       "2014-04-08       7\n",
       "2014-03-15       6\n",
       "2014-05-10       6\n",
       "2014-05-07       6\n",
       "2014-01-23       6\n",
       "2014-06-03       5\n",
       "2014-04-15       5\n",
       "2014-05-05       5\n",
       "2014-01-21       4\n",
       "2014-06-01       4\n",
       "2014-05-11       4\n",
       "2014-05-04       4\n",
       "2014-05-20       4\n",
       "2014-05-13       3\n",
       "2014-03-16       3\n",
       "2014-02-13       3\n",
       "2014-04-12       3\n",
       "2014-03-26       3\n",
       "2012-10-25       3\n",
       "2014-03-30       3\n",
       "2014-03-31       3\n",
       "2014-05-31       3\n",
       "2013-04-16       3\n",
       "2014-04-10       3\n",
       "2014-01-20       2\n",
       "2012-10-26       2\n",
       "05-03-13         2\n",
       "2014-06-04       2\n",
       "2014-01-28       2\n",
       "2014-06-07       2\n",
       "40-03-11         2\n",
       "2014-04-11       2\n",
       "22-03-11         2\n",
       "2014-05-12       2\n",
       "2013-04-22       2\n",
       "2013-04-23       1\n",
       "2014-05-26       1\n",
       "50-03-10         1\n",
       "36-03-17         1\n",
       "2014-01-25       1\n",
       "44-03-13         1\n",
       "13-03-10         1\n",
       "41-03-11         1\n",
       "2014-05-25       1\n",
       "2013-04-24       1\n",
       "2014-02-02       1\n",
       "2014-05-21       1\n",
       "2014-01-22       1\n",
       "2014-02-03       1\n",
       "2014-04-06       1\n",
       "38-03-10         1\n",
       "26-03-10         1\n",
       "09-03-09         1\n",
       "02-03-10         1\n",
       "2014-05-16       1\n",
       "2014-05-18       1\n",
       "2014-01-24       1\n",
       "2014-06-08       1\n",
       "45-03-10         1\n",
       "40-03-16         1\n",
       "2014-05-28       1\n",
       "06-03-14         1\n",
       "2014-06-09       1\n",
       "10-03-09         1\n",
       "36-03-11         1\n",
       "23-03-10         1\n",
       "26-03-17         1\n",
       "51-03-11         1\n",
       "Name: date, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_data['date'].describe()\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "train_data['date'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e51acf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14277 entries, 0 to 14276\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tweetId     14277 non-null  int64 \n",
      " 1   tweetText   14277 non-null  object\n",
      " 2   userId      14277 non-null  int64 \n",
      " 3   imageId(s)  14277 non-null  object\n",
      " 4   username    14277 non-null  object\n",
      " 5   timestamp   14277 non-null  object\n",
      " 6   label       14277 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 780.9+ KB\n"
     ]
    }
   ],
   "source": [
    "#remove weekday and date columns, only used to visualise the data\n",
    "train_data.drop('weekday', axis=1, inplace=True)\n",
    "train_data.drop('date', axis=1, inplace=True)\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a8407e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263046056240115712</td>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy.</td>\n",
       "      <td>21226711</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>iAnnieM</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>262995061304852481</td>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\"  REAL! RT.</td>\n",
       "      <td>192378571</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>CarlosVerareal</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262979898002534400</td>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy</td>\n",
       "      <td>132303095</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>LucasPalape</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262996108400271360</td>\n",
       "      <td>Scary shit #hurricane #NY</td>\n",
       "      <td>241995902</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Haaaaarryyy</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263018881839411200</td>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ</td>\n",
       "      <td>250315890</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>princess__natt</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>263364439582060545</td>\n",
       "      <td>42nd #time #square #NYC #subway #hurricane</td>\n",
       "      <td>163674788</td>\n",
       "      <td>sandyA_fake_23</td>\n",
       "      <td>classycg</td>\n",
       "      <td>Tue Oct 30 19:39:10 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>262927032705490944</td>\n",
       "      <td>Just in time for #halloween a photo of #hurricane #sandy #frankenstorm</td>\n",
       "      <td>246153081</td>\n",
       "      <td>sandyA_fake_14</td>\n",
       "      <td>j_unit87</td>\n",
       "      <td>Mon Oct 29 14:41:04 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>263321078884077568</td>\n",
       "      <td>Crazy pic of #Hurricane #Sandy prayers go out to family and friends on the East Coast</td>\n",
       "      <td>199565482</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>MrBlakMagik</td>\n",
       "      <td>Tue Oct 30 16:46:52 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>263111677485142017</td>\n",
       "      <td>#sandy #newyork #hurricane #statueofliberty #USA</td>\n",
       "      <td>78475739</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>safi37</td>\n",
       "      <td>Tue Oct 30 02:54:46 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>262977091983785985</td>\n",
       "      <td>#nyc #hurricane</td>\n",
       "      <td>869777653</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>kingmichael03</td>\n",
       "      <td>Mon Oct 29 17:59:59 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>262989009930833920</td>\n",
       "      <td>robertosalibaba  god be with u brother #sandy #hurricane #newyork</td>\n",
       "      <td>359592461</td>\n",
       "      <td>sandyA_fake_08</td>\n",
       "      <td>Michael_Saliba</td>\n",
       "      <td>Mon Oct 29 18:47:20 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>263129115207536640</td>\n",
       "      <td>#Crazy #Hurricane #Sandy</td>\n",
       "      <td>31305940</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>SLAZARO31</td>\n",
       "      <td>Tue Oct 30 04:04:04 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>263091320871063552</td>\n",
       "      <td>#shark #newjersey #swim #sandy #hurricane ÓêêÓÑáÓêæ</td>\n",
       "      <td>51599800</td>\n",
       "      <td>sandyA_fake_11</td>\n",
       "      <td>anaceciggt</td>\n",
       "      <td>Tue Oct 30 01:33:53 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>262990978611286016</td>\n",
       "      <td>Good luck #ny #newyork #usa #hurricane #sandy</td>\n",
       "      <td>125724906</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>gsevigny</td>\n",
       "      <td>Mon Oct 29 18:55:10 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>263070862977167360</td>\n",
       "      <td>Wow.... Fishing anyone? #hurricane #sandy</td>\n",
       "      <td>332477807</td>\n",
       "      <td>sandyA_fake_11</td>\n",
       "      <td>shawtyvegasbomb</td>\n",
       "      <td>Tue Oct 30 00:12:36 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>263270378221228033</td>\n",
       "      <td>Well #howdy there #hurricane #sandy . Just wanted to let you know that you took my power, internet, &amp;amp;&amp;amp; happi</td>\n",
       "      <td>714666746</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>x0_nashalee_0x</td>\n",
       "      <td>Tue Oct 30 13:25:24 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>263229641999925249</td>\n",
       "      <td>Just known this bcs of #jason #chen updated the pic! Everyone be safe! #newyork #sandy #hurricane #nature #e</td>\n",
       "      <td>31382827</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>aran08</td>\n",
       "      <td>Tue Oct 30 10:43:31 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>263184498072645632</td>\n",
       "      <td>My thoughts and prayers go to all of the people going thru #hurricane #sandy #ct #de #ma #md #me #nc #nh #nj</td>\n",
       "      <td>164399110</td>\n",
       "      <td>sandyA_fake_48</td>\n",
       "      <td>RaulSanchezBeat</td>\n",
       "      <td>Tue Oct 30 07:44:08 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>263001267926880258</td>\n",
       "      <td>Stay safe my New York family...#nyc #newyork #storm #hurricane #wind</td>\n",
       "      <td>332854270</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>RSG_the_Large_1</td>\n",
       "      <td>Mon Oct 29 19:36:03 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>263363592508829697</td>\n",
       "      <td>New York #hurricane #Sandy</td>\n",
       "      <td>30897781</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>tito23</td>\n",
       "      <td>Tue Oct 30 19:35:48 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweetId  \\\n",
       "0   263046056240115712   \n",
       "1   262995061304852481   \n",
       "2   262979898002534400   \n",
       "3   262996108400271360   \n",
       "4   263018881839411200   \n",
       "5   263364439582060545   \n",
       "6   262927032705490944   \n",
       "7   263321078884077568   \n",
       "8   263111677485142017   \n",
       "9   262977091983785985   \n",
       "10  262989009930833920   \n",
       "11  263129115207536640   \n",
       "12  263091320871063552   \n",
       "13  262990978611286016   \n",
       "14  263070862977167360   \n",
       "15  263270378221228033   \n",
       "16  263229641999925249   \n",
       "17  263184498072645632   \n",
       "18  263001267926880258   \n",
       "19  263363592508829697   \n",
       "\n",
       "                                                                                                                tweetText  \\\n",
       "0      ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy.    \n",
       "1       @milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\"  REAL! RT.   \n",
       "2                        Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy     \n",
       "3                                                                                              Scary shit #hurricane #NY    \n",
       "4                                                   My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ    \n",
       "5                                                                             42nd #time #square #NYC #subway #hurricane    \n",
       "6                                                 Just in time for #halloween a photo of #hurricane #sandy #frankenstorm    \n",
       "7                                  Crazy pic of #Hurricane #Sandy prayers go out to family and friends on the East Coast    \n",
       "8                                                                       #sandy #newyork #hurricane #statueofliberty #USA    \n",
       "9                                                                                                        #nyc #hurricane    \n",
       "10                                                     robertosalibaba  god be with u brother #sandy #hurricane #newyork    \n",
       "11                                                                                              #Crazy #Hurricane #Sandy    \n",
       "12                                                                         #shark #newjersey #swim #sandy #hurricane ÓêêÓÑáÓêæ    \n",
       "13                                                                         Good luck #ny #newyork #usa #hurricane #sandy    \n",
       "14                                                                             Wow.... Fishing anyone? #hurricane #sandy    \n",
       "15  Well #howdy there #hurricane #sandy . Just wanted to let you know that you took my power, internet, &amp;&amp; happi    \n",
       "16          Just known this bcs of #jason #chen updated the pic! Everyone be safe! #newyork #sandy #hurricane #nature #e    \n",
       "17          My thoughts and prayers go to all of the people going thru #hurricane #sandy #ct #de #ma #md #me #nc #nh #nj    \n",
       "18                                                  Stay safe my New York family...#nyc #newyork #storm #hurricane #wind    \n",
       "19                                                                                            New York #hurricane #Sandy    \n",
       "\n",
       "       userId      imageId(s)         username  \\\n",
       "0    21226711  sandyA_fake_46          iAnnieM   \n",
       "1   192378571  sandyA_fake_09   CarlosVerareal   \n",
       "2   132303095  sandyA_fake_09      LucasPalape   \n",
       "3   241995902  sandyA_fake_29      Haaaaarryyy   \n",
       "4   250315890  sandyA_fake_15   princess__natt   \n",
       "5   163674788  sandyA_fake_23         classycg   \n",
       "6   246153081  sandyA_fake_14         j_unit87   \n",
       "7   199565482  sandyA_fake_29      MrBlakMagik   \n",
       "8    78475739  sandyA_fake_15           safi37   \n",
       "9   869777653  sandyA_fake_29    kingmichael03   \n",
       "10  359592461  sandyA_fake_08   Michael_Saliba   \n",
       "11   31305940  sandyA_fake_29        SLAZARO31   \n",
       "12   51599800  sandyA_fake_11       anaceciggt   \n",
       "13  125724906  sandyA_fake_29         gsevigny   \n",
       "14  332477807  sandyA_fake_11  shawtyvegasbomb   \n",
       "15  714666746  sandyA_fake_29   x0_nashalee_0x   \n",
       "16   31382827  sandyA_fake_29           aran08   \n",
       "17  164399110  sandyA_fake_48  RaulSanchezBeat   \n",
       "18  332854270  sandyA_fake_29  RSG_the_Large_1   \n",
       "19   30897781  sandyA_fake_15           tito23   \n",
       "\n",
       "                         timestamp label  length  \n",
       "0   Mon Oct 29 22:34:01 +0000 2012  fake     114  \n",
       "1   Mon Oct 29 19:11:23 +0000 2012  fake     113  \n",
       "2   Mon Oct 29 18:11:08 +0000 2012  fake      96  \n",
       "3   Mon Oct 29 19:15:33 +0000 2012  fake      26  \n",
       "4   Mon Oct 29 20:46:02 +0000 2012  fake      69  \n",
       "5   Tue Oct 30 19:39:10 +0000 2012  fake      43  \n",
       "6   Mon Oct 29 14:41:04 +0000 2012  fake      71  \n",
       "7   Tue Oct 30 16:46:52 +0000 2012  fake      86  \n",
       "8   Tue Oct 30 02:54:46 +0000 2012  fake      49  \n",
       "9   Mon Oct 29 17:59:59 +0000 2012  fake      16  \n",
       "10  Mon Oct 29 18:47:20 +0000 2012  fake      66  \n",
       "11  Tue Oct 30 04:04:04 +0000 2012  fake      25  \n",
       "12  Tue Oct 30 01:33:53 +0000 2012  fake      46  \n",
       "13  Mon Oct 29 18:55:10 +0000 2012  fake      46  \n",
       "14  Tue Oct 30 00:12:36 +0000 2012  fake      42  \n",
       "15  Tue Oct 30 13:25:24 +0000 2012  fake     117  \n",
       "16  Tue Oct 30 10:43:31 +0000 2012  fake     109  \n",
       "17  Tue Oct 30 07:44:08 +0000 2012  fake     109  \n",
       "18  Mon Oct 29 19:36:03 +0000 2012  fake      69  \n",
       "19  Tue Oct 30 19:35:48 +0000 2012  fake      27  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pre-processing\n",
    "\n",
    "#some entries contain multiple entries, first task is to remove these entries\n",
    "def removeURLs(text):\n",
    "    return re.sub(r'(https?://[^\\s]+)','', text)\n",
    "    \n",
    "#remove URLs\n",
    "train_data['tweetText']= train_data['tweetText'].apply(lambda x: removeURLs(x))\n",
    "test_data['tweetText'] = test_data['tweetText'].apply(lambda x: removeURLs(x))\n",
    "#get length of tweets with URL removed\n",
    "train_data['length'] = train_data['tweetText'].apply(lambda x: len(x))\n",
    "test_data['length'] = test_data['tweetText'].apply(lambda x: len(x))\n",
    "\n",
    "#pd.set_option('display.max_colwidth', None) #allows you to see full fields\n",
    "#train_data.loc[train_data['length'] > 200]\n",
    "\n",
    "#every tweet with no URL length more than 217 contains multiple tweets so should be removed\n",
    "#decided to make 280 the cutoff as this is the max number of characters allowed in a tweet atm\n",
    "train_data = train_data.loc[train_data['length'] <= 280]\n",
    "test_data = test_data.loc[test_data['length'] <= 280]\n",
    "\n",
    "train_data.loc[train_data['length'] > 200]\n",
    "\n",
    "pd.set_option('display.max_colwidth', None) #allows you to see full fields\n",
    "\n",
    "train_data.head(20)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62455952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>cleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263046056240115712</td>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy.</td>\n",
       "      <td>21226711</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>iAnnieM</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>114</td>\n",
       "      <td>remember the movie the day after tomorrow reminds me of what is happening with hurricane sandy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>262995061304852481</td>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\"  REAL! RT.</td>\n",
       "      <td>192378571</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>CarlosVerareal</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>113</td>\n",
       "      <td>milenagimon look at sandy in ny tremendous image of the hurricane looks like independence day  real rt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262979898002534400</td>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy</td>\n",
       "      <td>132303095</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>LucasPalape</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>96</td>\n",
       "      <td>good the picture of hurricane sandy reminds me of the movie independence day id sandy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262996108400271360</td>\n",
       "      <td>Scary shit #hurricane #NY</td>\n",
       "      <td>241995902</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Haaaaarryyy</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>26</td>\n",
       "      <td>scary shit hurricane ny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263018881839411200</td>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ</td>\n",
       "      <td>250315890</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>princess__natt</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>69</td>\n",
       "      <td>my fave place in the world nyc hurricane sandy statueofliberty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>263364439582060545</td>\n",
       "      <td>42nd #time #square #NYC #subway #hurricane</td>\n",
       "      <td>163674788</td>\n",
       "      <td>sandyA_fake_23</td>\n",
       "      <td>classycg</td>\n",
       "      <td>Tue Oct 30 19:39:10 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>43</td>\n",
       "      <td>nd time square nyc subway hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>262927032705490944</td>\n",
       "      <td>Just in time for #halloween a photo of #hurricane #sandy #frankenstorm</td>\n",
       "      <td>246153081</td>\n",
       "      <td>sandyA_fake_14</td>\n",
       "      <td>j_unit87</td>\n",
       "      <td>Mon Oct 29 14:41:04 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>71</td>\n",
       "      <td>just in time for halloween a photo of hurricane sandy frankenstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>263321078884077568</td>\n",
       "      <td>Crazy pic of #Hurricane #Sandy prayers go out to family and friends on the East Coast</td>\n",
       "      <td>199565482</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>MrBlakMagik</td>\n",
       "      <td>Tue Oct 30 16:46:52 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>86</td>\n",
       "      <td>crazy pic of hurricane sandy prayers go out to family and friends on the east coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>263111677485142017</td>\n",
       "      <td>#sandy #newyork #hurricane #statueofliberty #USA</td>\n",
       "      <td>78475739</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>safi37</td>\n",
       "      <td>Tue Oct 30 02:54:46 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>49</td>\n",
       "      <td>sandy newyork hurricane statueofliberty usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>262977091983785985</td>\n",
       "      <td>#nyc #hurricane</td>\n",
       "      <td>869777653</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>kingmichael03</td>\n",
       "      <td>Mon Oct 29 17:59:59 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>16</td>\n",
       "      <td>nyc hurricane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId  \\\n",
       "0  263046056240115712   \n",
       "1  262995061304852481   \n",
       "2  262979898002534400   \n",
       "3  262996108400271360   \n",
       "4  263018881839411200   \n",
       "5  263364439582060545   \n",
       "6  262927032705490944   \n",
       "7  263321078884077568   \n",
       "8  263111677485142017   \n",
       "9  262977091983785985   \n",
       "\n",
       "                                                                                                            tweetText  \\\n",
       "0  ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy.    \n",
       "1   @milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\"  REAL! RT.   \n",
       "2                    Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy     \n",
       "3                                                                                          Scary shit #hurricane #NY    \n",
       "4                                               My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ    \n",
       "5                                                                         42nd #time #square #NYC #subway #hurricane    \n",
       "6                                             Just in time for #halloween a photo of #hurricane #sandy #frankenstorm    \n",
       "7                              Crazy pic of #Hurricane #Sandy prayers go out to family and friends on the East Coast    \n",
       "8                                                                   #sandy #newyork #hurricane #statueofliberty #USA    \n",
       "9                                                                                                    #nyc #hurricane    \n",
       "\n",
       "      userId      imageId(s)        username                       timestamp  \\\n",
       "0   21226711  sandyA_fake_46         iAnnieM  Mon Oct 29 22:34:01 +0000 2012   \n",
       "1  192378571  sandyA_fake_09  CarlosVerareal  Mon Oct 29 19:11:23 +0000 2012   \n",
       "2  132303095  sandyA_fake_09     LucasPalape  Mon Oct 29 18:11:08 +0000 2012   \n",
       "3  241995902  sandyA_fake_29     Haaaaarryyy  Mon Oct 29 19:15:33 +0000 2012   \n",
       "4  250315890  sandyA_fake_15  princess__natt  Mon Oct 29 20:46:02 +0000 2012   \n",
       "5  163674788  sandyA_fake_23        classycg  Tue Oct 30 19:39:10 +0000 2012   \n",
       "6  246153081  sandyA_fake_14        j_unit87  Mon Oct 29 14:41:04 +0000 2012   \n",
       "7  199565482  sandyA_fake_29     MrBlakMagik  Tue Oct 30 16:46:52 +0000 2012   \n",
       "8   78475739  sandyA_fake_15          safi37  Tue Oct 30 02:54:46 +0000 2012   \n",
       "9  869777653  sandyA_fake_29   kingmichael03  Mon Oct 29 17:59:59 +0000 2012   \n",
       "\n",
       "  label  length  \\\n",
       "0  fake     114   \n",
       "1  fake     113   \n",
       "2  fake      96   \n",
       "3  fake      26   \n",
       "4  fake      69   \n",
       "5  fake      43   \n",
       "6  fake      71   \n",
       "7  fake      86   \n",
       "8  fake      49   \n",
       "9  fake      16   \n",
       "\n",
       "                                                                                                cleanText  \n",
       "0          remember the movie the day after tomorrow reminds me of what is happening with hurricane sandy  \n",
       "1  milenagimon look at sandy in ny tremendous image of the hurricane looks like independence day  real rt  \n",
       "2                   good the picture of hurricane sandy reminds me of the movie independence day id sandy  \n",
       "3                                                                                 scary shit hurricane ny  \n",
       "4                                          my fave place in the world nyc hurricane sandy statueofliberty  \n",
       "5                                                                     nd time square nyc subway hurricane  \n",
       "6                                      just in time for halloween a photo of hurricane sandy frankenstorm  \n",
       "7                     crazy pic of hurricane sandy prayers go out to family and friends on the east coast  \n",
       "8                                                             sandy newyork hurricane statueofliberty usa  \n",
       "9                                                                                           nyc hurricane  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pre-processing - prepare text for translation then translate it\n",
    "\n",
    "#translate a tweet to english\n",
    "def translate_tweet(text):\n",
    "    try:\n",
    "        translated = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "        return translated\n",
    "    except:\n",
    "        return text\n",
    "\n",
    "#kremove numbers last as might aid in translation e.g. 2nd, 1st\n",
    "#remove non word characters, &amp, new line indicators, convert to same case and translate\n",
    "def clean_text(tweet):\n",
    "    tweet = re.sub(r'&\\S+', '', tweet) # remove '&amp'\n",
    "    tweet = tweet.replace(\"\\\\n\",'') # remove end of line signs '\\n'\n",
    "    tweet = re.sub(r'[^\\w\\s]','',tweet) # remove non word characters\n",
    "    tweet = tweet.lower() #convert to lower case\n",
    "    tweet = translate_tweet(tweet) #translate to english\n",
    "    tweet = re.sub(r'[0-9]','',tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "\n",
    "#train_data['cleanText'] = train_data['tweetText'].apply(lambda x: clean_text(x))\n",
    "#test_data['cleanText'] = test_data['tweetText'].apply(lambda x: clean_text(x))\n",
    "#test_data.head()\n",
    "#train_data.loc[train_data['tweetId'] == 263490059825729537]\n",
    "#entry = train_data.iloc[12157]['tweetText']\n",
    "#print(entry)\n",
    "#print(clean_text(entry))\n",
    "\n",
    "\n",
    "                   \n",
    "pd.set_option('display.max_colwidth', None) #allows you to see full fields\n",
    "train_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c83d170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#example to make sure new line actually is removed\n",
    "#train_data.loc[train_data['tweetId'] == 263490059825729537]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aaae0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write cleanText to csv file so won't have to spend so long running \n",
    "#train_data.to_csv('train_tweets.csv',index=False)\n",
    "#test_data.to_csv('test_tweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e7e39a92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>cleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>578854927457349632</td>\n",
       "      <td>kereeen RT @Shyman33: Eclipse from ISS....</td>\n",
       "      <td>70824972</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>peay_s</td>\n",
       "      <td>Fri Mar 20 09:45:43 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>43</td>\n",
       "      <td>kereeen rt shyman eclipse from iss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578874632670953472</td>\n",
       "      <td>Absolutely beautiful! RT @Shyman33: Eclipse from ISS....</td>\n",
       "      <td>344707006</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>JaredUcanChange</td>\n",
       "      <td>Fri Mar 20 11:04:02 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>57</td>\n",
       "      <td>absolutely beautiful rt shyman eclipse from iss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>578891261353984000</td>\n",
       "      <td>‚Äú@Shyman33: Eclipse from ISS....  Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!</td>\n",
       "      <td>224839607</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>tpjp1231</td>\n",
       "      <td>Fri Mar 20 12:10:06 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>61</td>\n",
       "      <td>shyman eclipse from iss  eclipse seen from space wow amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578846612312748032</td>\n",
       "      <td>Eclipse from ISS....</td>\n",
       "      <td>134543073</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Shyman33</td>\n",
       "      <td>Fri Mar 20 09:12:41 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>21</td>\n",
       "      <td>eclipse from iss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>578975333841551360</td>\n",
       "      <td>@ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç</td>\n",
       "      <td>1150728872</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Epimethee_</td>\n",
       "      <td>Fri Mar 20 17:44:11 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>87</td>\n",
       "      <td>ebonfigli eclipse view of liss something else divine creation has no limit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>579274670853226496</td>\n",
       "      <td>‚Äú@ebonfigli: √âclipse vue de l'ISS... Autre chose...</td>\n",
       "      <td>470889709</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>BusineMi</td>\n",
       "      <td>Sat Mar 21 13:33:38 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>52</td>\n",
       "      <td>ebonfigli eclipse view of liss something else</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>578861590482665472</td>\n",
       "      <td>√âclipse vue de l'ISS... Autre chose...</td>\n",
       "      <td>383831305</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>ebonfigli</td>\n",
       "      <td>Fri Mar 20 10:12:12 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>39</td>\n",
       "      <td>eclipse view of liss something else</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>578976098052091904</td>\n",
       "      <td>@ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç</td>\n",
       "      <td>3044246089</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>OumNur</td>\n",
       "      <td>Fri Mar 20 17:47:13 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>87</td>\n",
       "      <td>ebonfigli eclipse view of liss something else divine creation has no limit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>578844275061981184</td>\n",
       "      <td>Dit dus \\0/ RT ‚Äú@News_Executive: The Solar eclipse seen from International Space Station. #SolarEclipse #ISS #Space</td>\n",
       "      <td>291020879</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>PatriciaKusters</td>\n",
       "      <td>Fri Mar 20 09:03:24 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>116</td>\n",
       "      <td>dit dus  rt news_executive the solar eclipse seen from international space station solareclipse iss space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>578838737448235008</td>\n",
       "      <td>Photo: The Solar eclipse as seen from the International Space Station. #SolarEclipse #ISS #Space</td>\n",
       "      <td>364810202</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>News_Executive</td>\n",
       "      <td>Fri Mar 20 08:41:23 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>97</td>\n",
       "      <td>photo the solar eclipse as seen from the international space station solareclipse iss space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId  \\\n",
       "0  578854927457349632   \n",
       "1  578874632670953472   \n",
       "2  578891261353984000   \n",
       "3  578846612312748032   \n",
       "4  578975333841551360   \n",
       "5  579274670853226496   \n",
       "6  578861590482665472   \n",
       "7  578976098052091904   \n",
       "8  578844275061981184   \n",
       "9  578838737448235008   \n",
       "\n",
       "                                                                                                              tweetText  \\\n",
       "0                                                                           kereeen RT @Shyman33: Eclipse from ISS....    \n",
       "1                                                             Absolutely beautiful! RT @Shyman33: Eclipse from ISS....    \n",
       "2                                                         ‚Äú@Shyman33: Eclipse from ISS....  Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!   \n",
       "3                                                                                                 Eclipse from ISS....    \n",
       "4                               @ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç   \n",
       "5                                                                  ‚Äú@ebonfigli: √âclipse vue de l'ISS... Autre chose...    \n",
       "6                                                                               √âclipse vue de l'ISS... Autre chose...    \n",
       "7                               @ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç   \n",
       "8  Dit dus \\0/ RT ‚Äú@News_Executive: The Solar eclipse seen from International Space Station. #SolarEclipse #ISS #Space    \n",
       "9                     Photo: The Solar eclipse as seen from the International Space Station. #SolarEclipse #ISS #Space    \n",
       "\n",
       "       userId   imageId(s)         username                       timestamp  \\\n",
       "0    70824972  eclipse_01            peay_s  Fri Mar 20 09:45:43 +0000 2015   \n",
       "1   344707006  eclipse_01   JaredUcanChange  Fri Mar 20 11:04:02 +0000 2015   \n",
       "2   224839607  eclipse_01          tpjp1231  Fri Mar 20 12:10:06 +0000 2015   \n",
       "3   134543073  eclipse_01          Shyman33  Fri Mar 20 09:12:41 +0000 2015   \n",
       "4  1150728872   eclipse_01       Epimethee_  Fri Mar 20 17:44:11 +0000 2015   \n",
       "5   470889709  eclipse_01          BusineMi  Sat Mar 21 13:33:38 +0000 2015   \n",
       "6   383831305  eclipse_01         ebonfigli  Fri Mar 20 10:12:12 +0000 2015   \n",
       "7  3044246089   eclipse_01           OumNur  Fri Mar 20 17:47:13 +0000 2015   \n",
       "8   291020879   eclipse_01  PatriciaKusters  Fri Mar 20 09:03:24 +0000 2015   \n",
       "9   364810202   eclipse_01   News_Executive  Fri Mar 20 08:41:23 +0000 2015   \n",
       "\n",
       "  label  length  \\\n",
       "0  fake      43   \n",
       "1  fake      57   \n",
       "2  fake      61   \n",
       "3  fake      21   \n",
       "4  fake      87   \n",
       "5  fake      52   \n",
       "6  fake      39   \n",
       "7  fake      87   \n",
       "8  fake     116   \n",
       "9  fake      97   \n",
       "\n",
       "                                                                                                   cleanText  \n",
       "0                                                                         kereeen rt shyman eclipse from iss  \n",
       "1                                                            absolutely beautiful rt shyman eclipse from iss  \n",
       "2                                               shyman eclipse from iss  eclipse seen from space wow amazing  \n",
       "3                                                                                           eclipse from iss  \n",
       "4                                 ebonfigli eclipse view of liss something else divine creation has no limit  \n",
       "5                                                              ebonfigli eclipse view of liss something else  \n",
       "6                                                                        eclipse view of liss something else  \n",
       "7                                 ebonfigli eclipse view of liss something else divine creation has no limit  \n",
       "8  dit dus  rt news_executive the solar eclipse seen from international space station solareclipse iss space  \n",
       "9                photo the solar eclipse as seen from the international space station solareclipse iss space  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#retrieve cleaned and translated tweets\n",
    "train_data = pd.read_csv('train_tweets.csv')\n",
    "#train_data.head(10)\n",
    "test_data = pd.read_csv('test_tweets.csv')\n",
    "test_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7178f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14262 entries, 0 to 14261\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   tweetId     14262 non-null  int64 \n",
      " 1   tweetText   14261 non-null  object\n",
      " 2   userId      14262 non-null  int64 \n",
      " 3   imageId(s)  14262 non-null  object\n",
      " 4   username    14262 non-null  object\n",
      " 5   timestamp   14262 non-null  object\n",
      " 6   label       14262 non-null  object\n",
      " 7   length      14262 non-null  int64 \n",
      " 8   cleanText   14261 non-null  object\n",
      "dtypes: int64(3), object(6)\n",
      "memory usage: 1002.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8efaaaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>cleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263046056240115712</td>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy.</td>\n",
       "      <td>21226711</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>iAnnieM</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>114</td>\n",
       "      <td>remember movie day tomorrow reminds happening hurricane sandy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>262995061304852481</td>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\"  REAL! RT.</td>\n",
       "      <td>192378571</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>CarlosVerareal</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>113</td>\n",
       "      <td>milenagimon look sandy ny tremendous image hurricane looks like independence day real rt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262979898002534400</td>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy</td>\n",
       "      <td>132303095</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>LucasPalape</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>96</td>\n",
       "      <td>good picture hurricane sandy reminds movie independence day id sandy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262996108400271360</td>\n",
       "      <td>Scary shit #hurricane #NY</td>\n",
       "      <td>241995902</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Haaaaarryyy</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>26</td>\n",
       "      <td>scary shit hurricane ny</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263018881839411200</td>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ</td>\n",
       "      <td>250315890</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>princess__natt</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>69</td>\n",
       "      <td>fave place world nyc hurricane sandy statueofliberty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>263364439582060545</td>\n",
       "      <td>42nd #time #square #NYC #subway #hurricane</td>\n",
       "      <td>163674788</td>\n",
       "      <td>sandyA_fake_23</td>\n",
       "      <td>classycg</td>\n",
       "      <td>Tue Oct 30 19:39:10 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>43</td>\n",
       "      <td>nd time square nyc subway hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>262927032705490944</td>\n",
       "      <td>Just in time for #halloween a photo of #hurricane #sandy #frankenstorm</td>\n",
       "      <td>246153081</td>\n",
       "      <td>sandyA_fake_14</td>\n",
       "      <td>j_unit87</td>\n",
       "      <td>Mon Oct 29 14:41:04 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>71</td>\n",
       "      <td>time halloween photo hurricane sandy frankenstorm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>263321078884077568</td>\n",
       "      <td>Crazy pic of #Hurricane #Sandy prayers go out to family and friends on the East Coast</td>\n",
       "      <td>199565482</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>MrBlakMagik</td>\n",
       "      <td>Tue Oct 30 16:46:52 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>86</td>\n",
       "      <td>crazy pic hurricane sandy prayers go family friends east coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>263111677485142017</td>\n",
       "      <td>#sandy #newyork #hurricane #statueofliberty #USA</td>\n",
       "      <td>78475739</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>safi37</td>\n",
       "      <td>Tue Oct 30 02:54:46 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>49</td>\n",
       "      <td>sandy newyork hurricane statueofliberty usa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>262977091983785985</td>\n",
       "      <td>#nyc #hurricane</td>\n",
       "      <td>869777653</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>kingmichael03</td>\n",
       "      <td>Mon Oct 29 17:59:59 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>16</td>\n",
       "      <td>nyc hurricane</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId  \\\n",
       "0  263046056240115712   \n",
       "1  262995061304852481   \n",
       "2  262979898002534400   \n",
       "3  262996108400271360   \n",
       "4  263018881839411200   \n",
       "5  263364439582060545   \n",
       "6  262927032705490944   \n",
       "7  263321078884077568   \n",
       "8  263111677485142017   \n",
       "9  262977091983785985   \n",
       "\n",
       "                                                                                                            tweetText  \\\n",
       "0  ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy.    \n",
       "1   @milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\"  REAL! RT.   \n",
       "2                    Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy     \n",
       "3                                                                                          Scary shit #hurricane #NY    \n",
       "4                                               My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ    \n",
       "5                                                                         42nd #time #square #NYC #subway #hurricane    \n",
       "6                                             Just in time for #halloween a photo of #hurricane #sandy #frankenstorm    \n",
       "7                              Crazy pic of #Hurricane #Sandy prayers go out to family and friends on the East Coast    \n",
       "8                                                                   #sandy #newyork #hurricane #statueofliberty #USA    \n",
       "9                                                                                                    #nyc #hurricane    \n",
       "\n",
       "      userId      imageId(s)        username                       timestamp  \\\n",
       "0   21226711  sandyA_fake_46         iAnnieM  Mon Oct 29 22:34:01 +0000 2012   \n",
       "1  192378571  sandyA_fake_09  CarlosVerareal  Mon Oct 29 19:11:23 +0000 2012   \n",
       "2  132303095  sandyA_fake_09     LucasPalape  Mon Oct 29 18:11:08 +0000 2012   \n",
       "3  241995902  sandyA_fake_29     Haaaaarryyy  Mon Oct 29 19:15:33 +0000 2012   \n",
       "4  250315890  sandyA_fake_15  princess__natt  Mon Oct 29 20:46:02 +0000 2012   \n",
       "5  163674788  sandyA_fake_23        classycg  Tue Oct 30 19:39:10 +0000 2012   \n",
       "6  246153081  sandyA_fake_14        j_unit87  Mon Oct 29 14:41:04 +0000 2012   \n",
       "7  199565482  sandyA_fake_29     MrBlakMagik  Tue Oct 30 16:46:52 +0000 2012   \n",
       "8   78475739  sandyA_fake_15          safi37  Tue Oct 30 02:54:46 +0000 2012   \n",
       "9  869777653  sandyA_fake_29   kingmichael03  Mon Oct 29 17:59:59 +0000 2012   \n",
       "\n",
       "  label  length  \\\n",
       "0  fake     114   \n",
       "1  fake     113   \n",
       "2  fake      96   \n",
       "3  fake      26   \n",
       "4  fake      69   \n",
       "5  fake      43   \n",
       "6  fake      71   \n",
       "7  fake      86   \n",
       "8  fake      49   \n",
       "9  fake      16   \n",
       "\n",
       "                                                                                  cleanText  \n",
       "0                             remember movie day tomorrow reminds happening hurricane sandy  \n",
       "1  milenagimon look sandy ny tremendous image hurricane looks like independence day real rt  \n",
       "2                      good picture hurricane sandy reminds movie independence day id sandy  \n",
       "3                                                                   scary shit hurricane ny  \n",
       "4                                      fave place world nyc hurricane sandy statueofliberty  \n",
       "5                                                       nd time square nyc subway hurricane  \n",
       "6                                         time halloween photo hurricane sandy frankenstorm  \n",
       "7                            crazy pic hurricane sandy prayers go family friends east coast  \n",
       "8                                               sandy newyork hurricane statueofliberty usa  \n",
       "9                                                                             nyc hurricane  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pre-processing - remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stop_words(tweet):\n",
    "    return \" \".join([word for word in str(tweet).split() if word not in stop_words])\n",
    "\n",
    "train_data['cleanText'] = train_data['cleanText'].apply(lambda x: remove_stop_words(x))\n",
    "test_data['cleanText'] = test_data['cleanText'].apply(lambda x: remove_stop_words(x))\n",
    "\n",
    "train_data.head(10)\n",
    "\n",
    "#string = 'crazy pic of hurricane sandy prayers go out to family and friends on the east coast'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72f12c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pre-processing - try to correct spelling mistakes (and maybe abbreviations)\n",
    "string = 'well howdy hurricane sandy wanted let know took power internet happi'\n",
    "spell = Speller(lang='en')\n",
    "\n",
    "def correct_spelling(tweet):\n",
    "    words = tweet.split()\n",
    "    return \" \".join([spell(item) for item in words])\n",
    "\n",
    "#correct_spelling(string)\n",
    "\n",
    "#train_data['cleanText'] = train_data['cleanText'].apply(lambda x: correct_spelling(x))\n",
    "#train_data.head(20)\n",
    "#test_data['cleanText'] = test_data['cleanText'].apply(lambda x: correct_spelling(x))\n",
    "#test_data.head(10)\n",
    "\n",
    "\n",
    "#words such as happi puppi\n",
    "#known([words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fce47923",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write spell checked cleanText to csv file so won't have to spend so long running \n",
    "#train_data.to_csv('train_tweets_spellcheck.csv',index=False)\n",
    "#test_data.to_csv('test_tweets_spellcheck.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "07e9bf4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>cleanText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>578854927457349632</td>\n",
       "      <td>kereeen RT @Shyman33: Eclipse from ISS....</td>\n",
       "      <td>70824972</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>peay_s</td>\n",
       "      <td>Fri Mar 20 09:45:43 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>43</td>\n",
       "      <td>kereeen rt shaman eclipse iss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578874632670953472</td>\n",
       "      <td>Absolutely beautiful! RT @Shyman33: Eclipse from ISS....</td>\n",
       "      <td>344707006</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>JaredUcanChange</td>\n",
       "      <td>Fri Mar 20 11:04:02 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>57</td>\n",
       "      <td>absolutely beautiful rt shaman eclipse iss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>578891261353984000</td>\n",
       "      <td>‚Äú@Shyman33: Eclipse from ISS....  Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!</td>\n",
       "      <td>224839607</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>tpjp1231</td>\n",
       "      <td>Fri Mar 20 12:10:06 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>61</td>\n",
       "      <td>shaman eclipse iss eclipse seen space wow amazing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578846612312748032</td>\n",
       "      <td>Eclipse from ISS....</td>\n",
       "      <td>134543073</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Shyman33</td>\n",
       "      <td>Fri Mar 20 09:12:41 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>21</td>\n",
       "      <td>eclipse iss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>578975333841551360</td>\n",
       "      <td>@ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç</td>\n",
       "      <td>1150728872</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Epimethee_</td>\n",
       "      <td>Fri Mar 20 17:44:11 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>87</td>\n",
       "      <td>ebonfigli eclipse view list something else divine creation limit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>579274670853226496</td>\n",
       "      <td>‚Äú@ebonfigli: √âclipse vue de l'ISS... Autre chose...</td>\n",
       "      <td>470889709</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>BusineMi</td>\n",
       "      <td>Sat Mar 21 13:33:38 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>52</td>\n",
       "      <td>ebonfigli eclipse view list something else</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>578861590482665472</td>\n",
       "      <td>√âclipse vue de l'ISS... Autre chose...</td>\n",
       "      <td>383831305</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>ebonfigli</td>\n",
       "      <td>Fri Mar 20 10:12:12 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>39</td>\n",
       "      <td>eclipse view list something else</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>578976098052091904</td>\n",
       "      <td>@ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç</td>\n",
       "      <td>3044246089</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>OumNur</td>\n",
       "      <td>Fri Mar 20 17:47:13 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>87</td>\n",
       "      <td>ebonfigli eclipse view list something else divine creation limit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>578844275061981184</td>\n",
       "      <td>Dit dus \\0/ RT ‚Äú@News_Executive: The Solar eclipse seen from International Space Station. #SolarEclipse #ISS #Space</td>\n",
       "      <td>291020879</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>PatriciaKusters</td>\n",
       "      <td>Fri Mar 20 09:03:24 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>116</td>\n",
       "      <td>dit us rt news_executive solar eclipse seen international space station solareclipse iss space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>578838737448235008</td>\n",
       "      <td>Photo: The Solar eclipse as seen from the International Space Station. #SolarEclipse #ISS #Space</td>\n",
       "      <td>364810202</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>News_Executive</td>\n",
       "      <td>Fri Mar 20 08:41:23 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>97</td>\n",
       "      <td>photo solar eclipse seen international space station solareclipse iss space</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId  \\\n",
       "0  578854927457349632   \n",
       "1  578874632670953472   \n",
       "2  578891261353984000   \n",
       "3  578846612312748032   \n",
       "4  578975333841551360   \n",
       "5  579274670853226496   \n",
       "6  578861590482665472   \n",
       "7  578976098052091904   \n",
       "8  578844275061981184   \n",
       "9  578838737448235008   \n",
       "\n",
       "                                                                                                              tweetText  \\\n",
       "0                                                                           kereeen RT @Shyman33: Eclipse from ISS....    \n",
       "1                                                             Absolutely beautiful! RT @Shyman33: Eclipse from ISS....    \n",
       "2                                                         ‚Äú@Shyman33: Eclipse from ISS....  Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!   \n",
       "3                                                                                                 Eclipse from ISS....    \n",
       "4                               @ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç   \n",
       "5                                                                  ‚Äú@ebonfigli: √âclipse vue de l'ISS... Autre chose...    \n",
       "6                                                                               √âclipse vue de l'ISS... Autre chose...    \n",
       "7                               @ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç   \n",
       "8  Dit dus \\0/ RT ‚Äú@News_Executive: The Solar eclipse seen from International Space Station. #SolarEclipse #ISS #Space    \n",
       "9                     Photo: The Solar eclipse as seen from the International Space Station. #SolarEclipse #ISS #Space    \n",
       "\n",
       "       userId   imageId(s)         username                       timestamp  \\\n",
       "0    70824972  eclipse_01            peay_s  Fri Mar 20 09:45:43 +0000 2015   \n",
       "1   344707006  eclipse_01   JaredUcanChange  Fri Mar 20 11:04:02 +0000 2015   \n",
       "2   224839607  eclipse_01          tpjp1231  Fri Mar 20 12:10:06 +0000 2015   \n",
       "3   134543073  eclipse_01          Shyman33  Fri Mar 20 09:12:41 +0000 2015   \n",
       "4  1150728872   eclipse_01       Epimethee_  Fri Mar 20 17:44:11 +0000 2015   \n",
       "5   470889709  eclipse_01          BusineMi  Sat Mar 21 13:33:38 +0000 2015   \n",
       "6   383831305  eclipse_01         ebonfigli  Fri Mar 20 10:12:12 +0000 2015   \n",
       "7  3044246089   eclipse_01           OumNur  Fri Mar 20 17:47:13 +0000 2015   \n",
       "8   291020879   eclipse_01  PatriciaKusters  Fri Mar 20 09:03:24 +0000 2015   \n",
       "9   364810202   eclipse_01   News_Executive  Fri Mar 20 08:41:23 +0000 2015   \n",
       "\n",
       "  label  length  \\\n",
       "0  fake      43   \n",
       "1  fake      57   \n",
       "2  fake      61   \n",
       "3  fake      21   \n",
       "4  fake      87   \n",
       "5  fake      52   \n",
       "6  fake      39   \n",
       "7  fake      87   \n",
       "8  fake     116   \n",
       "9  fake      97   \n",
       "\n",
       "                                                                                        cleanText  \n",
       "0                                                                   kereeen rt shaman eclipse iss  \n",
       "1                                                      absolutely beautiful rt shaman eclipse iss  \n",
       "2                                               shaman eclipse iss eclipse seen space wow amazing  \n",
       "3                                                                                     eclipse iss  \n",
       "4                                ebonfigli eclipse view list something else divine creation limit  \n",
       "5                                                      ebonfigli eclipse view list something else  \n",
       "6                                                                eclipse view list something else  \n",
       "7                                ebonfigli eclipse view list something else divine creation limit  \n",
       "8  dit us rt news_executive solar eclipse seen international space station solareclipse iss space  \n",
       "9                     photo solar eclipse seen international space station solareclipse iss space  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('train_tweets_spellcheck.csv')\n",
    "test_data = pd.read_csv('test_tweets_spellcheck.csv')\n",
    "\n",
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "368bd1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>sentimentScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>578854927457349632</td>\n",
       "      <td>kereeen RT @Shyman33: Eclipse from ISS....</td>\n",
       "      <td>70824972</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>peay_s</td>\n",
       "      <td>Fri Mar 20 09:45:43 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>43</td>\n",
       "      <td>kereeen rt shaman eclipse iss</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578874632670953472</td>\n",
       "      <td>Absolutely beautiful! RT @Shyman33: Eclipse from ISS....</td>\n",
       "      <td>344707006</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>JaredUcanChange</td>\n",
       "      <td>Fri Mar 20 11:04:02 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>57</td>\n",
       "      <td>absolutely beautiful rt shaman eclipse iss</td>\n",
       "      <td>0.6361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>578891261353984000</td>\n",
       "      <td>‚Äú@Shyman33: Eclipse from ISS....  Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!</td>\n",
       "      <td>224839607</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>tpjp1231</td>\n",
       "      <td>Fri Mar 20 12:10:06 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>61</td>\n",
       "      <td>shaman eclipse iss eclipse seen space wow amazing</td>\n",
       "      <td>0.8225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578846612312748032</td>\n",
       "      <td>Eclipse from ISS....</td>\n",
       "      <td>134543073</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Shyman33</td>\n",
       "      <td>Fri Mar 20 09:12:41 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>21</td>\n",
       "      <td>eclipse iss</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>578975333841551360</td>\n",
       "      <td>@ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç</td>\n",
       "      <td>1150728872</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Epimethee_</td>\n",
       "      <td>Fri Mar 20 17:44:11 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>87</td>\n",
       "      <td>ebonfigli eclipse view list something else divine creation limit</td>\n",
       "      <td>0.6908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>579274670853226496</td>\n",
       "      <td>‚Äú@ebonfigli: √âclipse vue de l'ISS... Autre chose...</td>\n",
       "      <td>470889709</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>BusineMi</td>\n",
       "      <td>Sat Mar 21 13:33:38 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>52</td>\n",
       "      <td>ebonfigli eclipse view list something else</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>578861590482665472</td>\n",
       "      <td>√âclipse vue de l'ISS... Autre chose...</td>\n",
       "      <td>383831305</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>ebonfigli</td>\n",
       "      <td>Fri Mar 20 10:12:12 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>39</td>\n",
       "      <td>eclipse view list something else</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>578976098052091904</td>\n",
       "      <td>@ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç</td>\n",
       "      <td>3044246089</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>OumNur</td>\n",
       "      <td>Fri Mar 20 17:47:13 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>87</td>\n",
       "      <td>ebonfigli eclipse view list something else divine creation limit</td>\n",
       "      <td>0.6908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>578844275061981184</td>\n",
       "      <td>Dit dus \\0/ RT ‚Äú@News_Executive: The Solar eclipse seen from International Space Station. #SolarEclipse #ISS #Space</td>\n",
       "      <td>291020879</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>PatriciaKusters</td>\n",
       "      <td>Fri Mar 20 09:03:24 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>116</td>\n",
       "      <td>dit us rt news_executive solar eclipse seen international space station solareclipse iss space</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>578838737448235008</td>\n",
       "      <td>Photo: The Solar eclipse as seen from the International Space Station. #SolarEclipse #ISS #Space</td>\n",
       "      <td>364810202</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>News_Executive</td>\n",
       "      <td>Fri Mar 20 08:41:23 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>97</td>\n",
       "      <td>photo solar eclipse seen international space station solareclipse iss space</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId  \\\n",
       "0  578854927457349632   \n",
       "1  578874632670953472   \n",
       "2  578891261353984000   \n",
       "3  578846612312748032   \n",
       "4  578975333841551360   \n",
       "5  579274670853226496   \n",
       "6  578861590482665472   \n",
       "7  578976098052091904   \n",
       "8  578844275061981184   \n",
       "9  578838737448235008   \n",
       "\n",
       "                                                                                                              tweetText  \\\n",
       "0                                                                           kereeen RT @Shyman33: Eclipse from ISS....    \n",
       "1                                                             Absolutely beautiful! RT @Shyman33: Eclipse from ISS....    \n",
       "2                                                         ‚Äú@Shyman33: Eclipse from ISS....  Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!   \n",
       "3                                                                                                 Eclipse from ISS....    \n",
       "4                               @ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç   \n",
       "5                                                                  ‚Äú@ebonfigli: √âclipse vue de l'ISS... Autre chose...    \n",
       "6                                                                               √âclipse vue de l'ISS... Autre chose...    \n",
       "7                               @ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç   \n",
       "8  Dit dus \\0/ RT ‚Äú@News_Executive: The Solar eclipse seen from International Space Station. #SolarEclipse #ISS #Space    \n",
       "9                     Photo: The Solar eclipse as seen from the International Space Station. #SolarEclipse #ISS #Space    \n",
       "\n",
       "       userId   imageId(s)         username                       timestamp  \\\n",
       "0    70824972  eclipse_01            peay_s  Fri Mar 20 09:45:43 +0000 2015   \n",
       "1   344707006  eclipse_01   JaredUcanChange  Fri Mar 20 11:04:02 +0000 2015   \n",
       "2   224839607  eclipse_01          tpjp1231  Fri Mar 20 12:10:06 +0000 2015   \n",
       "3   134543073  eclipse_01          Shyman33  Fri Mar 20 09:12:41 +0000 2015   \n",
       "4  1150728872   eclipse_01       Epimethee_  Fri Mar 20 17:44:11 +0000 2015   \n",
       "5   470889709  eclipse_01          BusineMi  Sat Mar 21 13:33:38 +0000 2015   \n",
       "6   383831305  eclipse_01         ebonfigli  Fri Mar 20 10:12:12 +0000 2015   \n",
       "7  3044246089   eclipse_01           OumNur  Fri Mar 20 17:47:13 +0000 2015   \n",
       "8   291020879   eclipse_01  PatriciaKusters  Fri Mar 20 09:03:24 +0000 2015   \n",
       "9   364810202   eclipse_01   News_Executive  Fri Mar 20 08:41:23 +0000 2015   \n",
       "\n",
       "  label  length  \\\n",
       "0  fake      43   \n",
       "1  fake      57   \n",
       "2  fake      61   \n",
       "3  fake      21   \n",
       "4  fake      87   \n",
       "5  fake      52   \n",
       "6  fake      39   \n",
       "7  fake      87   \n",
       "8  fake     116   \n",
       "9  fake      97   \n",
       "\n",
       "                                                                                        cleanText  \\\n",
       "0                                                                   kereeen rt shaman eclipse iss   \n",
       "1                                                      absolutely beautiful rt shaman eclipse iss   \n",
       "2                                               shaman eclipse iss eclipse seen space wow amazing   \n",
       "3                                                                                     eclipse iss   \n",
       "4                                ebonfigli eclipse view list something else divine creation limit   \n",
       "5                                                      ebonfigli eclipse view list something else   \n",
       "6                                                                eclipse view list something else   \n",
       "7                                ebonfigli eclipse view list something else divine creation limit   \n",
       "8  dit us rt news_executive solar eclipse seen international space station solareclipse iss space   \n",
       "9                     photo solar eclipse seen international space station solareclipse iss space   \n",
       "\n",
       "   sentimentScore  \n",
       "0          0.0000  \n",
       "1          0.6361  \n",
       "2          0.8225  \n",
       "3          0.0000  \n",
       "4          0.6908  \n",
       "5          0.0000  \n",
       "6          0.0000  \n",
       "7          0.6908  \n",
       "8          0.0000  \n",
       "9          0.0000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#feature extraction - get sentiment scores of tweets\n",
    "\n",
    "#taken from https://www.geeksforgeeks.org/python-sentiment-analysis-using-vader/\n",
    "# function to print sentiments of the sentence.\n",
    "def sentiment_scores(sentence):\n",
    "    # Create a SentimentIntensityAnalyzer object.\n",
    "    sid_obj = SentimentIntensityAnalyzer()\n",
    " \n",
    "    # polarity_scores method of SentimentIntensityAnalyzer\n",
    "    # object gives a sentiment dictionary.\n",
    "    # which contains pos, neg, neu, and compound scores.\n",
    "    try:\n",
    "        sentiment_dict = sid_obj.polarity_scores(sentence)\n",
    "        return sentiment_dict['compound']\n",
    "    except:\n",
    "        return 0\n",
    "     \n",
    "    #print(\"Overall sentiment dictionary is : \", sentiment_dict['compound'])\n",
    "    #print(\"sentence was rated as \", sentiment_dict['neg']*100, \"% Negative\")\n",
    "    #print(\"sentence was rated as \", sentiment_dict['neu']*100, \"% Neutral\")\n",
    "    #print(\"sentence was rated as \", sentiment_dict['pos']*100, \"% Positive\")\n",
    " \n",
    "    \n",
    "#string = 'well howdy hurricane sandy wanted let know took power bad unhappy'\n",
    "#sentiment_scores(string)\n",
    "#train_data['sentimentScore'] = train_data['cleanText'].apply(lambda x: sentiment_scores(x))\n",
    "#test_data['sentimentScore'] = test_data['cleanText'].apply(lambda x: sentiment_scores(x))\n",
    "#train_data.head(20)\n",
    "test_data.head(10)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eabd9666",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.to_csv('train_tweets_sentiment.csv',index=False)\n",
    "#test_data.to_csv('test_tweets_sentiment.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1cd9bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>sentimentScore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>578854927457349632</td>\n",
       "      <td>kereeen RT @Shyman33: Eclipse from ISS....</td>\n",
       "      <td>70824972</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>peay_s</td>\n",
       "      <td>Fri Mar 20 09:45:43 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>43</td>\n",
       "      <td>kereeen rt shaman eclipse iss</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578874632670953472</td>\n",
       "      <td>Absolutely beautiful! RT @Shyman33: Eclipse from ISS....</td>\n",
       "      <td>344707006</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>JaredUcanChange</td>\n",
       "      <td>Fri Mar 20 11:04:02 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>57</td>\n",
       "      <td>absolutely beautiful rt shaman eclipse iss</td>\n",
       "      <td>0.6361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>578891261353984000</td>\n",
       "      <td>‚Äú@Shyman33: Eclipse from ISS....  Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!</td>\n",
       "      <td>224839607</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>tpjp1231</td>\n",
       "      <td>Fri Mar 20 12:10:06 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>61</td>\n",
       "      <td>shaman eclipse iss eclipse seen space wow amazing</td>\n",
       "      <td>0.8225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578846612312748032</td>\n",
       "      <td>Eclipse from ISS....</td>\n",
       "      <td>134543073</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Shyman33</td>\n",
       "      <td>Fri Mar 20 09:12:41 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>21</td>\n",
       "      <td>eclipse iss</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>578975333841551360</td>\n",
       "      <td>@ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç</td>\n",
       "      <td>1150728872</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Epimethee_</td>\n",
       "      <td>Fri Mar 20 17:44:11 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>87</td>\n",
       "      <td>ebonfigli eclipse view list something else divine creation limit</td>\n",
       "      <td>0.6908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId  \\\n",
       "0  578854927457349632   \n",
       "1  578874632670953472   \n",
       "2  578891261353984000   \n",
       "3  578846612312748032   \n",
       "4  578975333841551360   \n",
       "\n",
       "                                                                                 tweetText  \\\n",
       "0                                              kereeen RT @Shyman33: Eclipse from ISS....    \n",
       "1                                Absolutely beautiful! RT @Shyman33: Eclipse from ISS....    \n",
       "2                            ‚Äú@Shyman33: Eclipse from ISS....  Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!   \n",
       "3                                                                    Eclipse from ISS....    \n",
       "4  @ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç   \n",
       "\n",
       "       userId   imageId(s)         username                       timestamp  \\\n",
       "0    70824972  eclipse_01            peay_s  Fri Mar 20 09:45:43 +0000 2015   \n",
       "1   344707006  eclipse_01   JaredUcanChange  Fri Mar 20 11:04:02 +0000 2015   \n",
       "2   224839607  eclipse_01          tpjp1231  Fri Mar 20 12:10:06 +0000 2015   \n",
       "3   134543073  eclipse_01          Shyman33  Fri Mar 20 09:12:41 +0000 2015   \n",
       "4  1150728872   eclipse_01       Epimethee_  Fri Mar 20 17:44:11 +0000 2015   \n",
       "\n",
       "  label  length  \\\n",
       "0  fake      43   \n",
       "1  fake      57   \n",
       "2  fake      61   \n",
       "3  fake      21   \n",
       "4  fake      87   \n",
       "\n",
       "                                                          cleanText  \\\n",
       "0                                     kereeen rt shaman eclipse iss   \n",
       "1                        absolutely beautiful rt shaman eclipse iss   \n",
       "2                 shaman eclipse iss eclipse seen space wow amazing   \n",
       "3                                                       eclipse iss   \n",
       "4  ebonfigli eclipse view list something else divine creation limit   \n",
       "\n",
       "   sentimentScore  \n",
       "0          0.0000  \n",
       "1          0.6361  \n",
       "2          0.8225  \n",
       "3          0.0000  \n",
       "4          0.6908  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('test_tweets_sentiment.csv')\n",
    "train_data = pd.read_csv('train_tweets_sentiment.csv')\n",
    "test_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24e9ebcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6735.000000\n",
       "mean        0.072473\n",
       "std         0.368732\n",
       "min        -0.946800\n",
       "25%         0.000000\n",
       "50%         0.000000\n",
       "75%         0.361200\n",
       "max         0.939300\n",
       "Name: sentimentScore, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_tweets = train_data[train_data['label'] == ('fake' or 'humor')]\n",
    "fake_tweets['sentimentScore'].describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "aa33ea6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'sentimentScore'}>]], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEICAYAAAC9E5gJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAagElEQVR4nO3dfZAc9X3n8ffHAhSZBSEdsBGSYuGzQqyHM7E2ss6Yu11DGUGwhavCnSjKSDGOHApXzEW5Y7GrYs627pRUZF8ojBKBOMTJZk+JjVEQOhsrbHGxkbFEAYuQCQKtQQ9eBRCSlnCyJb73R/8Ud+3OzvbuzM406POqmpqeX/+6+zs9D5/ph5lRRGBmZie3dzW7ADMzaz6HgZmZOQzMzMxhYGZmOAzMzAyHgZmZ4TCwk4ikzZKWNLsOszJyGNg7kqRbJa3Pt0XE5RGxrgm13CPpqwPaPiLpR5IOSXpN0g8l/U6jazM74ZRmF2B2spF0JvAgcAOwATgNuBg4WufljIuI4/Wcp71zecvASkHSzZL2Sjoi6TlJl0h6l6ROSS9IelXSBkmTU/8ZkkLSEkkvSXpF0hfTuIXAF4D/KKlf0lOpvVvSZ9Lw0vRp/OuSXpf0oqQPp/aXJR3I71KSNF7SX6Rl9Un6K0kT0rh2SXskLU/T7Zf0+2ncMuBa4L+kWv4O+E2AiLgvIo5HxJsR8f2IeDq3vD+QtDOtj2clfTC1vz/dj9cl7ZD0idw090haLekhSW8AHZLOk/RtSf8kabekPxq7R9He1iLCF1+aegEuAF4Gzku3ZwD/GrgJ2ApMA8YDfw3cl+sTwJ3ABOADZJ+s35/G3wqsH7CcbuAzaXgpcAz4fWAc8FXgJeAbaVkfA44ALan//wA2ApOBM4C/A/57Gtee5vVl4FTgCuCfgUlp/D3AV3N1nAm8CqwDLj/RLzf+amAv8DuAgPcB70nz3kUWdKcBH001XpBbziHgIrIPeu8GtgN/mvq/F3gRuKzZj7kv5bt4y8DK4DjZG/AsSadGRG9EvAB8FvhiROyJiKNkb/C/Jym/e/O/RvbJ+ingKbJQKGp3RPzPyHal/G9gOvDliDgaEd8HfgG8T5KAPwD+U0S8FhFHgP8GLM7N65dp2l9GxENAP1nIDRIRh4GP8Ksw+ydJGyW1pi6fAf48In4SmV0R8TNgAdACrIyIX0TE35PtbromN/sHIuKHEfEWMBc4JyK+nPq/mJaXr9sM8DEDK4GI2CXpJrI3+9mSvgf8Mdmn4fslvZXrfhxozd3+eW74n8neLIvqyw2/mWoZ2NYCnEP6lJ3lApB9Yh+X6/tqRBwrWktE7CTbOkHSbwHrybY+riELpRcqTHYe8HJ6oz/hZ8DU3O2Xc8PvAc6T9HqubRzwf4eqy05e3jKwUoiIb0XER8jewAL4M7I3tssj4qzc5dciYm+RWdaxvFfIgmF2ro6JEVE0eKrWEhE/JdvFMyc1vUy2m2ygfcB0SfnX7W+Q7VKqtKyXybZ+8uvvjIi4omDddhJxGFjTSbpA0kcljQf+H9kb73Hgr4AVkt6T+p0jaVHB2fYBMwa8cY5K+iR+J/B1SeemWqZKumwEtbz3xA1Jv5UONk9Lt6eTbRFsTV3uAv5E0jxl3pfWwY+BN8gORp8qqR34ONA1xHIfBw6ng/MTJI2TNMensFolDgMrg/HASrJP4D8HziU7SPqXZAdtvy/pCNmb5YcKzvNv0vWrkp6oQ403kx283SrpMPADhjgmUMFasuMhr0v6LtlB3w8BP05n/WwFngGWA0TE3wArgG+lvt8FJkfEL4BPkB10fgW4A7gubVkMko6FfBy4ENidprkLmDiC+20nCUX4z23MzE523jIwMzOHgZmZOQzMzAyHgZmZ8Tb40tnZZ58dM2bMGNT+xhtvcPrppze+oBEoe42ur3Zlr9H11a7sNQ5V3/bt21+JiHMKz6jZv4cx3GXevHlRySOPPFKxvUzKXqPrq13Za3R9tSt7jUPVB2wL/zaRmZmNhMPAzMwcBmZm5jAwMzMcBmZmhsPAzMxwGJiZGQ4DMzPDYWBmZhT4OQpJvwY8SvYHJKcAfxsRX5I0mexPxGcAvcB/iIiDaZpbgOvJ/q3qjyLie6l9Htnf+00AHgI+n74pZ/a2M6NzE8vnHmNp56aq/XpX/m6DKjIbvSJbBkeBj0bEB8j+MWmhpAVAJ7AlImYCW9JtJM0CFgOzgYXAHZJO/HH4amAZMDNdFtbvrpiZ2WgNGwbpZy76081T0yWARcC61L4OuCoNLwK6IuJoROwm+6vA+ZKmAGdGxGNpa+De3DRmZtZEhf72Mn2y3w68D/hGRNws6fWIOCvX52BETJJ0O7A1Itan9rXAZrJdSSsj4tLUfjFwc0RcWWF5y8i2IGhtbZ3X1TX4/777+/tpaWkZ4d1trLLX6Ppq07P3EK0ToO/N6v3mTm3eXw6XfR2WvT4of41D1dfR0bE9ItqKzqfQT1hH9sfaF0o6C7hf0pwq3VVpFlXaKy1vDbAGoK2tLdrb2wf16e7uplJ7mZS9RtdXm6XpmMGqnuovo95r2xtTUAVlX4dlrw/KX2O96hvR2UQR8TrQTbavvy/t+iFdH0jd9gDTc5NNA/al9mkV2s3MrMmGDQNJ56QtAiRNAC4FfgpsBJakbkuAB9LwRmCxpPGSzic7UPx4ROwHjkhaIEnAdblpzMysiYrsJpoCrEvHDd4FbIiIByU9BmyQdD3wEnA1QETskLQBeBY4BtyYdjMB3MCvTi3dnC5mZtZkw4ZBRDwN/HaF9leBS4aYZgWwokL7NqDa8QYzM2sCfwPZzMwcBmZm5jAwMzMcBmZmhsPAzMxwGJiZGQ4DMzPDYWBmZjgMzMwMh4GZmeEwMDMzHAZmZobDwMzMcBiYmRkOAzMzw2FgZmY4DMzMDIeBmZnhMDAzMxwGZmaGw8DMzHAYmJkZDgMzM8NhYGZmOAzMzIwCYSBpuqRHJO2UtEPS51P7rZL2SnoyXa7ITXOLpF2SnpN0Wa59nqSeNO42SRqbu2VmZiNxSoE+x4DlEfGEpDOA7ZIeTuO+HhF/ke8saRawGJgNnAf8QNJvRsRxYDWwDNgKPAQsBDbX566YmdloDbtlEBH7I+KJNHwE2AlMrTLJIqArIo5GxG5gFzBf0hTgzIh4LCICuBe4qtY7YGZmtVP2vlywszQDeBSYA/wxsBQ4DGwj23o4KOl2YGtErE/TrCX79N8LrIyIS1P7xcDNEXFlheUsI9uCoLW1dV5XV9egWvr7+2lpaSlcezOUvUbXV5uevYdonQB9b1bvN3fqxMYUVEHZ12HZ64Py1zhUfR0dHdsjoq3ofIrsJgJAUgvwbeCmiDgsaTXwFSDS9Srg00Cl4wBRpX1wY8QaYA1AW1tbtLe3D+rT3d1NpfYyKXuNrq82Szs3sXzuMVb1VH8Z9V7b3piCKij7Oix7fVD+GutVX6GziSSdShYE34yI7wBERF9EHI+It4A7gfmp+x5gem7yacC+1D6tQruZmTVZkbOJBKwFdkbE13LtU3LdPgk8k4Y3AosljZd0PjATeDwi9gNHJC1I87wOeKBO98PMzGpQZDfRRcCngB5JT6a2LwDXSLqQbFdPL/BZgIjYIWkD8CzZmUg3pjOJAG4A7gEmkB1H8JlEZmYlMGwYRMQ/UHl//0NVplkBrKjQvo3s4LOZmZWIv4FsZmYOAzMzcxiYmRkOAzMzw2FgZmY4DMzMDIeBmZnhMDAzMxwGZmaGw8DMzHAYmJkZDgMzM8NhYGZmOAzMzAyHgZmZ4TAwMzMcBmZmhsPAzMxwGJiZGQ4DMzPDYWBmZjgMzMwMh4GZmeEwMDMzHAZmZkaBMJA0XdIjknZK2iHp86l9sqSHJT2friflprlF0i5Jz0m6LNc+T1JPGnebJI3N3TIzs5EosmVwDFgeEe8HFgA3SpoFdAJbImImsCXdJo1bDMwGFgJ3SBqX5rUaWAbMTJeFdbwvZmY2SsOGQUTsj4gn0vARYCcwFVgErEvd1gFXpeFFQFdEHI2I3cAuYL6kKcCZEfFYRARwb24aMzNrImXvywU7SzOAR4E5wEsRcVZu3MGImCTpdmBrRKxP7WuBzUAvsDIiLk3tFwM3R8SVFZazjGwLgtbW1nldXV2Daunv76elpaVw7c1Q9hpdX2169h6idQL0vVm939ypExtTUAVlX4dlrw/KX+NQ9XV0dGyPiLai8zmlaEdJLcC3gZsi4nCV3f2VRkSV9sGNEWuANQBtbW3R3t4+qE93dzeV2suk7DW6vtos7dzE8rnHWNVT/WXUe217YwqqoOzrsOz1QflrrFd9hc4mknQqWRB8MyK+k5r70q4f0vWB1L4HmJ6bfBqwL7VPq9BuZmZNVuRsIgFrgZ0R8bXcqI3AkjS8BHgg175Y0nhJ55MdKH48IvYDRyQtSPO8LjeNmZk1UZHdRBcBnwJ6JD2Z2r4ArAQ2SLoeeAm4GiAidkjaADxLdibSjRFxPE13A3APMIHsOMLm+twNMzOrxbBhEBH/QOX9/QCXDDHNCmBFhfZtZAefzcysRPwNZDMzcxiYmZnDwMzMcBiYmRkOAzMzw2FgZmY4DMzMDIeBmZnhMDAzMxwGZmaGw8DMzHAYmJkZDgMzM8NhYGZmOAzMzAyHgZmZ4TAwMzMcBmZmhsPAzMxwGJiZGQ4DMzPDYWBmZjgMzMwMh4GZmeEwMDMzCoSBpLslHZD0TK7tVkl7JT2ZLlfkxt0iaZek5yRdlmufJ6knjbtNkup/d8zMbDSKbBncAyys0P71iLgwXR4CkDQLWAzMTtPcIWlc6r8aWAbMTJdK8zQzsyYYNgwi4lHgtYLzWwR0RcTRiNgN7ALmS5oCnBkRj0VEAPcCV42yZjMzqzNl783DdJJmAA9GxJx0+1ZgKXAY2AYsj4iDkm4HtkbE+tRvLbAZ6AVWRsSlqf1i4OaIuHKI5S0j24qgtbV1XldX16A+/f39tLS0jOCuNl7Za3R9tenZe4jWCdD3ZvV+c6dObExBFZR9HZa9Pih/jUPV19HRsT0i2orO55RRLn818BUg0vUq4NNApeMAUaW9oohYA6wBaGtri/b29kF9uru7qdReJmWv0fXVZmnnJpbPPcaqnuovo95r2xtTUAVlX4dlrw/KX2O96hvV2UQR0RcRxyPiLeBOYH4atQeYnus6DdiX2qdVaDczsxIYVRikYwAnfBI4cabRRmCxpPGSzic7UPx4ROwHjkhakM4iug54oIa6zcysjobdTSTpPqAdOFvSHuBLQLukC8l29fQCnwWIiB2SNgDPAseAGyPieJrVDWRnJk0gO46wuY73w8zMajBsGETENRWa11bpvwJYUaF9GzBnRNWZmVlD+BvIZmbmMDAzM4eBmZnhMDAzMxwGZmaGw8DMzHAYmJkZDgMzM8NhYGZmOAzMzAyHgZmZ4TAwMzMcBmZmhsPAzMxwGJiZGQ4DMzPDYWBmZjgMzMwMh4GZmeEwMDMzHAZmZobDwMzMcBiYmRkOAzMzw2FgZmYUCANJd0s6IOmZXNtkSQ9Lej5dT8qNu0XSLknPSbos1z5PUk8ad5sk1f/umJnZaBTZMrgHWDigrRPYEhEzgS3pNpJmAYuB2WmaOySNS9OsBpYBM9Nl4DzNzKxJhg2DiHgUeG1A8yJgXRpeB1yVa++KiKMRsRvYBcyXNAU4MyIei4gA7s1NY2ZmTabsvXmYTtIM4MGImJNuvx4RZ+XGH4yISZJuB7ZGxPrUvhbYDPQCKyPi0tR+MXBzRFw5xPKWkW1F0NraOq+rq2tQn/7+flpaWorf0yYoe42urzY9ew/ROgH63qzeb+7UiY0pqIKyr8Oy1wflr3Go+jo6OrZHRFvR+ZxS16qg0nGAqNJeUUSsAdYAtLW1RXt7+6A+3d3dVGovk7LX6Ppqs7RzE8vnHmNVT/WXUe+17Y0pqIKyr8Oy1wflr7Fe9Y32bKK+tOuHdH0gte8Bpuf6TQP2pfZpFdrNzKwERhsGG4ElaXgJ8ECufbGk8ZLOJztQ/HhE7AeOSFqQziK6LjeNmZk12bC7iSTdB7QDZ0vaA3wJWAlskHQ98BJwNUBE7JC0AXgWOAbcGBHH06xuIDszaQLZcYTNdb0nZmY2asOGQURcM8SoS4bovwJYUaF9GzBnRNWZmVlD+BvIZmbmMDAzM4eBmZnhMDAzMxwGZmaGw8DMzHAYmJkZDgMzM8NhYGZmOAzMzAyHgZmZ4TAwMzMcBmZmhsPAzMxwGJiZGQ4DMzPDYWBmZjgMzMwMh4GZmeEwMDMzHAZmZobDwMzMcBiYmRkOAzMzw2FgZmbUGAaSeiX1SHpS0rbUNlnSw5KeT9eTcv1vkbRL0nOSLqu1eDMzq496bBl0RMSFEdGWbncCWyJiJrAl3UbSLGAxMBtYCNwhaVwdlm9mZjUai91Ei4B1aXgdcFWuvSsijkbEbmAXMH8Mlm9mZiOkiBj9xNJu4CAQwF9HxBpJr0fEWbk+ByNikqTbga0RsT61rwU2R8TfVpjvMmAZQGtr67yurq5By+7v76elpWXUtTdC2Wt0fbXp2XuI1gnQ92b1fnOnTmxMQRWUfR2WvT4of41D1dfR0bE9t8dmWKfUWMdFEbFP0rnAw5J+WqWvKrRVTKKIWAOsAWhra4v29vZBfbq7u6nUXiZlr9H11WZp5yaWzz3Gqp7qL6Pea9sbU1AFZV+HZa8Pyl9jveqraTdRROxL1weA+8l2+/RJmgKQrg+k7nuA6bnJpwH7alm+mZnVx6jDQNLpks44MQx8DHgG2AgsSd2WAA+k4Y3AYknjJZ0PzAQeH+3yzcysfmrZTdQK3C/pxHy+FRH/R9JPgA2SrgdeAq4GiIgdkjYAzwLHgBsj4nhN1ZuZWV2MOgwi4kXgAxXaXwUuGWKaFcCK0S7TzMzGhr+BbGZmDgMzM3MYmJkZtX/PwMxszM3o3FS4b+/K3x3DSt65vGVgZmbeMjCz+iv6Sf6ehaePcSVWlLcMzMzMYWBmZg4DMzPDYWBmZjgMzMwMh4GZmeFTS82M4qeC+gtd71zeMjAzM28ZWHUj+RmAIvzJ0qycHAZm1jQ9ew+xtM4fOGx0HAbvMN73a2aj4TA4Sc3o3MTyucf8qcxGpN67Da08HAZmZlUU3ZX1dt/adhiY2Ump6FbO8rljXEhJOAzM3sFOxgO03pU1Og6Dtwk/wWtX9oPrY/EYnyyfasug7M+v4TgMrKFG8oZX9I9Pyh6UZa/PDBwGVmIn4y4Os2ZxGIzASD/h+dTNtyd/kreTUcPDQNJC4C+BccBdEbGy0TUM5Be/mTVKWY8tNPSH6iSNA74BXA7MAq6RNKuRNZiZ2WCN3jKYD+yKiBcBJHUBi4Bnx2Jh/sRvZlaMIqJxC5N+D1gYEZ9Jtz8FfCgiPjeg3zJgWbp5AfBchdmdDbwyhuXWQ9lrdH21K3uNrq92Za9xqPreExHnFJ1Jo7cMVKFtUBpFxBpgTdUZSdsioq1ehY2Fstfo+mpX9hpdX+3KXmO96mv0n9vsAabnbk8D9jW4BjMzG6DRYfATYKak8yWdBiwGNja4BjMzG6Chu4ki4pikzwHfIzu19O6I2DHK2VXdjVQSZa/R9dWu7DW6vtqVvca61NfQA8hmZlZOjd5NZGZmJeQwMDOzcoeBpKsl7ZD0lqQhT52StFDSc5J2SerMtU+W9LCk59P1pDrXN+z8JV0g6cnc5bCkm9K4WyXtzY27op71Fa0x9euV1JPq2DbS6ceyPknTJT0iaWd6Pnw+N25M1uFQz6nceEm6LY1/WtIHi07boPquTXU9LelHkj6QG1fxsW5Cje2SDuUeuz8tOm2D6vvPudqekXRc0uQ0bszXoaS7JR2Q9MwQ4+v7HIyI0l6A95N96awbaBuizzjgBeC9wGnAU8CsNO7Pgc403An8WZ3rG9H8U60/J/syCMCtwJ+M8TosVCPQC5xd630ci/qAKcAH0/AZwD/mHuO6r8Nqz6lcnyuAzWTfnVkA/LjotA2q78PApDR8+Yn6qj3WTaixHXhwNNM2or4B/T8O/H2D1+G/Az4IPDPE+Lo+B0u9ZRAROyOi0reP8/7lJy4i4hfAiZ+4IF2vS8PrgKvqXOJI538J8EJE/KzOdVRT6zpo+jqMiP0R8UQaPgLsBKbWuY68as+pExYB90ZmK3CWpCkFpx3z+iLiRxFxMN3cSvadnkaqZT2UYh0OcA1wX51rqCoiHgVeq9Klrs/BUodBQVOBl3O39/CrN4rWiNgP2RsKcG6dlz3S+S9m8BPqc2kT7+5674IZYY0BfF/SdmU/BzLS6ce6PgAkzQB+G/hxrrne67Dac2q4PkWmbUR9edeTfYI8YajHup6K1vhvJT0labOk2SOcthH1IendwELg27nmRqzD4dT1Odj0/zOQ9APg1yuM+mJEPFBkFhXa6na+bLX6Rjif04BPALfkmlcDXyGr9yvAKuDTTarxoojYJ+lc4GFJP02fTGpWx3XYQvaCvCkiDqfmuqzDgYuq0DbwOTVUnzF9Pg6z7MEdpQ6yMPhIrnnMHusR1vgE2S7T/nSs57vAzILT1moky/g48MOIyH9Kb8Q6HE5dn4NND4OIuLTGWVT7iYs+SVMiYn/afDpQz/okjWT+lwNPRERfbt7/MizpTuDBkdZXrxojYl+6PiDpfrJNzUcpyTqUdCpZEHwzIr6Tm3dd1uEARX42Zag+pxWYthH1IenfAHcBl0fEqyfaqzzWDa0xF+hExEOS7pB0dpFpG1FfzqAt+gatw+HU9Tn4TthNVO0nLjYCS9LwEqDIlsZIjGT+g/Y5pje/Ez4JVDxroEbD1ijpdElnnBgGPparpenrUJKAtcDOiPjagHFjsQ6L/GzKRuC6dEbHAuBQ2s3ViJ9cGXYZkn4D+A7wqYj4x1x7tce60TX+enpskTSf7P3o1SLTNqK+VNdE4N+Te142cB0Op77PwbE8Gl7rhezFvQc4CvQB30vt5wEP5fpdQXaGyQtku5dOtP8rYAvwfLqeXOf6Ks6/Qn3vJnuSTxww/f8CeoCn04M1ZQzW4bA1kp118FS67CjbOiTbxRFpPT2ZLleM5Tqs9JwC/hD4wzQssj9qeiEtv63atGPwuA5X313Awdz62jbcY92EGj+XaniK7CD3h8u0DtPtpUDXgOkasg7JPjzuB35J9j54/Vg+B/1zFGZm9o7YTWRmZjVyGJiZmcPAzMwcBmZmhsPAzMxwGJiZGQ4DMzMD/j/B9bfZ8W4laQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data.hist(column='sentimentScore', bins = 20)\n",
    "fake_tweets.hist(column='sentimentScore', bins = 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d39fdd2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'sentimentScore'}>]], dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data[train_data['label'] == 'real'].hist(column='sentimentScore', bins = 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e740c72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>sentimentScore</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>262987536568311808</td>\n",
       "      <td>#hurricane #sandy</td>\n",
       "      <td>288927110</td>\n",
       "      <td>sandyA_fake_25</td>\n",
       "      <td>djenna_</td>\n",
       "      <td>Mon Oct 29 18:41:29 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>18</td>\n",
       "      <td>hurricane sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>262996943909822464</td>\n",
       "      <td>#sandy</td>\n",
       "      <td>443420281</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>leah9702</td>\n",
       "      <td>Mon Oct 29 19:18:52 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>7</td>\n",
       "      <td>sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>263001118928420864</td>\n",
       "      <td>#Sandy</td>\n",
       "      <td>84682265</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>javi1978</td>\n",
       "      <td>Mon Oct 29 19:35:27 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>7</td>\n",
       "      <td>sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>836</th>\n",
       "      <td>263210772228894720</td>\n",
       "      <td>#sandy #newyork</td>\n",
       "      <td>19613104</td>\n",
       "      <td>sandyA_fake_23</td>\n",
       "      <td>Ferriter</td>\n",
       "      <td>Tue Oct 30 09:28:32 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>16</td>\n",
       "      <td>sandy newyork</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>856</th>\n",
       "      <td>263195243514580992</td>\n",
       "      <td>–ê–∫—É–ª—ã –Ω–∞ —É–ª–∏—Ü–∞—Ö –ø–æ—Å–ª–µ –°—ç–Ω–¥–∏ #sandy</td>\n",
       "      <td>60127942</td>\n",
       "      <td>sandyA_fake_11</td>\n",
       "      <td>nhodakov</td>\n",
       "      <td>Tue Oct 30 08:26:50 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>35</td>\n",
       "      <td>sharks streets sandy sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1794</th>\n",
       "      <td>263254634792120320</td>\n",
       "      <td>A shark was photographed swimming in the front yard of a flooded home in Brigantine Beach, New Jersey #sandy pic:</td>\n",
       "      <td>153014664</td>\n",
       "      <td>sandyA_fake_11</td>\n",
       "      <td>Franklinsh</td>\n",
       "      <td>Tue Oct 30 12:22:50 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>114</td>\n",
       "      <td>shark photographed swimming front yard flooded home brigantine beach new jersey sandy pic</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1927</th>\n",
       "      <td>263096083515842560</td>\n",
       "      <td>El hurac√°n Sandy llev√≥ un tibur√≥n hasta las calles de New Jersey.</td>\n",
       "      <td>177770821</td>\n",
       "      <td>sandyA_fake_12</td>\n",
       "      <td>receballos</td>\n",
       "      <td>Tue Oct 30 01:52:49 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>66</td>\n",
       "      <td>Hurricane Sandy carried shark onto streets New Jersey</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>262965326428401664</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>92343366</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>DaDaDiesel</td>\n",
       "      <td>Mon Oct 29 17:13:14 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>6</td>\n",
       "      <td>sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2377</th>\n",
       "      <td>264358713643646976</td>\n",
       "      <td>Dos tiburones en el metro de New Jersey, arrastrados por el Hurac√°n Sandy. Flipante.</td>\n",
       "      <td>171526762</td>\n",
       "      <td>sandyB_fake_08</td>\n",
       "      <td>AlvaroLozano15</td>\n",
       "      <td>Fri Nov 02 13:30:03 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>85</td>\n",
       "      <td>two sharks new jersey subway washed away freaks hurricane sandy</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2461</th>\n",
       "      <td>264228445679124481</td>\n",
       "      <td>Obama tells marines they don't have to guard the Tomb of the Unknown Soldier due to Hurricane Sandy, they refuse. #usa</td>\n",
       "      <td>48785301</td>\n",
       "      <td>sandyA_fake_01</td>\n",
       "      <td>rockinridgway</td>\n",
       "      <td>Fri Nov 02 04:52:25 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>119</td>\n",
       "      <td>obama tells marines dont guard tomb unknown soldier due hurricane sandy refuse usa</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2898</th>\n",
       "      <td>263857138160517122</td>\n",
       "      <td>Even as Hurricane Sandy makes landfall, these men are still standing guard at the tomb of The Unknown Soldier‚ô• #Respect</td>\n",
       "      <td>487776267</td>\n",
       "      <td>sandyA_fake_03</td>\n",
       "      <td>madtownpenning</td>\n",
       "      <td>Thu Nov 01 04:16:58 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>120</td>\n",
       "      <td>even hurricane sandy makes landfall men still standing guard tomb unknown soldier respect</td>\n",
       "      <td>0.4767</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2988</th>\n",
       "      <td>264782302473756672</td>\n",
       "      <td>Hurricane sandy</td>\n",
       "      <td>406892438</td>\n",
       "      <td>sandyA_fake_08</td>\n",
       "      <td>TurdleTBG__OG</td>\n",
       "      <td>Sat Nov 03 17:33:15 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>16</td>\n",
       "      <td>hurricane sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2994</th>\n",
       "      <td>264022361672667136</td>\n",
       "      <td>Hurricane Sandy</td>\n",
       "      <td>124456869</td>\n",
       "      <td>sandyA_fake_07</td>\n",
       "      <td>TheeFifi</td>\n",
       "      <td>Thu Nov 01 15:13:30 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>16</td>\n",
       "      <td>hurricane sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3034</th>\n",
       "      <td>264828735499411456</td>\n",
       "      <td>Sandy.</td>\n",
       "      <td>312842712</td>\n",
       "      <td>sandyA_fake_08</td>\n",
       "      <td>Anotherfrikee_</td>\n",
       "      <td>Sat Nov 03 20:37:45 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>7</td>\n",
       "      <td>sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3052</th>\n",
       "      <td>265216895475650561</td>\n",
       "      <td>Seguro q nadie ha visto la cara de Sandy !</td>\n",
       "      <td>535632979</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>Fara_Frica</td>\n",
       "      <td>Sun Nov 04 22:20:10 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>43</td>\n",
       "      <td>surely one seen sandy's face</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3112</th>\n",
       "      <td>265253150582267905</td>\n",
       "      <td>AMAZING photo from New York, night before Sandy hit, looks like something you'd see out of the movie Day After Tomorrow</td>\n",
       "      <td>454578295</td>\n",
       "      <td>sandyA_fake_08</td>\n",
       "      <td>CellyHardAppare</td>\n",
       "      <td>Mon Nov 05 00:44:14 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>120</td>\n",
       "      <td>amazing photo new york night sandy hit looks like something you see movie day tomorrow</td>\n",
       "      <td>0.7430</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3271</th>\n",
       "      <td>264565002286010368</td>\n",
       "      <td>@pandazambrano  Mira pandita como se refugian con la llegada del huracan sandy en new york</td>\n",
       "      <td>314304684</td>\n",
       "      <td>sandyA_fake_41</td>\n",
       "      <td>rafita_mex</td>\n",
       "      <td>Sat Nov 03 03:09:46 +0000 2012</td>\n",
       "      <td>humor</td>\n",
       "      <td>91</td>\n",
       "      <td>pandazambrano watches pandora take refuge arrival hurricane sandy new york</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3447</th>\n",
       "      <td>265230038356611072</td>\n",
       "      <td>Liberty stands up to #sandy. Picture symbolizes that we have have to stand tall even when we are battered &amp;amp; bruised.</td>\n",
       "      <td>318121829</td>\n",
       "      <td>sandyA_fake_07</td>\n",
       "      <td>FdnyChief</td>\n",
       "      <td>Sun Nov 04 23:12:23 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>121</td>\n",
       "      <td>liberty stands sandy picture symbolizes stand tall even battered bruised</td>\n",
       "      <td>0.5267</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3500</th>\n",
       "      <td>264458337100365825</td>\n",
       "      <td>When Mans Best Friend Needs a Lift. Photo from #Sandy 's Wake</td>\n",
       "      <td>485124571</td>\n",
       "      <td>sandyA_fake_21</td>\n",
       "      <td>JustSayinApp</td>\n",
       "      <td>Fri Nov 02 20:05:55 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>62</td>\n",
       "      <td>mans best friend needs lift photo sandy wake</td>\n",
       "      <td>0.8126</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3522</th>\n",
       "      <td>263792467961344000</td>\n",
       "      <td>Espectacular foto de #Sandy \\n#scary</td>\n",
       "      <td>574626413</td>\n",
       "      <td>sandyA_fake_08</td>\n",
       "      <td>GerardoUGR</td>\n",
       "      <td>Thu Nov 01 00:00:00 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>37</td>\n",
       "      <td>spectacular photo sandy scary</td>\n",
       "      <td>-0.4939</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>263942157914763264</td>\n",
       "      <td>#RT Si tu soutiens les New-Yorkais ‚ô•. #Sandy</td>\n",
       "      <td>704812087</td>\n",
       "      <td>sandyA_fake_08</td>\n",
       "      <td>hazzalious</td>\n",
       "      <td>Thu Nov 01 09:54:48 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>45</td>\n",
       "      <td>rt support new workers sandy</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>265214754459623426</td>\n",
       "      <td>LMFAO!!! üòÇ #Sandy</td>\n",
       "      <td>319754864</td>\n",
       "      <td>sandyA_fake_40</td>\n",
       "      <td>CLOUT_drips</td>\n",
       "      <td>Sun Nov 04 22:11:39 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>18</td>\n",
       "      <td>mfa sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3780</th>\n",
       "      <td>263834944684564480</td>\n",
       "      <td>„Åì„Çå„ÇÇ„Ç¨„Çª„ÄÇ„Çà„ÅèË¶ã„Çã„Å®ÂÅ¥Ê∫ù„Å´Ê∞¥„Åå„Åü„Åæ„Å£„Å¶„Çãwww  RT mokumura: „Åì„Çå„ÅØ‰ø°„Åò„Çâ„Çå„Å™„ÅÑ„ÄÅNY„ÅÆÂú∞‰∏ãÈâÑ„ÅÆÈßÖ„ÅåÂÆåÂÖ®„Å´Ê∞¥Ê≤°„Åß„ÄÅÊΩúÊ∞¥Â£´„ÅåÊΩú„Å£„Å¶„ÄÅÊ§úÊüª„ÄÇÂæ©Êóß„Å´„ÅØÊï∞ÈÄ±Èñì</td>\n",
       "      <td>273721228</td>\n",
       "      <td>sandyA_fake_16</td>\n",
       "      <td>wolverinetw</td>\n",
       "      <td>Thu Nov 01 02:48:47 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>84</td>\n",
       "      <td>If look closely too, water accumulated cutters www rt mokumura This incredible ny subway station completely submerged diver dives takes several weeks recover inspection</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3811</th>\n",
       "      <td>263999133231247360</td>\n",
       "      <td>Ara que Sandy ha passat ya puc eixir? #humorenvalencia</td>\n",
       "      <td>374711785</td>\n",
       "      <td>sandyA_fake_41</td>\n",
       "      <td>naneta_sanju</td>\n",
       "      <td>Thu Nov 01 13:41:12 +0000 2012</td>\n",
       "      <td>humor</td>\n",
       "      <td>55</td>\n",
       "      <td>sandy get humor</td>\n",
       "      <td>0.2732</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4258</th>\n",
       "      <td>263062482002776064</td>\n",
       "      <td>ÿπÿßÿ¨ŸÑ : ÿ™ŸÖ Ÿáÿ±Ÿàÿ® ŸàÿßÿÆÿ™ÿ®ÿßÿ° ÿ™ŸÖÿ´ÿßŸÑ ÿßŸÑÿ≠ÿ±ŸäŸá ŸÖŸÜ ÿßŸÑÿßÿπÿµÿßÿ± #ÿ≥ÿßŸÜÿØŸä #sandy</td>\n",
       "      <td>606112876</td>\n",
       "      <td>sandyA_fake_42</td>\n",
       "      <td>a9665</td>\n",
       "      <td>Mon Oct 29 23:39:17 +0000 2012</td>\n",
       "      <td>humor</td>\n",
       "      <td>61</td>\n",
       "      <td>Urgent The Statue Liberty escaped hid Hurricane Sandy</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5582</th>\n",
       "      <td>263144279512346624</td>\n",
       "      <td>#hurricanesandy</td>\n",
       "      <td>288035344</td>\n",
       "      <td>sandyA_fake_40</td>\n",
       "      <td>djkiddchris</td>\n",
       "      <td>Tue Oct 30 05:04:19 +0000 2012</td>\n",
       "      <td>humor</td>\n",
       "      <td>16</td>\n",
       "      <td>hurricanesandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5591</th>\n",
       "      <td>263291954165600257</td>\n",
       "      <td>#HurricaneSandy</td>\n",
       "      <td>381485241</td>\n",
       "      <td>sandyA_fake_42</td>\n",
       "      <td>K5k5Vip</td>\n",
       "      <td>Tue Oct 30 14:51:08 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>16</td>\n",
       "      <td>hurricanesandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9634</th>\n",
       "      <td>265108375434563584</td>\n",
       "      <td>RT @stephgosk: A mob of marathoners heading over to Staten Island. No race They're going to help out instead. #sandy</td>\n",
       "      <td>47742491</td>\n",
       "      <td>sandyB_real_41</td>\n",
       "      <td>triguy58</td>\n",
       "      <td>Sun Nov 04 15:08:56 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>117</td>\n",
       "      <td>rt stephgosk mob marathons heading state island race theyre going help instead sandy</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9701</th>\n",
       "      <td>265136484909211648</td>\n",
       "      <td>Pretty incredible. If you haven't yet- check out this week's @NYMag cover photo #Sandy</td>\n",
       "      <td>37782983</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>mustafamohammed</td>\n",
       "      <td>Sun Nov 04 17:00:38 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>87</td>\n",
       "      <td>pretty incredible havent yet check weeks nymag cover photo sandy</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9778</th>\n",
       "      <td>264655125237342208</td>\n",
       "      <td>‚Äú@GovChristie: Talking with Pres. Obama about needs in #NJ at the ROIC Friday evening. #Sandy</td>\n",
       "      <td>21974380</td>\n",
       "      <td>sandyB_real_94</td>\n",
       "      <td>MsNatTurner</td>\n",
       "      <td>Sat Nov 03 09:07:53 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>94</td>\n",
       "      <td>govchristie talking pres obama needs nj ric friday evening sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10090</th>\n",
       "      <td>263886148835885056</td>\n",
       "      <td>This is...unreal. Essentially the only building with power in #NYC right now is the Empire State Building. #Sandy</td>\n",
       "      <td>511250040</td>\n",
       "      <td>sandyB_real_72</td>\n",
       "      <td>BiernerJuli</td>\n",
       "      <td>Thu Nov 01 06:12:15 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>114</td>\n",
       "      <td>surreal essentially building power nyc right empire state building sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10399</th>\n",
       "      <td>264927551896551424</td>\n",
       "      <td>Amazing cover from New York Magazine. A view of Manhattan from above when Sandy hit</td>\n",
       "      <td>217179938</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>jonathon_platt</td>\n",
       "      <td>Sun Nov 04 03:10:25 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>84</td>\n",
       "      <td>amazing cover new york magazine view manhattan sandy hit</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10401</th>\n",
       "      <td>264887801881952256</td>\n",
       "      <td>@thecoolhunter: Amazing cover from New York Magazine. A view of Manhattan from above when Sandy hit</td>\n",
       "      <td>108167385</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>mizmayette</td>\n",
       "      <td>Sun Nov 04 00:32:28 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>100</td>\n",
       "      <td>thecoolhunter amazing cover new york magazine view manhattan sandy hit</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10417</th>\n",
       "      <td>264874729947688960</td>\n",
       "      <td>‚Äú@thecoolhunter: Amazing cover from New York Magazine. A view of Manhattan from above when Sandy hit</td>\n",
       "      <td>260145762</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>mgbanbury</td>\n",
       "      <td>Sat Nov 03 23:40:31 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>101</td>\n",
       "      <td>thecoolhunter amazing cover new york magazine view manhattan sandy hit</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10453</th>\n",
       "      <td>265211886151294976</td>\n",
       "      <td>AMAZING PHOTO- MARATHON RUNNERS BOARDING STATEN ISLAND FERRY READY TO VOLUNTEER INSTEAD OF RUNNING. #SANDY #911BUFF</td>\n",
       "      <td>21114834</td>\n",
       "      <td>sandyB_real_20</td>\n",
       "      <td>RickJameswife</td>\n",
       "      <td>Sun Nov 04 22:00:15 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>116</td>\n",
       "      <td>amazing photo marathon runners boarding state island ferry ready volunteer instead running sandy buff</td>\n",
       "      <td>0.7430</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10556</th>\n",
       "      <td>265295361411776512</td>\n",
       "      <td>An amazing cover photo from NYMag:   (via @gruber)</td>\n",
       "      <td>240146380</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>nurulwidiaa</td>\n",
       "      <td>Mon Nov 05 03:31:57 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>51</td>\n",
       "      <td>amazing cover photo nymag via ruler</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10557</th>\n",
       "      <td>265189510487363584</td>\n",
       "      <td>Portad√≥n del @NYmag #Sandy #coverjunkie</td>\n",
       "      <td>373913383</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>Niniasur</td>\n",
       "      <td>Sun Nov 04 20:31:20 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>40</td>\n",
       "      <td>nymag sandy coverjunkie hatchback</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10559</th>\n",
       "      <td>264894243405955072</td>\n",
       "      <td>Remarkable #Sandy @nymag cover  (h/t @EthanKlapper via @lrozen)</td>\n",
       "      <td>205974971</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>joshuakanter</td>\n",
       "      <td>Sun Nov 04 00:58:03 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>63</td>\n",
       "      <td>remarkable sandy nymag cover ht ethanklapper via frozen</td>\n",
       "      <td>0.5574</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10560</th>\n",
       "      <td>264873441918869504</td>\n",
       "      <td>The @NYMag #Sandy NYC cover is definitely today's most passed around image.</td>\n",
       "      <td>14665204</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>warnerblaster</td>\n",
       "      <td>Sat Nov 03 23:35:24 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>76</td>\n",
       "      <td>nymag sandy nyc cover definitely todays passed around image</td>\n",
       "      <td>0.4019</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10567</th>\n",
       "      <td>264876384286031872</td>\n",
       "      <td>Stunning @NYMag Cover | #sandy #NYC</td>\n",
       "      <td>67021002</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>_aaisha</td>\n",
       "      <td>Sat Nov 03 23:47:05 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>37</td>\n",
       "      <td>stunning nymag cover sandy nyc</td>\n",
       "      <td>0.3818</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10568</th>\n",
       "      <td>264978602297282560</td>\n",
       "      <td>Surreal @NYMag  cover photo of post NYC #Sandy.</td>\n",
       "      <td>806252851</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>syahidah_xq</td>\n",
       "      <td>Sun Nov 04 06:33:16 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>48</td>\n",
       "      <td>surreal nymag cover photo post nyc sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10569</th>\n",
       "      <td>264831718312267777</td>\n",
       "      <td>I know everyone has retweeted it, but the @nymag front cover about Sandy is quite amazing:  #Sandy</td>\n",
       "      <td>19148866</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>sris22</td>\n",
       "      <td>Sat Nov 03 20:49:36 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>98</td>\n",
       "      <td>know everyone retreated nymag front cover sandy quite amazing sandy</td>\n",
       "      <td>0.6240</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10576</th>\n",
       "      <td>265084711712460800</td>\n",
       "      <td>.@NYMag's Incredible #Sandy Cover:</td>\n",
       "      <td>337500628</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>RoughlyHarmless</td>\n",
       "      <td>Sun Nov 04 13:34:54 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>35</td>\n",
       "      <td>nymag incredible sandy cover</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10585</th>\n",
       "      <td>264869760326574080</td>\n",
       "      <td>Amazing post #Sandy cover of @NYMag.</td>\n",
       "      <td>125663139</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>Pkjaer</td>\n",
       "      <td>Sat Nov 03 23:20:46 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>37</td>\n",
       "      <td>amazing post sandy cover nymag</td>\n",
       "      <td>0.5859</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10604</th>\n",
       "      <td>265115635636379649</td>\n",
       "      <td>Stunning #Sandy @NYmag cover of half-blacked-out NYC by Iwaan Ban</td>\n",
       "      <td>39750128</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>daphneedenis</td>\n",
       "      <td>Sun Nov 04 15:37:47 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>66</td>\n",
       "      <td>stunning sandy nymag cover halfblackedout nyc wasn ban</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10607</th>\n",
       "      <td>264918029018755073</td>\n",
       "      <td>Breathtaking @nymag cover this week.  @BuzzFeed #sandy</td>\n",
       "      <td>18092306</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>kaykas</td>\n",
       "      <td>Sun Nov 04 02:32:34 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>54</td>\n",
       "      <td>breathtaking nymag cover week buzzfeed sandy</td>\n",
       "      <td>0.4588</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10638</th>\n",
       "      <td>265141163210715136</td>\n",
       "      <td>Dramatic @NYMag cover photo shows the Manhattan power/no power dividing line. Taken Wednesday nite by Iwan Baan. #Sandy</td>\n",
       "      <td>47534846</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>chestnuthell</td>\n",
       "      <td>Sun Nov 04 17:19:14 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>120</td>\n",
       "      <td>dramatic nymag cover photo shows manhattan power power dividing line taken wednesday site ian ban sandy</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10653</th>\n",
       "      <td>264833886608035840</td>\n",
       "      <td>Wow, cover of new issue of @NYmag is amazing.</td>\n",
       "      <td>117293877</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>bfalkenhagen</td>\n",
       "      <td>Sat Nov 03 20:58:13 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>47</td>\n",
       "      <td>wow cover new issue nymag amazing</td>\n",
       "      <td>0.8225</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10819</th>\n",
       "      <td>265032174791700480</td>\n",
       "      <td>#GroundZero #NYC #SANDY</td>\n",
       "      <td>346128615</td>\n",
       "      <td>sandyA_real_02</td>\n",
       "      <td>effyfleur</td>\n",
       "      <td>Sun Nov 04 10:06:09 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>24</td>\n",
       "      <td>groundzero nyc sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10823</th>\n",
       "      <td>264711132344897537</td>\n",
       "      <td>Hoy como durante la ultima semana nuestros pensamientos y deseos d pronta recuperacion a todos los afectados x huracan</td>\n",
       "      <td>160683478</td>\n",
       "      <td>sandyA_real_18</td>\n",
       "      <td>Yurered</td>\n",
       "      <td>Sat Nov 03 12:50:26 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>119</td>\n",
       "      <td>today last week thoughts wishes speedy recovery affected hurricane</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10827</th>\n",
       "      <td>263848231371423744</td>\n",
       "      <td>PHOTO: Stunning before and after of Seaside Heights boardwalk area. #sandy #njwx #flood #hurricane #superstorm</td>\n",
       "      <td>71626076</td>\n",
       "      <td>sandyA_real_27</td>\n",
       "      <td>AJAndrew83</td>\n",
       "      <td>Thu Nov 01 03:41:35 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>111</td>\n",
       "      <td>photo stunning seaside heights boardwalk area sandy news flood hurricane superstar</td>\n",
       "      <td>0.3818</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10835</th>\n",
       "      <td>265235545616171008</td>\n",
       "      <td>God Bless Our First Responders #BreezyPoint #Sandy @FDNY</td>\n",
       "      <td>381297385</td>\n",
       "      <td>sandyA_real_15</td>\n",
       "      <td>MizLizW</td>\n",
       "      <td>Sun Nov 04 23:34:16 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>57</td>\n",
       "      <td>god bless first responders breezypoint sandy font</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10846</th>\n",
       "      <td>264111099467665409</td>\n",
       "      <td>A parking lot full of yellow cabs flooded in Hoboken. #Sandy</td>\n",
       "      <td>250268745</td>\n",
       "      <td>sandyA_real_18</td>\n",
       "      <td>Beltox3000</td>\n",
       "      <td>Thu Nov 01 21:06:07 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>61</td>\n",
       "      <td>parking lot full yellow cabs flooded hooked sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10849</th>\n",
       "      <td>264433583542902784</td>\n",
       "      <td>This is Breezy Point, Queens as seen by @TomKaminskiWCBS in Chopper 880 this morning.  #Sandy</td>\n",
       "      <td>50721239</td>\n",
       "      <td>sandyA_real_52</td>\n",
       "      <td>ninafrankel</td>\n",
       "      <td>Fri Nov 02 18:27:33 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>94</td>\n",
       "      <td>breeze point queens seen tomkaminskiwcbs chopper morning sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10875</th>\n",
       "      <td>263998797749817345</td>\n",
       "      <td>Regardez les images impressionnantes de l'ouragan #Sandy diffus√©es dans #Le13H de #TF1:</td>\n",
       "      <td>139028999</td>\n",
       "      <td>sandyA_real_36</td>\n",
       "      <td>FaraHxBenni</td>\n",
       "      <td>Thu Nov 01 13:39:52 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>89</td>\n",
       "      <td>watch impressive images hurricane sandy broadcast tf's p.m.</td>\n",
       "      <td>0.5106</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10909</th>\n",
       "      <td>263796636722032640</td>\n",
       "      <td>Flooding has turned Ground Zero into a waterfall. @AP photo #NYC #Sandy</td>\n",
       "      <td>76062485</td>\n",
       "      <td>sandyA_real_02</td>\n",
       "      <td>lkateyl13</td>\n",
       "      <td>Thu Nov 01 00:16:33 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>72</td>\n",
       "      <td>flooding turned ground zero waterfall ap photo nyc sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11061</th>\n",
       "      <td>262657561000542208</td>\n",
       "      <td>President Obama on Hurricane #Sandy: \"Take this very seriously\":  Photo @FEMA today:</td>\n",
       "      <td>36117822</td>\n",
       "      <td>sandyB_real_11</td>\n",
       "      <td>BishopJakes</td>\n",
       "      <td>Sun Oct 28 20:50:17 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>85</td>\n",
       "      <td>president obama hurricane sandy take seriously photo fea today</td>\n",
       "      <td>-0.1779</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11066</th>\n",
       "      <td>264176527846027265</td>\n",
       "      <td>‚Äú #generosity #Sandy</td>\n",
       "      <td>14096022</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>Tobinklinger</td>\n",
       "      <td>Fri Nov 02 01:26:07 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>21</td>\n",
       "      <td>generosity sandy</td>\n",
       "      <td>0.5106</td>\n",
       "      <td>36.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11067</th>\n",
       "      <td>264143391946854400</td>\n",
       "      <td>Dear Mainstream Media - Cuba is also affected by #Sandy  RT @AnonymouSkY ~ #MSN</td>\n",
       "      <td>161080029</td>\n",
       "      <td>sandyB_real_10</td>\n",
       "      <td>MeowMinako</td>\n",
       "      <td>Thu Nov 01 23:14:26 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>79</td>\n",
       "      <td>dear mainstream media cuba also affected sandy rt anonymously msn</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11073</th>\n",
       "      <td>264088551900200961</td>\n",
       "      <td>Bel exemple d'entraide aux Etats-Unis ! #Sandy</td>\n",
       "      <td>186927459</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>Flo___B</td>\n",
       "      <td>Thu Nov 01 19:36:31 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>47</td>\n",
       "      <td>good example mutual aid sandy united states</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11075</th>\n",
       "      <td>264327602381139968</td>\n",
       "      <td>Cuba exists too! #sandy #cuba</td>\n",
       "      <td>236946455</td>\n",
       "      <td>sandyB_real_10</td>\n",
       "      <td>fultygp</td>\n",
       "      <td>Fri Nov 02 11:26:26 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>30</td>\n",
       "      <td>cuba exists sandy cuba</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11076</th>\n",
       "      <td>264914868740685824</td>\n",
       "      <td>Weeks ago, #unions were thugs and freeloaders. Now, their out saving your ass. #Sandy</td>\n",
       "      <td>15973356</td>\n",
       "      <td>sandyB_real_10</td>\n",
       "      <td>trillium_mimi</td>\n",
       "      <td>Sun Nov 04 02:20:01 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>86</td>\n",
       "      <td>weeks ago unions thugs freeloaders saving ass sandy</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11077</th>\n",
       "      <td>264969912680869890</td>\n",
       "      <td>Mitt #Romney's awkward moment #Sandy #FEMA</td>\n",
       "      <td>447421787</td>\n",
       "      <td>sandyB_real_10</td>\n",
       "      <td>imparfaitliz</td>\n",
       "      <td>Sun Nov 04 05:58:44 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>43</td>\n",
       "      <td>mitt romney awkward moment sandy fea</td>\n",
       "      <td>-0.1531</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11078</th>\n",
       "      <td>263943810235650048</td>\n",
       "      <td>Dear mainstream media. From Cuba. #sandy</td>\n",
       "      <td>82375960</td>\n",
       "      <td>sandyB_real_10</td>\n",
       "      <td>mcksdwsk</td>\n",
       "      <td>Thu Nov 01 10:01:22 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>41</td>\n",
       "      <td>dear mainstream media cuba sandy</td>\n",
       "      <td>0.3818</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11079</th>\n",
       "      <td>265135119327719424</td>\n",
       "      <td>l'entraide entre les New-Yorkais apr√®s #Sandy n'est pas une vue de l'esprit ! Bravo !</td>\n",
       "      <td>18772778</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>iam_Gael</td>\n",
       "      <td>Sun Nov 04 16:55:13 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>86</td>\n",
       "      <td>mutual aid New Workers sandy sight mind bravo</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11081</th>\n",
       "      <td>264206565404471296</td>\n",
       "      <td>Passing a gas line in Sunset Park- 8... 9... 10 blocks... 11... 12.... 13... 14... 15... 16 blocks. OMFG. #sandy</td>\n",
       "      <td>52189673</td>\n",
       "      <td>sandyB_real_16</td>\n",
       "      <td>48thAve</td>\n",
       "      <td>Fri Nov 02 03:25:28 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>113</td>\n",
       "      <td>passing gas line sunset park blocks blocks omg sandy</td>\n",
       "      <td>-0.4215</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11085</th>\n",
       "      <td>264951702053060608</td>\n",
       "      <td>RT @TerynSpears: American Spirit  #Sandy #ILoveNY  // This pic says it all...</td>\n",
       "      <td>333139514</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>Paz_Ronald</td>\n",
       "      <td>Sun Nov 04 04:46:22 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>77</td>\n",
       "      <td>rt terynspears american spirit sandy lovely pic says</td>\n",
       "      <td>0.6705</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11087</th>\n",
       "      <td>264037206031753217</td>\n",
       "      <td>Now this is a TRUE Neighbor. #sandy</td>\n",
       "      <td>195205456</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>TheDaltonHicks</td>\n",
       "      <td>Thu Nov 01 16:12:30 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>36</td>\n",
       "      <td>true neighbor sandy</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11089</th>\n",
       "      <td>264018598174019585</td>\n",
       "      <td>Fressepreiheit! Cuba was fucked by #Sandy too. Btw.</td>\n",
       "      <td>284565051</td>\n",
       "      <td>sandyB_real_10</td>\n",
       "      <td>Atombyte</td>\n",
       "      <td>Thu Nov 01 14:58:33 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>52</td>\n",
       "      <td>fressepreiheit cuba sucked sandy btw</td>\n",
       "      <td>-0.4588</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11090</th>\n",
       "      <td>263978285359321088</td>\n",
       "      <td>SANDY HIT CUBA TOO #NYC #SANDY #CUBA</td>\n",
       "      <td>7532</td>\n",
       "      <td>sandyB_real_10</td>\n",
       "      <td>SupremeFactory</td>\n",
       "      <td>Thu Nov 01 12:18:22 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>37</td>\n",
       "      <td>sandy hit cuba nyc sandy cuba</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11091</th>\n",
       "      <td>264319766980861952</td>\n",
       "      <td>This restores my faith in humanity. After Hurricane #Sandy...</td>\n",
       "      <td>61647830</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>julienboisvert</td>\n",
       "      <td>Fri Nov 02 10:55:17 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>62</td>\n",
       "      <td>restores faith humanity hurricane sandy</td>\n",
       "      <td>0.6124</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11093</th>\n",
       "      <td>264111622488997888</td>\n",
       "      <td>Dear Mainstream Media - Cuba is also affected by #Sandy</td>\n",
       "      <td>259397789</td>\n",
       "      <td>sandyB_real_10</td>\n",
       "      <td>zwagdi</td>\n",
       "      <td>Thu Nov 01 21:08:12 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>56</td>\n",
       "      <td>dear mainstream media cuba also affected sandy</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>29.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11096</th>\n",
       "      <td>264075698753400832</td>\n",
       "      <td>Still some good folks out there #Sandy</td>\n",
       "      <td>45343255</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>xxlangyxx</td>\n",
       "      <td>Thu Nov 01 18:45:27 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>39</td>\n",
       "      <td>still good folks sandy</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11107</th>\n",
       "      <td>265093477682016256</td>\n",
       "      <td>Man named Ralph in Hoboken, NJ shows true humanity after getting power back. #goodpeople #Sandy Bless you, Ralph!</td>\n",
       "      <td>61669381</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>MNvksfn</td>\n",
       "      <td>Sun Nov 04 14:09:44 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>114</td>\n",
       "      <td>man named ralph hooked nj shows true humanity getting power back goodpeople sandy bless ralph</td>\n",
       "      <td>0.6808</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11109</th>\n",
       "      <td>265216626444599296</td>\n",
       "      <td>@AnonyOps @OccupyWallStNYC @OccupyWallSt @SuicideGirls @AnonIRC They say a picture tells a thousand words. #Sandy #NYC</td>\n",
       "      <td>63349852</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>malac0da13</td>\n",
       "      <td>Sun Nov 04 22:19:05 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>119</td>\n",
       "      <td>anonymous occupywallstnyc occupywallst suicidegirls anonirc say picture tells thousand words sandy nyc</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11110</th>\n",
       "      <td>264085604764119041</td>\n",
       "      <td>Humanity :) photo from New Jersey #Sandy</td>\n",
       "      <td>101061328</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>ManarHeikal</td>\n",
       "      <td>Thu Nov 01 19:24:49 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>41</td>\n",
       "      <td>humanity photo new jersey sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11118</th>\n",
       "      <td>264881899485421568</td>\n",
       "      <td>Whoah. RT @Belstaff: This picture says it all. #Sandy #newyorkcity</td>\n",
       "      <td>302136867</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>FaceofBalen</td>\n",
       "      <td>Sun Nov 04 00:09:00 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>67</td>\n",
       "      <td>which rt belstaff picture says sandy newyorkcity</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11119</th>\n",
       "      <td>264129504975392768</td>\n",
       "      <td>#Sandy on Cuba</td>\n",
       "      <td>142365279</td>\n",
       "      <td>sandyB_real_10</td>\n",
       "      <td>SucoRATM</td>\n",
       "      <td>Thu Nov 01 22:19:15 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>15</td>\n",
       "      <td>sandy cuba</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11120</th>\n",
       "      <td>264119013171417088</td>\n",
       "      <td>Queridos Medios de \"Comunicaci√≥n\", #Sandy tambi√©n nos ha jodido a nosotros. Atentamente, Cuba.</td>\n",
       "      <td>295746691</td>\n",
       "      <td>sandyB_real_10</td>\n",
       "      <td>albertoaran</td>\n",
       "      <td>Thu Nov 01 21:37:34 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>95</td>\n",
       "      <td>dear media sandy also sucked us attentive cuba</td>\n",
       "      <td>-0.1027</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11122</th>\n",
       "      <td>263835977255104514</td>\n",
       "      <td>This is so unreal. #OCMD #Sandy</td>\n",
       "      <td>270561518</td>\n",
       "      <td>sandyB_real_76</td>\n",
       "      <td>_namescarolinee</td>\n",
       "      <td>Thu Nov 01 02:52:53 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>32</td>\n",
       "      <td>unreal cmd sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11123</th>\n",
       "      <td>264436721159135232</td>\n",
       "      <td>Why individuals &amp;gt; government. #LetThemEatMarathons #Sandy</td>\n",
       "      <td>18377669</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>Skipppy</td>\n",
       "      <td>Fri Nov 02 18:40:01 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>61</td>\n",
       "      <td>individuals government letthemeatmarathons sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11130</th>\n",
       "      <td>264041447504900096</td>\n",
       "      <td>#Sandy Cuba.</td>\n",
       "      <td>256586206</td>\n",
       "      <td>sandyB_real_10</td>\n",
       "      <td>constantinos___</td>\n",
       "      <td>Thu Nov 01 16:29:21 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>13</td>\n",
       "      <td>sandy cuba</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11138</th>\n",
       "      <td>264090027024330753</td>\n",
       "      <td>C'mon Son!! #sandy</td>\n",
       "      <td>269002609</td>\n",
       "      <td>sandyB_real_6</td>\n",
       "      <td>StandGRANDyo</td>\n",
       "      <td>Thu Nov 01 19:42:23 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>19</td>\n",
       "      <td>con son sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11141</th>\n",
       "      <td>264776663005548544</td>\n",
       "      <td>Tears. Fireman saving dog caught in hurricane:  (Photo via @PrincessGwenie1) #Sandy</td>\n",
       "      <td>318395113</td>\n",
       "      <td>sandyB_real_38</td>\n",
       "      <td>wehave12dogs</td>\n",
       "      <td>Sat Nov 03 17:10:50 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>83</td>\n",
       "      <td>tears fireman saving dog caught hurricane photo via princessgwenie sandy</td>\n",
       "      <td>-0.2263</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11150</th>\n",
       "      <td>265040705632747520</td>\n",
       "      <td>@ecazatormentas Una imagen vale m√°s que mil palabras #Sandy</td>\n",
       "      <td>362940223</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>cule_3</td>\n",
       "      <td>Sun Nov 04 10:40:03 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>60</td>\n",
       "      <td>stormtrooper picture worth thousand words sandy</td>\n",
       "      <td>0.2263</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11161</th>\n",
       "      <td>264098587301011456</td>\n",
       "      <td>Hurricane A-rod #mlbmemes</td>\n",
       "      <td>915382603</td>\n",
       "      <td>sandyB_real_10</td>\n",
       "      <td>Gmacneal40</td>\n",
       "      <td>Thu Nov 01 20:16:24 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>26</td>\n",
       "      <td>hurricane prod mlbmemes</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11164</th>\n",
       "      <td>264083197585940480</td>\n",
       "      <td>Ils sont sympa ces new-yorkais (via @ppgarcia75 ) #Sandy</td>\n",
       "      <td>408360332</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>LovelyMeIody</td>\n",
       "      <td>Thu Nov 01 19:15:15 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>57</td>\n",
       "      <td>nice new workers via garcia sandy</td>\n",
       "      <td>0.4215</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11171</th>\n",
       "      <td>264491516167344128</td>\n",
       "      <td>Wow! Heartbreaking MT‚Äú@JohnSchriffen: Unbelievable scene flying over #StatenIsland in an #NYPD helicopter #Sandy @ABC</td>\n",
       "      <td>48548102</td>\n",
       "      <td>sandyB_real_56</td>\n",
       "      <td>mikey_shoe</td>\n",
       "      <td>Fri Nov 02 22:17:46 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>118</td>\n",
       "      <td>wow heartbreaking mtjohnschriffen unbelievable scene flying statenisland nypd helicopter sandy abc</td>\n",
       "      <td>0.3818</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11175</th>\n",
       "      <td>264918950104670209</td>\n",
       "      <td>This picture says it all. #Sandy #newyorkcity</td>\n",
       "      <td>473806784</td>\n",
       "      <td>sandyB_real_59</td>\n",
       "      <td>KakimanRe555555</td>\n",
       "      <td>Sun Nov 04 02:36:14 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>46</td>\n",
       "      <td>picture says sandy newyorkcity</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11176</th>\n",
       "      <td>264301525084540928</td>\n",
       "      <td>Im√°genes que no se ven en los medios. #Sandy tambi√©n azot√≥ Cuba, Jamaica, Hait√≠ y Rep√∫blica Dominicana</td>\n",
       "      <td>346720029</td>\n",
       "      <td>sandyB_real_22</td>\n",
       "      <td>BekiHerrero</td>\n",
       "      <td>Fri Nov 02 09:42:48 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>103</td>\n",
       "      <td>images seen media sandy also hit Cuba Jamaica Haiti Dominican Republic</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11190</th>\n",
       "      <td>264181736907370496</td>\n",
       "      <td>Proof of Humanity, spotted in NYC. #Sandy #socialgood</td>\n",
       "      <td>47342086</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>KellyHook</td>\n",
       "      <td>Fri Nov 02 01:46:48 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>54</td>\n",
       "      <td>proof humanity spotted nyc sandy socialgood</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11199</th>\n",
       "      <td>264086933377019904</td>\n",
       "      <td>Neighbors helping neighbors in Hoboken, NJ (via Facebook) #sandy</td>\n",
       "      <td>32576782</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>AmandaSueHill</td>\n",
       "      <td>Thu Nov 01 19:30:06 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>65</td>\n",
       "      <td>neighbors helping neighbors hooked nj via facebook sandy</td>\n",
       "      <td>0.2960</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11225</th>\n",
       "      <td>263951952147259392</td>\n",
       "      <td>National Guard near 25th and 3rd #Sandy</td>\n",
       "      <td>273162685</td>\n",
       "      <td>sandyB_real_46</td>\n",
       "      <td>coldronin</td>\n",
       "      <td>Thu Nov 01 10:33:44 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>40</td>\n",
       "      <td>national guard near th rd sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11230</th>\n",
       "      <td>263974428361248769</td>\n",
       "      <td>Remember when President Bush was on vacation during Hurricane Katrina? This is President Obama #p2</td>\n",
       "      <td>28699299</td>\n",
       "      <td>sandyB_real_11</td>\n",
       "      <td>zoellelovee</td>\n",
       "      <td>Thu Nov 01 12:03:02 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>99</td>\n",
       "      <td>remember president bush vacation hurricane katrina president obama p</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11257</th>\n",
       "      <td>264139804973879297</td>\n",
       "      <td>#newyorkers are resilient even in #sandy's aftermath. love this image</td>\n",
       "      <td>490062109</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>bustybeaver</td>\n",
       "      <td>Thu Nov 01 23:00:11 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>70</td>\n",
       "      <td>newyorker resilient even sandy aftermath love image</td>\n",
       "      <td>0.6369</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11275</th>\n",
       "      <td>263916546882154496</td>\n",
       "      <td>On parle de NY (o√π vit ma famille), mais piti√© n'oublions pas Ha√Øti, Cuba et la R√©publique Dominicaine.. #sandy</td>\n",
       "      <td>398619759</td>\n",
       "      <td>sandyB_real_22</td>\n",
       "      <td>ramina63</td>\n",
       "      <td>Thu Nov 01 08:13:02 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>112</td>\n",
       "      <td>talking ny family lives please let's forget haiti cuba dominican republic sandy</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11378</th>\n",
       "      <td>264072526831972353</td>\n",
       "      <td>Tutti parlano di #Sandy a NY , ecco i danni di #Sandy  nella repubblica Domenicana. #Sandy di cui non si parla</td>\n",
       "      <td>71053746</td>\n",
       "      <td>sandyB_real_22</td>\n",
       "      <td>Sandrosen1</td>\n",
       "      <td>Thu Nov 01 18:32:51 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>111</td>\n",
       "      <td>everyone talks sandy ny damage sandy dominican republic sandy mentioned</td>\n",
       "      <td>-0.4939</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11396</th>\n",
       "      <td>264489506030366720</td>\n",
       "      <td>Unbelievable scene flying over #StatenIsland in an #NYPD helicopter. Those are not little toys! #Sandy @ABC</td>\n",
       "      <td>65586007</td>\n",
       "      <td>sandyB_real_56</td>\n",
       "      <td>desertgardens</td>\n",
       "      <td>Fri Nov 02 22:09:46 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>108</td>\n",
       "      <td>unbelievable scene flying statenisland nypd helicopter little toys sandy abc</td>\n",
       "      <td>0.2023</td>\n",
       "      <td>42.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11456</th>\n",
       "      <td>264638134304768000</td>\n",
       "      <td>And generosity of spirit ... \\n#sandy</td>\n",
       "      <td>286852460</td>\n",
       "      <td>sandyB_real_71</td>\n",
       "      <td>S6SLG</td>\n",
       "      <td>Sat Nov 03 08:00:22 +0000 2012</td>\n",
       "      <td>real</td>\n",
       "      <td>38</td>\n",
       "      <td>generosity spirit sandy</td>\n",
       "      <td>0.6124</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12844</th>\n",
       "      <td>325008910694092800</td>\n",
       "      <td>#prayforboston #suspect #suspects #bostonsuspect #bostonsuspects #bostonbombingsuspectsŒ≤‚Ç¨¬¶</td>\n",
       "      <td>1059330391</td>\n",
       "      <td>boston_real_04</td>\n",
       "      <td>LisaAnneKiraly</td>\n",
       "      <td>Thu Apr 18 22:12:17 +0000 2013</td>\n",
       "      <td>real</td>\n",
       "      <td>91</td>\n",
       "      <td>prayforboston suspect suspects bostonsuspect bostonsuspects bostonbombingsuspectsŒ≤</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12852</th>\n",
       "      <td>510054087455277056</td>\n",
       "      <td>#ColumbianChemicals The information about chemical plant explosion received from the witnesses</td>\n",
       "      <td>2676589545</td>\n",
       "      <td>columbianChemicals_fake_08</td>\n",
       "      <td>Laura_1991sun</td>\n",
       "      <td>Thu Sep 11 13:15:45 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>95</td>\n",
       "      <td>columbianchemicals information chemical plant explosion received witnesses</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12876</th>\n",
       "      <td>510052210533601280</td>\n",
       "      <td>The explosion happened at the chemical plant located in Centerville, LA #ColumbianChemicals</td>\n",
       "      <td>2753584697</td>\n",
       "      <td>columbianChemicals_fake_03</td>\n",
       "      <td>MichaelJWill</td>\n",
       "      <td>Thu Sep 11 13:08:17 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>92</td>\n",
       "      <td>explosion happened chemical plant located centerville la columbianchemicals</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12889</th>\n",
       "      <td>510077381030457344</td>\n",
       "      <td>The information about plant explosion had been received from the witnesses #ColumbianChemicals</td>\n",
       "      <td>1647147420</td>\n",
       "      <td>columbianChemicals_fake_02</td>\n",
       "      <td>Trishanumba1</td>\n",
       "      <td>Thu Sep 11 14:48:19 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>95</td>\n",
       "      <td>information plant explosion received witnesses columbianchemicals</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13036</th>\n",
       "      <td>462173893093253120</td>\n",
       "      <td>Bringbackourgirls</td>\n",
       "      <td>1446530198</td>\n",
       "      <td>bringback_fake_07</td>\n",
       "      <td>e_adamu</td>\n",
       "      <td>Fri May 02 10:16:57 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>18</td>\n",
       "      <td>bringbackourgirls</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>19.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13046</th>\n",
       "      <td>462421890368540673</td>\n",
       "      <td>BringBackOurGirls</td>\n",
       "      <td>910999466</td>\n",
       "      <td>bringback_fake_07</td>\n",
       "      <td>JohnOnya</td>\n",
       "      <td>Sat May 03 02:42:24 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>18</td>\n",
       "      <td>bringbackourgirls</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13216</th>\n",
       "      <td>452995674456326144</td>\n",
       "      <td>Underwater bedroom at Poseidon Undersea Resort located in Fiji. Who wouldn't like to live in a place like this</td>\n",
       "      <td>601858422</td>\n",
       "      <td>underwater_fake_01</td>\n",
       "      <td>MonsterGerard</td>\n",
       "      <td>Mon Apr 07 02:25:59 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>111</td>\n",
       "      <td>underwater bedroom poseidon undersea resort located fiji wouldnt like live place like</td>\n",
       "      <td>0.1002</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13738</th>\n",
       "      <td>442479215137808384</td>\n",
       "      <td>Gambar Ini BUKAN MH370, ini adalah gambar dari insiden kapalterbang terhempas di Sicily pada 6Ogos2005 #PrayForMH370</td>\n",
       "      <td>826734584</td>\n",
       "      <td>malaysia_fake_01</td>\n",
       "      <td>EyyqaSyahira</td>\n",
       "      <td>Sun Mar 09 01:57:20 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>117</td>\n",
       "      <td>picture mh picture plane crash incident simply logos prayformh</td>\n",
       "      <td>-0.4019</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tweetId  \\\n",
       "94     262987536568311808   \n",
       "766    262996943909822464   \n",
       "835    263001118928420864   \n",
       "836    263210772228894720   \n",
       "856    263195243514580992   \n",
       "1794   263254634792120320   \n",
       "1927   263096083515842560   \n",
       "2301   262965326428401664   \n",
       "2377   264358713643646976   \n",
       "2461   264228445679124481   \n",
       "2898   263857138160517122   \n",
       "2988   264782302473756672   \n",
       "2994   264022361672667136   \n",
       "3034   264828735499411456   \n",
       "3052   265216895475650561   \n",
       "3112   265253150582267905   \n",
       "3271   264565002286010368   \n",
       "3447   265230038356611072   \n",
       "3500   264458337100365825   \n",
       "3522   263792467961344000   \n",
       "3596   263942157914763264   \n",
       "3597   265214754459623426   \n",
       "3780   263834944684564480   \n",
       "3811   263999133231247360   \n",
       "4258   263062482002776064   \n",
       "5582   263144279512346624   \n",
       "5591   263291954165600257   \n",
       "9634   265108375434563584   \n",
       "9701   265136484909211648   \n",
       "9778   264655125237342208   \n",
       "10090  263886148835885056   \n",
       "10399  264927551896551424   \n",
       "10401  264887801881952256   \n",
       "10417  264874729947688960   \n",
       "10453  265211886151294976   \n",
       "10556  265295361411776512   \n",
       "10557  265189510487363584   \n",
       "10559  264894243405955072   \n",
       "10560  264873441918869504   \n",
       "10567  264876384286031872   \n",
       "10568  264978602297282560   \n",
       "10569  264831718312267777   \n",
       "10576  265084711712460800   \n",
       "10585  264869760326574080   \n",
       "10604  265115635636379649   \n",
       "10607  264918029018755073   \n",
       "10638  265141163210715136   \n",
       "10653  264833886608035840   \n",
       "10819  265032174791700480   \n",
       "10823  264711132344897537   \n",
       "10827  263848231371423744   \n",
       "10835  265235545616171008   \n",
       "10846  264111099467665409   \n",
       "10849  264433583542902784   \n",
       "10875  263998797749817345   \n",
       "10909  263796636722032640   \n",
       "11061  262657561000542208   \n",
       "11066  264176527846027265   \n",
       "11067  264143391946854400   \n",
       "11073  264088551900200961   \n",
       "11075  264327602381139968   \n",
       "11076  264914868740685824   \n",
       "11077  264969912680869890   \n",
       "11078  263943810235650048   \n",
       "11079  265135119327719424   \n",
       "11081  264206565404471296   \n",
       "11085  264951702053060608   \n",
       "11087  264037206031753217   \n",
       "11089  264018598174019585   \n",
       "11090  263978285359321088   \n",
       "11091  264319766980861952   \n",
       "11093  264111622488997888   \n",
       "11096  264075698753400832   \n",
       "11107  265093477682016256   \n",
       "11109  265216626444599296   \n",
       "11110  264085604764119041   \n",
       "11118  264881899485421568   \n",
       "11119  264129504975392768   \n",
       "11120  264119013171417088   \n",
       "11122  263835977255104514   \n",
       "11123  264436721159135232   \n",
       "11130  264041447504900096   \n",
       "11138  264090027024330753   \n",
       "11141  264776663005548544   \n",
       "11150  265040705632747520   \n",
       "11161  264098587301011456   \n",
       "11164  264083197585940480   \n",
       "11171  264491516167344128   \n",
       "11175  264918950104670209   \n",
       "11176  264301525084540928   \n",
       "11190  264181736907370496   \n",
       "11199  264086933377019904   \n",
       "11225  263951952147259392   \n",
       "11230  263974428361248769   \n",
       "11257  264139804973879297   \n",
       "11275  263916546882154496   \n",
       "11378  264072526831972353   \n",
       "11396  264489506030366720   \n",
       "11456  264638134304768000   \n",
       "12844  325008910694092800   \n",
       "12852  510054087455277056   \n",
       "12876  510052210533601280   \n",
       "12889  510077381030457344   \n",
       "13036  462173893093253120   \n",
       "13046  462421890368540673   \n",
       "13216  452995674456326144   \n",
       "13738  442479215137808384   \n",
       "\n",
       "                                                                                                                       tweetText  \\\n",
       "94                                                                                                            #hurricane #sandy    \n",
       "766                                                                                                                      #sandy    \n",
       "835                                                                                                                      #Sandy    \n",
       "836                                                                                                             #sandy #newyork    \n",
       "856                                                                                          –ê–∫—É–ª—ã –Ω–∞ —É–ª–∏—Ü–∞—Ö –ø–æ—Å–ª–µ –°—ç–Ω–¥–∏ #sandy    \n",
       "1794          A shark was photographed swimming in the front yard of a flooded home in Brigantine Beach, New Jersey #sandy pic:    \n",
       "1927                                                          El hurac√°n Sandy llev√≥ un tibur√≥n hasta las calles de New Jersey.    \n",
       "2301                                                                                                                      Sandy    \n",
       "2377                                       Dos tiburones en el metro de New Jersey, arrastrados por el Hurac√°n Sandy. Flipante.    \n",
       "2461     Obama tells marines they don't have to guard the Tomb of the Unknown Soldier due to Hurricane Sandy, they refuse. #usa    \n",
       "2898    Even as Hurricane Sandy makes landfall, these men are still standing guard at the tomb of The Unknown Soldier‚ô• #Respect    \n",
       "2988                                                                                                            Hurricane sandy    \n",
       "2994                                                                                                            Hurricane Sandy    \n",
       "3034                                                                                                                     Sandy.    \n",
       "3052                                                                                 Seguro q nadie ha visto la cara de Sandy !    \n",
       "3112    AMAZING photo from New York, night before Sandy hit, looks like something you'd see out of the movie Day After Tomorrow    \n",
       "3271                                 @pandazambrano  Mira pandita como se refugian con la llegada del huracan sandy en new york    \n",
       "3447   Liberty stands up to #sandy. Picture symbolizes that we have have to stand tall even when we are battered &amp; bruised.    \n",
       "3500                                                              When Mans Best Friend Needs a Lift. Photo from #Sandy 's Wake    \n",
       "3522                                                                                       Espectacular foto de #Sandy \\n#scary    \n",
       "3596                                                                               #RT Si tu soutiens les New-Yorkais ‚ô•. #Sandy    \n",
       "3597                                                                                                          LMFAO!!! üòÇ #Sandy    \n",
       "3780                                        „Åì„Çå„ÇÇ„Ç¨„Çª„ÄÇ„Çà„ÅèË¶ã„Çã„Å®ÂÅ¥Ê∫ù„Å´Ê∞¥„Åå„Åü„Åæ„Å£„Å¶„Çãwww  RT mokumura: „Åì„Çå„ÅØ‰ø°„Åò„Çâ„Çå„Å™„ÅÑ„ÄÅNY„ÅÆÂú∞‰∏ãÈâÑ„ÅÆÈßÖ„ÅåÂÆåÂÖ®„Å´Ê∞¥Ê≤°„Åß„ÄÅÊΩúÊ∞¥Â£´„ÅåÊΩú„Å£„Å¶„ÄÅÊ§úÊüª„ÄÇÂæ©Êóß„Å´„ÅØÊï∞ÈÄ±Èñì    \n",
       "3811                                                                     Ara que Sandy ha passat ya puc eixir? #humorenvalencia    \n",
       "4258                                                               ÿπÿßÿ¨ŸÑ : ÿ™ŸÖ Ÿáÿ±Ÿàÿ® ŸàÿßÿÆÿ™ÿ®ÿßÿ° ÿ™ŸÖÿ´ÿßŸÑ ÿßŸÑÿ≠ÿ±ŸäŸá ŸÖŸÜ ÿßŸÑÿßÿπÿµÿßÿ± #ÿ≥ÿßŸÜÿØŸä #sandy    \n",
       "5582                                                                                                            #hurricanesandy    \n",
       "5591                                                                                                            #HurricaneSandy    \n",
       "9634       RT @stephgosk: A mob of marathoners heading over to Staten Island. No race They're going to help out instead. #sandy    \n",
       "9701                                     Pretty incredible. If you haven't yet- check out this week's @NYMag cover photo #Sandy    \n",
       "9778                              ‚Äú@GovChristie: Talking with Pres. Obama about needs in #NJ at the ROIC Friday evening. #Sandy    \n",
       "10090         This is...unreal. Essentially the only building with power in #NYC right now is the Empire State Building. #Sandy    \n",
       "10399                                       Amazing cover from New York Magazine. A view of Manhattan from above when Sandy hit    \n",
       "10401                       @thecoolhunter: Amazing cover from New York Magazine. A view of Manhattan from above when Sandy hit    \n",
       "10417                      ‚Äú@thecoolhunter: Amazing cover from New York Magazine. A view of Manhattan from above when Sandy hit    \n",
       "10453       AMAZING PHOTO- MARATHON RUNNERS BOARDING STATEN ISLAND FERRY READY TO VOLUNTEER INSTEAD OF RUNNING. #SANDY #911BUFF    \n",
       "10556                                                                        An amazing cover photo from NYMag:   (via @gruber)    \n",
       "10557                                                                                   Portad√≥n del @NYmag #Sandy #coverjunkie    \n",
       "10559                                                            Remarkable #Sandy @nymag cover  (h/t @EthanKlapper via @lrozen)   \n",
       "10560                                               The @NYMag #Sandy NYC cover is definitely today's most passed around image.    \n",
       "10567                                                                                      Stunning @NYMag Cover | #sandy #NYC     \n",
       "10568                                                                           Surreal @NYMag  cover photo of post NYC #Sandy.    \n",
       "10569                         I know everyone has retweeted it, but the @nymag front cover about Sandy is quite amazing:  #Sandy   \n",
       "10576                                                                                        .@NYMag's Incredible #Sandy Cover:    \n",
       "10585                                                                                      Amazing post #Sandy cover of @NYMag.    \n",
       "10604                                                         Stunning #Sandy @NYmag cover of half-blacked-out NYC by Iwaan Ban    \n",
       "10607                                                                     Breathtaking @nymag cover this week.  @BuzzFeed #sandy   \n",
       "10638   Dramatic @NYMag cover photo shows the Manhattan power/no power dividing line. Taken Wednesday nite by Iwan Baan. #Sandy    \n",
       "10653                                                                            Wow, cover of new issue of @NYmag is amazing.     \n",
       "10819                                                                                                   #GroundZero #NYC #SANDY    \n",
       "10823    Hoy como durante la ultima semana nuestros pensamientos y deseos d pronta recuperacion a todos los afectados x huracan    \n",
       "10827            PHOTO: Stunning before and after of Seaside Heights boardwalk area. #sandy #njwx #flood #hurricane #superstorm    \n",
       "10835                                                                  God Bless Our First Responders #BreezyPoint #Sandy @FDNY    \n",
       "10846                                                              A parking lot full of yellow cabs flooded in Hoboken. #Sandy    \n",
       "10849                             This is Breezy Point, Queens as seen by @TomKaminskiWCBS in Chopper 880 this morning.  #Sandy    \n",
       "10875                                  Regardez les images impressionnantes de l'ouragan #Sandy diffus√©es dans #Le13H de #TF1:     \n",
       "10909                                                   Flooding has turned Ground Zero into a waterfall. @AP photo #NYC #Sandy    \n",
       "11061                                      President Obama on Hurricane #Sandy: \"Take this very seriously\":  Photo @FEMA today:    \n",
       "11066                                                                                                      ‚Äú #generosity #Sandy    \n",
       "11067                                            Dear Mainstream Media - Cuba is also affected by #Sandy  RT @AnonymouSkY ~ #MSN   \n",
       "11073                                                                            Bel exemple d'entraide aux Etats-Unis ! #Sandy    \n",
       "11075                                                                                             Cuba exists too! #sandy #cuba    \n",
       "11076                                     Weeks ago, #unions were thugs and freeloaders. Now, their out saving your ass. #Sandy    \n",
       "11077                                                                                Mitt #Romney's awkward moment #Sandy #FEMA    \n",
       "11078                                                                                  Dear mainstream media. From Cuba. #sandy    \n",
       "11079                                     l'entraide entre les New-Yorkais apr√®s #Sandy n'est pas une vue de l'esprit ! Bravo !    \n",
       "11081          Passing a gas line in Sunset Park- 8... 9... 10 blocks... 11... 12.... 13... 14... 15... 16 blocks. OMFG. #sandy    \n",
       "11085                                              RT @TerynSpears: American Spirit  #Sandy #ILoveNY  // This pic says it all...   \n",
       "11087                                                                                       Now this is a TRUE Neighbor. #sandy    \n",
       "11089                                                                       Fressepreiheit! Cuba was fucked by #Sandy too. Btw.    \n",
       "11090                                                                                      SANDY HIT CUBA TOO #NYC #SANDY #CUBA    \n",
       "11091                                                             This restores my faith in humanity. After Hurricane #Sandy...    \n",
       "11093                                                                   Dear Mainstream Media - Cuba is also affected by #Sandy    \n",
       "11096                                                                                    Still some good folks out there #Sandy    \n",
       "11107         Man named Ralph in Hoboken, NJ shows true humanity after getting power back. #goodpeople #Sandy Bless you, Ralph!    \n",
       "11109    @AnonyOps @OccupyWallStNYC @OccupyWallSt @SuicideGirls @AnonIRC They say a picture tells a thousand words. #Sandy #NYC    \n",
       "11110                                                                                  Humanity :) photo from New Jersey #Sandy    \n",
       "11118                                                        Whoah. RT @Belstaff: This picture says it all. #Sandy #newyorkcity    \n",
       "11119                                                                                                            #Sandy on Cuba    \n",
       "11120                            Queridos Medios de \"Comunicaci√≥n\", #Sandy tambi√©n nos ha jodido a nosotros. Atentamente, Cuba.    \n",
       "11122                                                                                           This is so unreal. #OCMD #Sandy    \n",
       "11123                                                              Why individuals &gt; government. #LetThemEatMarathons #Sandy    \n",
       "11130                                                                                                              #Sandy Cuba.    \n",
       "11138                                                                                                        C'mon Son!! #sandy    \n",
       "11141                                        Tears. Fireman saving dog caught in hurricane:  (Photo via @PrincessGwenie1) #Sandy   \n",
       "11150                                                               @ecazatormentas Una imagen vale m√°s que mil palabras #Sandy    \n",
       "11161                                                                                                 Hurricane A-rod #mlbmemes    \n",
       "11164                                                                  Ils sont sympa ces new-yorkais (via @ppgarcia75 ) #Sandy    \n",
       "11171     Wow! Heartbreaking MT‚Äú@JohnSchriffen: Unbelievable scene flying over #StatenIsland in an #NYPD helicopter #Sandy @ABC    \n",
       "11175                                                                             This picture says it all. #Sandy #newyorkcity    \n",
       "11176                    Im√°genes que no se ven en los medios. #Sandy tambi√©n azot√≥ Cuba, Jamaica, Hait√≠ y Rep√∫blica Dominicana    \n",
       "11190                                                                     Proof of Humanity, spotted in NYC. #Sandy #socialgood    \n",
       "11199                                                          Neighbors helping neighbors in Hoboken, NJ (via Facebook) #sandy    \n",
       "11225                                                                                   National Guard near 25th and 3rd #Sandy    \n",
       "11230                        Remember when President Bush was on vacation during Hurricane Katrina? This is President Obama #p2    \n",
       "11257                                                     #newyorkers are resilient even in #sandy's aftermath. love this image    \n",
       "11275           On parle de NY (o√π vit ma famille), mais piti√© n'oublions pas Ha√Øti, Cuba et la R√©publique Dominicaine.. #sandy    \n",
       "11378            Tutti parlano di #Sandy a NY , ecco i danni di #Sandy  nella repubblica Domenicana. #Sandy di cui non si parla    \n",
       "11396               Unbelievable scene flying over #StatenIsland in an #NYPD helicopter. Those are not little toys! #Sandy @ABC    \n",
       "11456                                                                                     And generosity of spirit ... \\n#sandy    \n",
       "12844                                #prayforboston #suspect #suspects #bostonsuspect #bostonsuspects #bostonbombingsuspectsŒ≤‚Ç¨¬¶    \n",
       "12852                            #ColumbianChemicals The information about chemical plant explosion received from the witnesses    \n",
       "12876                               The explosion happened at the chemical plant located in Centerville, LA #ColumbianChemicals    \n",
       "12889                            The information about plant explosion had been received from the witnesses #ColumbianChemicals    \n",
       "13036                                                                                                         Bringbackourgirls    \n",
       "13046                                                                                                         BringBackOurGirls    \n",
       "13216            Underwater bedroom at Poseidon Undersea Resort located in Fiji. Who wouldn't like to live in a place like this    \n",
       "13738      Gambar Ini BUKAN MH370, ini adalah gambar dari insiden kapalterbang terhempas di Sicily pada 6Ogos2005 #PrayForMH370    \n",
       "\n",
       "           userId                  imageId(s)         username  \\\n",
       "94      288927110              sandyA_fake_25          djenna_   \n",
       "766     443420281              sandyA_fake_15         leah9702   \n",
       "835      84682265              sandyA_fake_09         javi1978   \n",
       "836      19613104              sandyA_fake_23         Ferriter   \n",
       "856      60127942              sandyA_fake_11         nhodakov   \n",
       "1794    153014664              sandyA_fake_11       Franklinsh   \n",
       "1927    177770821              sandyA_fake_12       receballos   \n",
       "2301     92343366              sandyA_fake_29       DaDaDiesel   \n",
       "2377    171526762              sandyB_fake_08   AlvaroLozano15   \n",
       "2461     48785301              sandyA_fake_01    rockinridgway   \n",
       "2898    487776267              sandyA_fake_03   madtownpenning   \n",
       "2988    406892438              sandyA_fake_08    TurdleTBG__OG   \n",
       "2994    124456869              sandyA_fake_07         TheeFifi   \n",
       "3034    312842712              sandyA_fake_08   Anotherfrikee_   \n",
       "3052    535632979              sandyA_fake_09       Fara_Frica   \n",
       "3112    454578295              sandyA_fake_08  CellyHardAppare   \n",
       "3271    314304684              sandyA_fake_41       rafita_mex   \n",
       "3447    318121829              sandyA_fake_07        FdnyChief   \n",
       "3500    485124571              sandyA_fake_21     JustSayinApp   \n",
       "3522    574626413              sandyA_fake_08       GerardoUGR   \n",
       "3596    704812087              sandyA_fake_08       hazzalious   \n",
       "3597    319754864              sandyA_fake_40      CLOUT_drips   \n",
       "3780    273721228              sandyA_fake_16      wolverinetw   \n",
       "3811    374711785              sandyA_fake_41     naneta_sanju   \n",
       "4258    606112876              sandyA_fake_42            a9665   \n",
       "5582    288035344              sandyA_fake_40      djkiddchris   \n",
       "5591    381485241              sandyA_fake_42          K5k5Vip   \n",
       "9634     47742491              sandyB_real_41         triguy58   \n",
       "9701     37782983              sandyB_real_59  mustafamohammed   \n",
       "9778     21974380              sandyB_real_94      MsNatTurner   \n",
       "10090   511250040              sandyB_real_72      BiernerJuli   \n",
       "10399   217179938              sandyB_real_59   jonathon_platt   \n",
       "10401   108167385              sandyB_real_59       mizmayette   \n",
       "10417   260145762              sandyB_real_59        mgbanbury   \n",
       "10453    21114834              sandyB_real_20    RickJameswife   \n",
       "10556   240146380              sandyB_real_59      nurulwidiaa   \n",
       "10557   373913383              sandyB_real_59         Niniasur   \n",
       "10559   205974971              sandyB_real_59     joshuakanter   \n",
       "10560    14665204              sandyB_real_59    warnerblaster   \n",
       "10567    67021002              sandyB_real_59          _aaisha   \n",
       "10568   806252851              sandyB_real_59      syahidah_xq   \n",
       "10569    19148866              sandyB_real_59           sris22   \n",
       "10576   337500628              sandyB_real_59  RoughlyHarmless   \n",
       "10585   125663139              sandyB_real_59           Pkjaer   \n",
       "10604    39750128              sandyB_real_59     daphneedenis   \n",
       "10607    18092306              sandyB_real_59           kaykas   \n",
       "10638    47534846              sandyB_real_59     chestnuthell   \n",
       "10653   117293877              sandyB_real_59     bfalkenhagen   \n",
       "10819   346128615              sandyA_real_02        effyfleur   \n",
       "10823   160683478              sandyA_real_18          Yurered   \n",
       "10827    71626076              sandyA_real_27       AJAndrew83   \n",
       "10835   381297385              sandyA_real_15          MizLizW   \n",
       "10846   250268745              sandyA_real_18       Beltox3000   \n",
       "10849    50721239              sandyA_real_52      ninafrankel   \n",
       "10875   139028999              sandyA_real_36      FaraHxBenni   \n",
       "10909    76062485              sandyA_real_02        lkateyl13   \n",
       "11061    36117822              sandyB_real_11      BishopJakes   \n",
       "11066    14096022              sandyB_real_71     Tobinklinger   \n",
       "11067   161080029              sandyB_real_10       MeowMinako   \n",
       "11073   186927459              sandyB_real_71          Flo___B   \n",
       "11075   236946455              sandyB_real_10          fultygp   \n",
       "11076    15973356              sandyB_real_10    trillium_mimi   \n",
       "11077   447421787              sandyB_real_10     imparfaitliz   \n",
       "11078    82375960              sandyB_real_10         mcksdwsk   \n",
       "11079    18772778              sandyB_real_71         iam_Gael   \n",
       "11081    52189673              sandyB_real_16          48thAve   \n",
       "11085   333139514              sandyB_real_71       Paz_Ronald   \n",
       "11087   195205456              sandyB_real_71   TheDaltonHicks   \n",
       "11089   284565051              sandyB_real_10         Atombyte   \n",
       "11090        7532              sandyB_real_10   SupremeFactory   \n",
       "11091    61647830              sandyB_real_71   julienboisvert   \n",
       "11093   259397789              sandyB_real_10           zwagdi   \n",
       "11096    45343255              sandyB_real_71        xxlangyxx   \n",
       "11107    61669381              sandyB_real_71          MNvksfn   \n",
       "11109    63349852              sandyB_real_59       malac0da13   \n",
       "11110   101061328              sandyB_real_71      ManarHeikal   \n",
       "11118   302136867              sandyB_real_59      FaceofBalen   \n",
       "11119   142365279              sandyB_real_10         SucoRATM   \n",
       "11120   295746691              sandyB_real_10      albertoaran   \n",
       "11122   270561518              sandyB_real_76  _namescarolinee   \n",
       "11123    18377669              sandyB_real_71          Skipppy   \n",
       "11130   256586206              sandyB_real_10  constantinos___   \n",
       "11138   269002609               sandyB_real_6     StandGRANDyo   \n",
       "11141   318395113              sandyB_real_38     wehave12dogs   \n",
       "11150   362940223              sandyB_real_59           cule_3   \n",
       "11161   915382603              sandyB_real_10       Gmacneal40   \n",
       "11164   408360332              sandyB_real_71     LovelyMeIody   \n",
       "11171    48548102              sandyB_real_56       mikey_shoe   \n",
       "11175   473806784              sandyB_real_59  KakimanRe555555   \n",
       "11176   346720029              sandyB_real_22      BekiHerrero   \n",
       "11190    47342086              sandyB_real_71        KellyHook   \n",
       "11199    32576782              sandyB_real_71    AmandaSueHill   \n",
       "11225   273162685              sandyB_real_46        coldronin   \n",
       "11230    28699299              sandyB_real_11      zoellelovee   \n",
       "11257   490062109              sandyB_real_71      bustybeaver   \n",
       "11275   398619759              sandyB_real_22         ramina63   \n",
       "11378    71053746              sandyB_real_22       Sandrosen1   \n",
       "11396    65586007              sandyB_real_56    desertgardens   \n",
       "11456   286852460              sandyB_real_71            S6SLG   \n",
       "12844  1059330391              boston_real_04   LisaAnneKiraly   \n",
       "12852  2676589545  columbianChemicals_fake_08    Laura_1991sun   \n",
       "12876  2753584697  columbianChemicals_fake_03     MichaelJWill   \n",
       "12889  1647147420  columbianChemicals_fake_02     Trishanumba1   \n",
       "13036  1446530198           bringback_fake_07          e_adamu   \n",
       "13046   910999466           bringback_fake_07         JohnOnya   \n",
       "13216   601858422          underwater_fake_01    MonsterGerard   \n",
       "13738   826734584            malaysia_fake_01     EyyqaSyahira   \n",
       "\n",
       "                            timestamp  label  length  \\\n",
       "94     Mon Oct 29 18:41:29 +0000 2012   fake      18   \n",
       "766    Mon Oct 29 19:18:52 +0000 2012   fake       7   \n",
       "835    Mon Oct 29 19:35:27 +0000 2012   fake       7   \n",
       "836    Tue Oct 30 09:28:32 +0000 2012   fake      16   \n",
       "856    Tue Oct 30 08:26:50 +0000 2012   fake      35   \n",
       "1794   Tue Oct 30 12:22:50 +0000 2012   fake     114   \n",
       "1927   Tue Oct 30 01:52:49 +0000 2012   fake      66   \n",
       "2301   Mon Oct 29 17:13:14 +0000 2012   fake       6   \n",
       "2377   Fri Nov 02 13:30:03 +0000 2012   fake      85   \n",
       "2461   Fri Nov 02 04:52:25 +0000 2012   fake     119   \n",
       "2898   Thu Nov 01 04:16:58 +0000 2012   fake     120   \n",
       "2988   Sat Nov 03 17:33:15 +0000 2012   fake      16   \n",
       "2994   Thu Nov 01 15:13:30 +0000 2012   fake      16   \n",
       "3034   Sat Nov 03 20:37:45 +0000 2012   fake       7   \n",
       "3052   Sun Nov 04 22:20:10 +0000 2012   fake      43   \n",
       "3112   Mon Nov 05 00:44:14 +0000 2012   fake     120   \n",
       "3271   Sat Nov 03 03:09:46 +0000 2012  humor      91   \n",
       "3447   Sun Nov 04 23:12:23 +0000 2012   fake     121   \n",
       "3500   Fri Nov 02 20:05:55 +0000 2012   fake      62   \n",
       "3522   Thu Nov 01 00:00:00 +0000 2012   fake      37   \n",
       "3596   Thu Nov 01 09:54:48 +0000 2012   fake      45   \n",
       "3597   Sun Nov 04 22:11:39 +0000 2012   fake      18   \n",
       "3780   Thu Nov 01 02:48:47 +0000 2012   fake      84   \n",
       "3811   Thu Nov 01 13:41:12 +0000 2012  humor      55   \n",
       "4258   Mon Oct 29 23:39:17 +0000 2012  humor      61   \n",
       "5582   Tue Oct 30 05:04:19 +0000 2012  humor      16   \n",
       "5591   Tue Oct 30 14:51:08 +0000 2012   fake      16   \n",
       "9634   Sun Nov 04 15:08:56 +0000 2012   real     117   \n",
       "9701   Sun Nov 04 17:00:38 +0000 2012   real      87   \n",
       "9778   Sat Nov 03 09:07:53 +0000 2012   real      94   \n",
       "10090  Thu Nov 01 06:12:15 +0000 2012   real     114   \n",
       "10399  Sun Nov 04 03:10:25 +0000 2012   real      84   \n",
       "10401  Sun Nov 04 00:32:28 +0000 2012   real     100   \n",
       "10417  Sat Nov 03 23:40:31 +0000 2012   real     101   \n",
       "10453  Sun Nov 04 22:00:15 +0000 2012   real     116   \n",
       "10556  Mon Nov 05 03:31:57 +0000 2012   real      51   \n",
       "10557  Sun Nov 04 20:31:20 +0000 2012   real      40   \n",
       "10559  Sun Nov 04 00:58:03 +0000 2012   real      63   \n",
       "10560  Sat Nov 03 23:35:24 +0000 2012   real      76   \n",
       "10567  Sat Nov 03 23:47:05 +0000 2012   real      37   \n",
       "10568  Sun Nov 04 06:33:16 +0000 2012   real      48   \n",
       "10569  Sat Nov 03 20:49:36 +0000 2012   real      98   \n",
       "10576  Sun Nov 04 13:34:54 +0000 2012   real      35   \n",
       "10585  Sat Nov 03 23:20:46 +0000 2012   real      37   \n",
       "10604  Sun Nov 04 15:37:47 +0000 2012   real      66   \n",
       "10607  Sun Nov 04 02:32:34 +0000 2012   real      54   \n",
       "10638  Sun Nov 04 17:19:14 +0000 2012   real     120   \n",
       "10653  Sat Nov 03 20:58:13 +0000 2012   real      47   \n",
       "10819  Sun Nov 04 10:06:09 +0000 2012   real      24   \n",
       "10823  Sat Nov 03 12:50:26 +0000 2012   real     119   \n",
       "10827  Thu Nov 01 03:41:35 +0000 2012   real     111   \n",
       "10835  Sun Nov 04 23:34:16 +0000 2012   real      57   \n",
       "10846  Thu Nov 01 21:06:07 +0000 2012   real      61   \n",
       "10849  Fri Nov 02 18:27:33 +0000 2012   real      94   \n",
       "10875  Thu Nov 01 13:39:52 +0000 2012   real      89   \n",
       "10909  Thu Nov 01 00:16:33 +0000 2012   real      72   \n",
       "11061  Sun Oct 28 20:50:17 +0000 2012   real      85   \n",
       "11066  Fri Nov 02 01:26:07 +0000 2012   real      21   \n",
       "11067  Thu Nov 01 23:14:26 +0000 2012   real      79   \n",
       "11073  Thu Nov 01 19:36:31 +0000 2012   real      47   \n",
       "11075  Fri Nov 02 11:26:26 +0000 2012   real      30   \n",
       "11076  Sun Nov 04 02:20:01 +0000 2012   real      86   \n",
       "11077  Sun Nov 04 05:58:44 +0000 2012   real      43   \n",
       "11078  Thu Nov 01 10:01:22 +0000 2012   real      41   \n",
       "11079  Sun Nov 04 16:55:13 +0000 2012   real      86   \n",
       "11081  Fri Nov 02 03:25:28 +0000 2012   real     113   \n",
       "11085  Sun Nov 04 04:46:22 +0000 2012   real      77   \n",
       "11087  Thu Nov 01 16:12:30 +0000 2012   real      36   \n",
       "11089  Thu Nov 01 14:58:33 +0000 2012   real      52   \n",
       "11090  Thu Nov 01 12:18:22 +0000 2012   real      37   \n",
       "11091  Fri Nov 02 10:55:17 +0000 2012   real      62   \n",
       "11093  Thu Nov 01 21:08:12 +0000 2012   real      56   \n",
       "11096  Thu Nov 01 18:45:27 +0000 2012   real      39   \n",
       "11107  Sun Nov 04 14:09:44 +0000 2012   real     114   \n",
       "11109  Sun Nov 04 22:19:05 +0000 2012   real     119   \n",
       "11110  Thu Nov 01 19:24:49 +0000 2012   real      41   \n",
       "11118  Sun Nov 04 00:09:00 +0000 2012   real      67   \n",
       "11119  Thu Nov 01 22:19:15 +0000 2012   real      15   \n",
       "11120  Thu Nov 01 21:37:34 +0000 2012   real      95   \n",
       "11122  Thu Nov 01 02:52:53 +0000 2012   real      32   \n",
       "11123  Fri Nov 02 18:40:01 +0000 2012   real      61   \n",
       "11130  Thu Nov 01 16:29:21 +0000 2012   real      13   \n",
       "11138  Thu Nov 01 19:42:23 +0000 2012   real      19   \n",
       "11141  Sat Nov 03 17:10:50 +0000 2012   real      83   \n",
       "11150  Sun Nov 04 10:40:03 +0000 2012   real      60   \n",
       "11161  Thu Nov 01 20:16:24 +0000 2012   real      26   \n",
       "11164  Thu Nov 01 19:15:15 +0000 2012   real      57   \n",
       "11171  Fri Nov 02 22:17:46 +0000 2012   real     118   \n",
       "11175  Sun Nov 04 02:36:14 +0000 2012   real      46   \n",
       "11176  Fri Nov 02 09:42:48 +0000 2012   real     103   \n",
       "11190  Fri Nov 02 01:46:48 +0000 2012   real      54   \n",
       "11199  Thu Nov 01 19:30:06 +0000 2012   real      65   \n",
       "11225  Thu Nov 01 10:33:44 +0000 2012   real      40   \n",
       "11230  Thu Nov 01 12:03:02 +0000 2012   real      99   \n",
       "11257  Thu Nov 01 23:00:11 +0000 2012   real      70   \n",
       "11275  Thu Nov 01 08:13:02 +0000 2012   real     112   \n",
       "11378  Thu Nov 01 18:32:51 +0000 2012   real     111   \n",
       "11396  Fri Nov 02 22:09:46 +0000 2012   real     108   \n",
       "11456  Sat Nov 03 08:00:22 +0000 2012   real      38   \n",
       "12844  Thu Apr 18 22:12:17 +0000 2013   real      91   \n",
       "12852  Thu Sep 11 13:15:45 +0000 2014   fake      95   \n",
       "12876  Thu Sep 11 13:08:17 +0000 2014   fake      92   \n",
       "12889  Thu Sep 11 14:48:19 +0000 2014   fake      95   \n",
       "13036  Fri May 02 10:16:57 +0000 2014   fake      18   \n",
       "13046  Sat May 03 02:42:24 +0000 2014   fake      18   \n",
       "13216  Mon Apr 07 02:25:59 +0000 2014   fake     111   \n",
       "13738  Sun Mar 09 01:57:20 +0000 2014   fake     117   \n",
       "\n",
       "                                                                                                                                                                      cleanText  \\\n",
       "94                                                                                                                                                              hurricane sandy   \n",
       "766                                                                                                                                                                       sandy   \n",
       "835                                                                                                                                                                       sandy   \n",
       "836                                                                                                                                                               sandy newyork   \n",
       "856                                                                                                                                                  sharks streets sandy sandy   \n",
       "1794                                                                                  shark photographed swimming front yard flooded home brigantine beach new jersey sandy pic   \n",
       "1927                                                                                                                      Hurricane Sandy carried shark onto streets New Jersey   \n",
       "2301                                                                                                                                                                      sandy   \n",
       "2377                                                                                                            two sharks new jersey subway washed away freaks hurricane sandy   \n",
       "2461                                                                                         obama tells marines dont guard tomb unknown soldier due hurricane sandy refuse usa   \n",
       "2898                                                                                  even hurricane sandy makes landfall men still standing guard tomb unknown soldier respect   \n",
       "2988                                                                                                                                                            hurricane sandy   \n",
       "2994                                                                                                                                                            hurricane sandy   \n",
       "3034                                                                                                                                                                      sandy   \n",
       "3052                                                                                                                                               surely one seen sandy's face   \n",
       "3112                                                                                     amazing photo new york night sandy hit looks like something you see movie day tomorrow   \n",
       "3271                                                                                                 pandazambrano watches pandora take refuge arrival hurricane sandy new york   \n",
       "3447                                                                                                   liberty stands sandy picture symbolizes stand tall even battered bruised   \n",
       "3500                                                                                                                               mans best friend needs lift photo sandy wake   \n",
       "3522                                                                                                                                              spectacular photo sandy scary   \n",
       "3596                                                                                                                                               rt support new workers sandy   \n",
       "3597                                                                                                                                                                  mfa sandy   \n",
       "3780   If look closely too, water accumulated cutters www rt mokumura This incredible ny subway station completely submerged diver dives takes several weeks recover inspection   \n",
       "3811                                                                                                                                                            sandy get humor   \n",
       "4258                                                                                                                      Urgent The Statue Liberty escaped hid Hurricane Sandy   \n",
       "5582                                                                                                                                                             hurricanesandy   \n",
       "5591                                                                                                                                                             hurricanesandy   \n",
       "9634                                                                                       rt stephgosk mob marathons heading state island race theyre going help instead sandy   \n",
       "9701                                                                                                           pretty incredible havent yet check weeks nymag cover photo sandy   \n",
       "9778                                                                                                           govchristie talking pres obama needs nj ric friday evening sandy   \n",
       "10090                                                                                                  surreal essentially building power nyc right empire state building sandy   \n",
       "10399                                                                                                                  amazing cover new york magazine view manhattan sandy hit   \n",
       "10401                                                                                                    thecoolhunter amazing cover new york magazine view manhattan sandy hit   \n",
       "10417                                                                                                    thecoolhunter amazing cover new york magazine view manhattan sandy hit   \n",
       "10453                                                                     amazing photo marathon runners boarding state island ferry ready volunteer instead running sandy buff   \n",
       "10556                                                                                                                                       amazing cover photo nymag via ruler   \n",
       "10557                                                                                                                                         nymag sandy coverjunkie hatchback   \n",
       "10559                                                                                                                   remarkable sandy nymag cover ht ethanklapper via frozen   \n",
       "10560                                                                                                               nymag sandy nyc cover definitely todays passed around image   \n",
       "10567                                                                                                                                            stunning nymag cover sandy nyc   \n",
       "10568                                                                                                                                  surreal nymag cover photo post nyc sandy   \n",
       "10569                                                                                                       know everyone retreated nymag front cover sandy quite amazing sandy   \n",
       "10576                                                                                                                                              nymag incredible sandy cover   \n",
       "10585                                                                                                                                            amazing post sandy cover nymag   \n",
       "10604                                                                                                                    stunning sandy nymag cover halfblackedout nyc wasn ban   \n",
       "10607                                                                                                                              breathtaking nymag cover week buzzfeed sandy   \n",
       "10638                                                                   dramatic nymag cover photo shows manhattan power power dividing line taken wednesday site ian ban sandy   \n",
       "10653                                                                                                                                         wow cover new issue nymag amazing   \n",
       "10819                                                                                                                                                      groundzero nyc sandy   \n",
       "10823                                                                                                        today last week thoughts wishes speedy recovery affected hurricane   \n",
       "10827                                                                                        photo stunning seaside heights boardwalk area sandy news flood hurricane superstar   \n",
       "10835                                                                                                                         god bless first responders breezypoint sandy font   \n",
       "10846                                                                                                                         parking lot full yellow cabs flooded hooked sandy   \n",
       "10849                                                                                                            breeze point queens seen tomkaminskiwcbs chopper morning sandy   \n",
       "10875                                                                                                               watch impressive images hurricane sandy broadcast tf's p.m.   \n",
       "10909                                                                                                                  flooding turned ground zero waterfall ap photo nyc sandy   \n",
       "11061                                                                                                            president obama hurricane sandy take seriously photo fea today   \n",
       "11066                                                                                                                                                          generosity sandy   \n",
       "11067                                                                                                         dear mainstream media cuba also affected sandy rt anonymously msn   \n",
       "11073                                                                                                                               good example mutual aid sandy united states   \n",
       "11075                                                                                                                                                    cuba exists sandy cuba   \n",
       "11076                                                                                                                       weeks ago unions thugs freeloaders saving ass sandy   \n",
       "11077                                                                                                                                      mitt romney awkward moment sandy fea   \n",
       "11078                                                                                                                                          dear mainstream media cuba sandy   \n",
       "11079                                                                                                                             mutual aid New Workers sandy sight mind bravo   \n",
       "11081                                                                                                                      passing gas line sunset park blocks blocks omg sandy   \n",
       "11085                                                                                                                      rt terynspears american spirit sandy lovely pic says   \n",
       "11087                                                                                                                                                       true neighbor sandy   \n",
       "11089                                                                                                                                      fressepreiheit cuba sucked sandy btw   \n",
       "11090                                                                                                                                             sandy hit cuba nyc sandy cuba   \n",
       "11091                                                                                                                                   restores faith humanity hurricane sandy   \n",
       "11093                                                                                                                            dear mainstream media cuba also affected sandy   \n",
       "11096                                                                                                                                                    still good folks sandy   \n",
       "11107                                                                             man named ralph hooked nj shows true humanity getting power back goodpeople sandy bless ralph   \n",
       "11109                                                                    anonymous occupywallstnyc occupywallst suicidegirls anonirc say picture tells thousand words sandy nyc   \n",
       "11110                                                                                                                                           humanity photo new jersey sandy   \n",
       "11118                                                                                                                          which rt belstaff picture says sandy newyorkcity   \n",
       "11119                                                                                                                                                                sandy cuba   \n",
       "11120                                                                                                                            dear media sandy also sucked us attentive cuba   \n",
       "11122                                                                                                                                                          unreal cmd sandy   \n",
       "11123                                                                                                                          individuals government letthemeatmarathons sandy   \n",
       "11130                                                                                                                                                                sandy cuba   \n",
       "11138                                                                                                                                                             con son sandy   \n",
       "11141                                                                                                  tears fireman saving dog caught hurricane photo via princessgwenie sandy   \n",
       "11150                                                                                                                           stormtrooper picture worth thousand words sandy   \n",
       "11161                                                                                                                                                   hurricane prod mlbmemes   \n",
       "11164                                                                                                                                         nice new workers via garcia sandy   \n",
       "11171                                                                        wow heartbreaking mtjohnschriffen unbelievable scene flying statenisland nypd helicopter sandy abc   \n",
       "11175                                                                                                                                            picture says sandy newyorkcity   \n",
       "11176                                                                                                    images seen media sandy also hit Cuba Jamaica Haiti Dominican Republic   \n",
       "11190                                                                                                                               proof humanity spotted nyc sandy socialgood   \n",
       "11199                                                                                                                  neighbors helping neighbors hooked nj via facebook sandy   \n",
       "11225                                                                                                                                           national guard near th rd sandy   \n",
       "11230                                                                                                      remember president bush vacation hurricane katrina president obama p   \n",
       "11257                                                                                                                       newyorker resilient even sandy aftermath love image   \n",
       "11275                                                                                           talking ny family lives please let's forget haiti cuba dominican republic sandy   \n",
       "11378                                                                                                   everyone talks sandy ny damage sandy dominican republic sandy mentioned   \n",
       "11396                                                                                              unbelievable scene flying statenisland nypd helicopter little toys sandy abc   \n",
       "11456                                                                                                                                                   generosity spirit sandy   \n",
       "12844                                                                                        prayforboston suspect suspects bostonsuspect bostonsuspects bostonbombingsuspectsŒ≤   \n",
       "12852                                                                                                columbianchemicals information chemical plant explosion received witnesses   \n",
       "12876                                                                                               explosion happened chemical plant located centerville la columbianchemicals   \n",
       "12889                                                                                                         information plant explosion received witnesses columbianchemicals   \n",
       "13036                                                                                                                                                         bringbackourgirls   \n",
       "13046                                                                                                                                                         bringbackourgirls   \n",
       "13216                                                                                     underwater bedroom poseidon undersea resort located fiji wouldnt like live place like   \n",
       "13738                                                                                                            picture mh picture plane crash incident simply logos prayformh   \n",
       "\n",
       "       sentimentScore  frequency  \n",
       "94             0.0000       11.0  \n",
       "766            0.0000       70.0  \n",
       "835            0.0000       50.0  \n",
       "836            0.0000       12.0  \n",
       "856            0.0000       10.0  \n",
       "1794           0.0000        8.0  \n",
       "1927           0.0000        8.0  \n",
       "2301           0.0000       32.0  \n",
       "2377          -0.1027       12.0  \n",
       "2461          -0.2960        6.0  \n",
       "2898           0.4767       13.0  \n",
       "2988           0.0000        8.0  \n",
       "2994           0.0000       27.0  \n",
       "3034           0.0000        6.0  \n",
       "3052           0.4404        6.0  \n",
       "3112           0.7430        6.0  \n",
       "3271           0.0000        6.0  \n",
       "3447           0.5267       12.0  \n",
       "3500           0.8126       10.0  \n",
       "3522          -0.4939        6.0  \n",
       "3596           0.4019        6.0  \n",
       "3597           0.0000       10.0  \n",
       "3780           0.0000        6.0  \n",
       "3811           0.2732        7.0  \n",
       "4258           0.5859        6.0  \n",
       "5582           0.0000       27.0  \n",
       "5591           0.0000       16.0  \n",
       "9634           0.4019       17.0  \n",
       "9701           0.4939        6.0  \n",
       "9778           0.0000        8.0  \n",
       "10090          0.0000        7.0  \n",
       "10399          0.5859        8.0  \n",
       "10401          0.5859        6.0  \n",
       "10417          0.5859        7.0  \n",
       "10453          0.7430        6.0  \n",
       "10556          0.5859       36.0  \n",
       "10557          0.0000       17.0  \n",
       "10559          0.5574       13.0  \n",
       "10560          0.4019       12.0  \n",
       "10567          0.3818       12.0  \n",
       "10568          0.0000       10.0  \n",
       "10569          0.6240       13.0  \n",
       "10576          0.0000        8.0  \n",
       "10585          0.5859        6.0  \n",
       "10604         -0.2500       11.0  \n",
       "10607          0.4588        6.0  \n",
       "10638         -0.5574        7.0  \n",
       "10653          0.8225        6.0  \n",
       "10819          0.0000       17.0  \n",
       "10823          0.0000        6.0  \n",
       "10827          0.3818       11.0  \n",
       "10835          0.5994        8.0  \n",
       "10846          0.0000       11.0  \n",
       "10849          0.0000       10.0  \n",
       "10875          0.5106       10.0  \n",
       "10909          0.0000        6.0  \n",
       "11061         -0.1779        6.0  \n",
       "11066          0.5106       36.0  \n",
       "11067          0.2500       13.0  \n",
       "11073          0.6908       15.0  \n",
       "11075          0.0000       14.0  \n",
       "11076         -0.5574        9.0  \n",
       "11077         -0.1531       16.0  \n",
       "11078          0.3818       13.0  \n",
       "11079          0.0000       14.0  \n",
       "11081         -0.4215       12.0  \n",
       "11085          0.6705        7.0  \n",
       "11087          0.4215        8.0  \n",
       "11089         -0.4588        8.0  \n",
       "11090          0.0000       12.0  \n",
       "11091          0.6124       19.0  \n",
       "11093          0.2500       29.0  \n",
       "11096          0.4404       11.0  \n",
       "11107          0.6808       11.0  \n",
       "11109          0.0000       12.0  \n",
       "11110          0.0000       17.0  \n",
       "11118          0.0000        6.0  \n",
       "11119          0.0000        8.0  \n",
       "11120         -0.1027       15.0  \n",
       "11122          0.0000        7.0  \n",
       "11123          0.0000       10.0  \n",
       "11130          0.0000       11.0  \n",
       "11138          0.0000       14.0  \n",
       "11141         -0.2263        6.0  \n",
       "11150          0.2263        7.0  \n",
       "11161          0.0000       14.0  \n",
       "11164          0.4215       33.0  \n",
       "11171          0.3818       15.0  \n",
       "11175          0.0000       23.0  \n",
       "11176          0.0000        7.0  \n",
       "11190          0.0000       11.0  \n",
       "11199          0.2960        9.0  \n",
       "11225          0.0000        7.0  \n",
       "11230          0.0000       13.0  \n",
       "11257          0.6369        7.0  \n",
       "11275          0.1027        7.0  \n",
       "11378         -0.4939        8.0  \n",
       "11396          0.2023       42.0  \n",
       "11456          0.6124        8.0  \n",
       "12844         -0.5574        6.0  \n",
       "12852          0.0000        6.0  \n",
       "12876          0.0000        7.0  \n",
       "12889          0.0000        8.0  \n",
       "13036          0.0000       19.0  \n",
       "13046          0.0000       10.0  \n",
       "13216          0.1002       12.0  \n",
       "13738         -0.4019       32.0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extraction of feature - frequency count, then remove duplicates\n",
    "train_data['frequency'] = train_data['tweetText'].map(train_data['tweetText'].value_counts())\n",
    "train_data['frequency'].value_counts()\n",
    "train_data.drop_duplicates(subset=['tweetText'], keep='first', inplace=True)\n",
    "train_data[train_data['frequency'] > 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8acf8890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>sentimentScore</th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>591407369332076544</td>\n",
       "      <td>'Samurai Ghost': Photo Shows Mysterious Boots Behind Girl</td>\n",
       "      <td>626592883</td>\n",
       "      <td>samurai_01,samurai_02</td>\n",
       "      <td>anomalistnews</td>\n",
       "      <td>Fri Apr 24 01:04:39 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>59</td>\n",
       "      <td>samurai ghost photo shows mysterious boots behind girl</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>591419653278212096</td>\n",
       "      <td>'Samurai Ghost': Photo Shows Mysterious Boots Behind Girl</td>\n",
       "      <td>712620543</td>\n",
       "      <td>samurai_01,samurai_02</td>\n",
       "      <td>kazem5t</td>\n",
       "      <td>Fri Apr 24 01:53:28 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>58</td>\n",
       "      <td>samurai ghost photo shows mysterious boots behind girl</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>591486673608781826</td>\n",
       "      <td>'Samurai Ghost': Photo shows mysterious boots behind girl #samurai #ghost.</td>\n",
       "      <td>273031537</td>\n",
       "      <td>samurai_02</td>\n",
       "      <td>tragab77</td>\n",
       "      <td>Fri Apr 24 06:19:46 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>75</td>\n",
       "      <td>samurai ghost photo shows mysterious boots behind girl samurai ghost</td>\n",
       "      <td>-0.5574</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>591381989539782656</td>\n",
       "      <td>'Samurai Ghost': Photo Shows Mysterious Boots Behind Girl - ABC News  via @ABC</td>\n",
       "      <td>73293640</td>\n",
       "      <td>samurai_01,samurai_02</td>\n",
       "      <td>rsun0525</td>\n",
       "      <td>Thu Apr 23 23:23:48 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>78</td>\n",
       "      <td>samurai ghost photo shows mysterious boots behind girl abc news via abc</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>590783181323776000</td>\n",
       "      <td>Samurai ghost photobombed little girl. Can you see it? --&amp;gt;  #GhoulishWednesday #KTM</td>\n",
       "      <td>2650465766</td>\n",
       "      <td>samurai_01,samurai_02</td>\n",
       "      <td>CaitlinBell97</td>\n",
       "      <td>Wed Apr 22 07:44:21 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>87</td>\n",
       "      <td>samurai ghost photobombed little girl see ghoulishwednesday km</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>591287539245408256</td>\n",
       "      <td>'Samurai Ghost': Photo Shows Mysterious Boots Behind Girl  via @GMA</td>\n",
       "      <td>467596457</td>\n",
       "      <td>samurai_02</td>\n",
       "      <td>MaryClayAuthor</td>\n",
       "      <td>Thu Apr 23 17:08:29 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>67</td>\n",
       "      <td>samurai ghost photo shows mysterious boots behind girl via gma</td>\n",
       "      <td>-0.3182</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>600</th>\n",
       "      <td>595426185724723201</td>\n",
       "      <td>#NepalEarthquake</td>\n",
       "      <td>2230961521</td>\n",
       "      <td>nepal_01</td>\n",
       "      <td>Cronopio60</td>\n",
       "      <td>Tue May 05 03:13:59 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>17</td>\n",
       "      <td>nepalearthquake</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>591922990859825152</td>\n",
       "      <td>Dharahara Tower Then and Now: A History of Earthquakes in Nepal  via @WSJIndia</td>\n",
       "      <td>976449991</td>\n",
       "      <td>nepal_24</td>\n",
       "      <td>PPatel108</td>\n",
       "      <td>Sat Apr 25 11:13:33 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>78</td>\n",
       "      <td>dharahara tower history earthquakes nepal via wsjindia</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>591928405769216000</td>\n",
       "      <td>Dharahara Tower then and now: A history of earthquakes in Nepal</td>\n",
       "      <td>124174094</td>\n",
       "      <td>nepal_24</td>\n",
       "      <td>Noatodo</td>\n",
       "      <td>Sat Apr 25 11:35:04 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>64</td>\n",
       "      <td>dharahara tower history earthquakes nepal</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>592026556337291265</td>\n",
       "      <td>Nepal's historic Dharahara Tower collapses, trapping hundreds</td>\n",
       "      <td>603540902</td>\n",
       "      <td>nepal_25</td>\n",
       "      <td>NeedObamaAgain</td>\n",
       "      <td>Sat Apr 25 18:05:05 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>62</td>\n",
       "      <td>nepal historic dharahara tower collapses trapping hundreds</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>966</th>\n",
       "      <td>592125524362727424</td>\n",
       "      <td>Saturday's earthquake collapsed Nepal's nine-story Dharahara Tower, which was built in 1832</td>\n",
       "      <td>2748784320</td>\n",
       "      <td>nepal_31</td>\n",
       "      <td>said_alfaruq</td>\n",
       "      <td>Sun Apr 26 00:38:20 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>93</td>\n",
       "      <td>saturday earthquake collapsed nepal ninestory dharahara tower built</td>\n",
       "      <td>-0.2732</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>591987398969430016</td>\n",
       "      <td>Nepal's historic Dharahara Tower collapses in massive earthquake: The historic Dharahara tower, a landmark in ...</td>\n",
       "      <td>1467962250</td>\n",
       "      <td>nepal_25</td>\n",
       "      <td>Jkenton09</td>\n",
       "      <td>Sat Apr 25 15:29:29 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>114</td>\n",
       "      <td>nepal historic dharahara tower collapses massive earthquake historic dharahara tower landmark</td>\n",
       "      <td>-0.2263</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>591988802328993793</td>\n",
       "      <td>Nepal's historic Dharahara Tower collapses in massive earthquake  \\n\\nThe historic Dharahara tower, a landmark in Nep‚Ä¶</td>\n",
       "      <td>1480797793</td>\n",
       "      <td>nepal_25</td>\n",
       "      <td>lavonna_kaschel</td>\n",
       "      <td>Sat Apr 25 15:35:03 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>118</td>\n",
       "      <td>nepal historic dharahara tower collapses massive earthquake historic dharahara tower landmark nep</td>\n",
       "      <td>-0.2263</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1150</th>\n",
       "      <td>591987270586007552</td>\n",
       "      <td>Nepal's historic Dharahara Tower collapses in massive earthquake  via @mashable</td>\n",
       "      <td>434423875</td>\n",
       "      <td>nepal_25</td>\n",
       "      <td>CooeeMedia</td>\n",
       "      <td>Sat Apr 25 15:28:58 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>79</td>\n",
       "      <td>nepal historic dharahara tower collapses massive earthquake via mashable</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1192</th>\n",
       "      <td>592002355752603648</td>\n",
       "      <td>Nepal's historic Dharahara tower collapses in massive earthquake  via @mashable</td>\n",
       "      <td>1003393316</td>\n",
       "      <td>nepal_25</td>\n",
       "      <td>madoqin</td>\n",
       "      <td>Sat Apr 25 16:28:55 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>79</td>\n",
       "      <td>nepal historic dharahara tower collapses massive earthquake via mashable</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1202</th>\n",
       "      <td>591987544872615937</td>\n",
       "      <td>Nepal's historic Dharahara Tower collapses in massive earthquake</td>\n",
       "      <td>238501031</td>\n",
       "      <td>nepal_25</td>\n",
       "      <td>longvdit</td>\n",
       "      <td>Sat Apr 25 15:30:03 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>65</td>\n",
       "      <td>nepal historic dharahara tower collapses massive earthquake</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1296</th>\n",
       "      <td>591987589374050304</td>\n",
       "      <td>Nepal's historic Dharahara Tower collapses in massive earthquake</td>\n",
       "      <td>115981166</td>\n",
       "      <td>nepal_25</td>\n",
       "      <td>eddieqx</td>\n",
       "      <td>Sat Apr 25 15:30:14 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>66</td>\n",
       "      <td>nepal historic dharahara tower collapses massive earthquake</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>591987795671003136</td>\n",
       "      <td>Nepal's historic Dharahara Tower collapses in massive earthquake.  ‚Ä¶</td>\n",
       "      <td>2450372724</td>\n",
       "      <td>nepal_25</td>\n",
       "      <td>Ad3laTorres</td>\n",
       "      <td>Sat Apr 25 15:31:03 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>69</td>\n",
       "      <td>nepal historic dharahara tower collapses massive earthquake</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1540</th>\n",
       "      <td>592009167122104320</td>\n",
       "      <td>Nepal's historic Dharahara tower collapses in massive earthquake</td>\n",
       "      <td>89829260</td>\n",
       "      <td>nepal_25</td>\n",
       "      <td>TechKnowledgeIt</td>\n",
       "      <td>Sat Apr 25 16:55:59 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>65</td>\n",
       "      <td>nepal historic dharahara tower collapses massive earthquake</td>\n",
       "      <td>-0.2960</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1830</th>\n",
       "      <td>592025470801055744</td>\n",
       "      <td>Nepal's historic Dharahara tower collapses in massive earthquake: The historic Dharahara tower, a landmark in ...</td>\n",
       "      <td>197874698</td>\n",
       "      <td>nepal_25</td>\n",
       "      <td>marketmobileapp</td>\n",
       "      <td>Sat Apr 25 18:00:46 +0000 2015</td>\n",
       "      <td>real</td>\n",
       "      <td>114</td>\n",
       "      <td>nepal historic dharahara tower collapses massive earthquake historic dharahara tower landmark</td>\n",
       "      <td>-0.2263</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>532177410205450240</td>\n",
       "      <td>SYRIA! SYRIAN HERO BOY rescue girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ</td>\n",
       "      <td>1489520599</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>saadalkhaldi48</td>\n",
       "      <td>Tue Nov 11 14:25:56 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>78</td>\n",
       "      <td>syria syrian hero boy rescue girl shootout see syrian hero boy</td>\n",
       "      <td>0.8885</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1933</th>\n",
       "      <td>531833568402485248</td>\n",
       "      <td>SYRIA! SYRIAN HERO BOY resque girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ</td>\n",
       "      <td>224528604</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>alkh81</td>\n",
       "      <td>Mon Nov 10 15:39:38 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>78</td>\n",
       "      <td>syria syrian hero boy rescue girl shootout see syrian hero boy</td>\n",
       "      <td>0.8885</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1947</th>\n",
       "      <td>531917772075585536</td>\n",
       "      <td>SYRIA! SYRIAN HERO BOY rescue girl in shootout ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ</td>\n",
       "      <td>179486518</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>bmaladin</td>\n",
       "      <td>Mon Nov 10 21:14:14 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>66</td>\n",
       "      <td>syria syrian hero boy rescue girl shootout</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>531803194742280192</td>\n",
       "      <td>SYRIA! SYRIAN HERO BOY rescue girl in shootout  ÿßŸÑÿ∑ŸÅŸÑ ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ</td>\n",
       "      <td>474702914</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>ratebduma</td>\n",
       "      <td>Mon Nov 10 13:38:56 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>67</td>\n",
       "      <td>syria syrian hero boy rescue girl shootout</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>532103045564207104</td>\n",
       "      <td>SYRIA! SYRIAN HERO BOY rescue girl in shootout  ÿßŸÑÿ∑ŸÅŸÑ ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ:  v√≠a @YouTube</td>\n",
       "      <td>98995688</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>SFerriz</td>\n",
       "      <td>Tue Nov 11 09:30:26 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>81</td>\n",
       "      <td>syria syrian hero boy rescue girl shootout v√≠a youtube</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012</th>\n",
       "      <td>532105500083494912</td>\n",
       "      <td>SYRIA! SYRIAN HERO BOY rescue girl in shootout  ÿßŸÑÿ∑ŸÅŸÑ ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ:  ÿπÿ®ÿ± @YouTube</td>\n",
       "      <td>434708383</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>alaakanawatitah</td>\n",
       "      <td>Tue Nov 11 09:40:11 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>81</td>\n",
       "      <td>syria syrian hero boy rescue girl shootout syrian hero via youtube</td>\n",
       "      <td>0.8885</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>532157941210558466</td>\n",
       "      <td>SYRIA! SYRIAN HERO BOY rescue girl in shootout ÿßŸÑÿ∑ŸÅŸÑ ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ</td>\n",
       "      <td>28564316</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>MFS002</td>\n",
       "      <td>Tue Nov 11 13:08:34 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>66</td>\n",
       "      <td>syria syrian hero boy rescue girl shootout</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2090</th>\n",
       "      <td>532434757208928256</td>\n",
       "      <td>SYRIA! SYRIAN HERO BOY rescue girl in shootout ÿßŸÑÿ∑ŸÅŸÑ ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ - YouTube</td>\n",
       "      <td>2883016028</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>KLapangan</td>\n",
       "      <td>Wed Nov 12 07:28:32 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>76</td>\n",
       "      <td>syria syrian hero boy rescue girl shootout youtube</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2674</th>\n",
       "      <td>531842683895947264</td>\n",
       "      <td>SYRIA! SYRIAN HERO BOY rescue girl in shootout  ÿßŸÑÿ∑ŸÅŸÑ ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ:  via @YouTube</td>\n",
       "      <td>93410612</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>RonaldElzenga</td>\n",
       "      <td>Mon Nov 10 16:15:51 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>81</td>\n",
       "      <td>syria syrian hero boy rescue girl shootout syrian hero boy via youtube</td>\n",
       "      <td>0.8885</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2749</th>\n",
       "      <td>531804219234209795</td>\n",
       "      <td>ÿ¥ÿ®ŸÉÿ© ÿ¥ÿßŸÖ ÿßŸÑÿ•ÿÆÿ®ÿßÿ±Ÿäÿ© - SYRIAN HERO BOY rescue girl in shootout</td>\n",
       "      <td>1898637019</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>basheermorroco</td>\n",
       "      <td>Mon Nov 10 13:43:00 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>61</td>\n",
       "      <td>Sham News Network syrian hero boy rescue girl shootout</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2761</th>\n",
       "      <td>531867359594561536</td>\n",
       "      <td>SYRIA! SYRIAN HERO BOY rescue girl in shootout  ÿß‚Ä¶:</td>\n",
       "      <td>92623517</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>aalsibaie</td>\n",
       "      <td>Mon Nov 10 17:53:54 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>52</td>\n",
       "      <td>syria syrian hero boy rescue girl shootout ÿß</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2826</th>\n",
       "      <td>532170378496331776</td>\n",
       "      <td>Me ha gustado un v√≠deo de @YouTube de @snn_shaam_video ( - SYRIA! SYRIAN HERO BOY rescue girl in shootout</td>\n",
       "      <td>357397143</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>Evilinx</td>\n",
       "      <td>Tue Nov 11 13:58:00 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>105</td>\n",
       "      <td>liked youtube video son_sham_video syria syrian hero boy rescue girl shootout</td>\n",
       "      <td>0.8658</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>532361786847019009</td>\n",
       "      <td>Gostei de um v√≠deo @YouTube de @snn_shaam_video  SYRIA! SYRIAN HERO BOY rescue girl in shootout ÿßŸÑÿ∑ŸÅŸÑ</td>\n",
       "      <td>32640504</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>lu_Nephesh</td>\n",
       "      <td>Wed Nov 12 02:38:35 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>101</td>\n",
       "      <td>I liked youtube video son_sham_video syria syrian hero boy rescue girl shootout ÿßŸÑÿ∑ŸÅŸÑ</td>\n",
       "      <td>0.8658</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2970</th>\n",
       "      <td>532096973126795264</td>\n",
       "      <td>Watch: Syrian 'hero boy' appears to brave sniper fire to rescue terrified girl in dramatic video - via @Telegraph</td>\n",
       "      <td>257042320</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>dormansi</td>\n",
       "      <td>Tue Nov 11 09:06:18 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>114</td>\n",
       "      <td>watch syrian hero boy appears brave sniper fire rescue terrified girl dramatic video via telegraph</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2971</th>\n",
       "      <td>532097026364669952</td>\n",
       "      <td>Watch: Syrian 'hero boy' appears to brave sniper fire to rescue terrified girl in dramatic video - Telegraph</td>\n",
       "      <td>566884841</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>SyriaSketch</td>\n",
       "      <td>Tue Nov 11 09:06:31 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>109</td>\n",
       "      <td>watch syrian hero boy appears brave sniper fire rescue terrified girl dramatic video telegraph</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2974</th>\n",
       "      <td>532100565598081025</td>\n",
       "      <td>Watch: Syrian 'hero boy' appears to brave sniper fire to rescue terrified girl in dramatic video</td>\n",
       "      <td>152491277</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>TelurBalado</td>\n",
       "      <td>Tue Nov 11 09:20:35 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>97</td>\n",
       "      <td>watch syrian hero boy appears brave sniper fire rescue terrified girl dramatic video</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3006</th>\n",
       "      <td>532180249258127360</td>\n",
       "      <td>Watch: Syrian 'hero boy' appears to brave sniper fire to rescue terrified girl in dramatic video: Amateur vide...</td>\n",
       "      <td>21487168</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>jevives</td>\n",
       "      <td>Tue Nov 11 14:37:13 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>114</td>\n",
       "      <td>watch syrian hero boy appears brave sniper fire rescue terrified girl dramatic video amateur video</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3053</th>\n",
       "      <td>532248987068551169</td>\n",
       "      <td>Syrian ‚ÄòHero Boy‚Äô Appears To Brave Sniper Fire To Rescue Terrified Girl In Dramatic Video -</td>\n",
       "      <td>292777349</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>PzFeed</td>\n",
       "      <td>Tue Nov 11 19:10:21 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>93</td>\n",
       "      <td>syrian hero boy appears brave sniper fire rescue terrified girl dramatic video</td>\n",
       "      <td>0.5994</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3142</th>\n",
       "      <td>531797675717300224</td>\n",
       "      <td>SYRIA! SYRIAN HERO BOY rescue girl in shootout. S‚Ä¶:</td>\n",
       "      <td>2508630144</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>Rabe3Zahra</td>\n",
       "      <td>Mon Nov 10 13:17:00 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>52</td>\n",
       "      <td>syria syrian hero boy rescue girl shootout</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3329</th>\n",
       "      <td>531816269108879360</td>\n",
       "      <td>SYRIA! SYRIAN HERO BOY rescue girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä ...:  ÿπÿ®ÿ± @YouTube</td>\n",
       "      <td>389364879</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>ez_aldin</td>\n",
       "      <td>Mon Nov 10 14:30:53 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>90</td>\n",
       "      <td>syria syrian hero boy rescue girl shootout see syrian boy via youtube</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3361</th>\n",
       "      <td>531869500803526657</td>\n",
       "      <td>ÿ£ÿπÿ¨ÿ®ŸÜŸä ŸÅŸäÿØŸäŸà @YouTube ÿπŸÑŸâ  SYRIA! SYRIAN HERO BOY rescue girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä</td>\n",
       "      <td>361895079</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>pato7alqarni</td>\n",
       "      <td>Mon Nov 10 18:02:25 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>91</td>\n",
       "      <td>Like youtube video syria syrian hero boy rescue girl shootout see boy</td>\n",
       "      <td>0.8555</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3463</th>\n",
       "      <td>531808719646896128</td>\n",
       "      <td>I liked a @YouTube video  SYRIA! SYRIAN HERO BOY rescue girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä</td>\n",
       "      <td>248646136</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>Fc__barce1ona</td>\n",
       "      <td>Mon Nov 10 14:00:53 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>97</td>\n",
       "      <td>liked youtube video syria syrian hero boy rescue girl shootout see ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä</td>\n",
       "      <td>0.8658</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3466</th>\n",
       "      <td>531887994051915776</td>\n",
       "      <td>SYRIA! SYRIAN HERO BOY rescue girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä ...:  via @YouTube</td>\n",
       "      <td>539935564</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>elouarak</td>\n",
       "      <td>Mon Nov 10 19:15:54 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>90</td>\n",
       "      <td>syria syrian hero boy rescue girl shootout see syrian boy via youtube</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3509</th>\n",
       "      <td>532141698399232000</td>\n",
       "      <td>Me ha gustado un v√≠deo de @YouTube ( - SYRIA! SYRIAN HERO BOY rescue girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä</td>\n",
       "      <td>128160716</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>anitamari31</td>\n",
       "      <td>Tue Nov 11 12:04:02 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>103</td>\n",
       "      <td>ha estado un v√≠deo de youtube syria syrian hero boy rescue girl shootout see ÿßŸÑÿµÿ®Ÿä</td>\n",
       "      <td>0.8519</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3526</th>\n",
       "      <td>532180699713785856</td>\n",
       "      <td>SYRIA! SYRIAN HERO BOY rescue girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä ...:  v√≠a @YouTube</td>\n",
       "      <td>571962587</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>charlieerg</td>\n",
       "      <td>Tue Nov 11 14:39:00 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>90</td>\n",
       "      <td>syria syrian hero boy rescue girl shootout see syrian boy v√≠a youtube</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3559</th>\n",
       "      <td>532335699324710912</td>\n",
       "      <td>Gostei de um v√≠deo @YouTube  SYRIAN HERO BOY rescue girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä</td>\n",
       "      <td>40133004</td>\n",
       "      <td>syrianboy_1</td>\n",
       "      <td>tufoborges</td>\n",
       "      <td>Wed Nov 12 00:54:55 +0000 2014</td>\n",
       "      <td>fake</td>\n",
       "      <td>93</td>\n",
       "      <td>poster de um v√≠deo youtube syrian hero boy rescue girl shootout see syrian boy</td>\n",
       "      <td>0.7845</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3694</th>\n",
       "      <td>578420939890786305</td>\n",
       "      <td>Riesen Verwirrung um Varoufakis-Video ZDF-Comedian Jan B√∂hmermann: Ich habe den Mittelfinger gef√§lscht</td>\n",
       "      <td>2785050622</td>\n",
       "      <td>varoufakis_1</td>\n",
       "      <td>trackballzero</td>\n",
       "      <td>Thu Mar 19 05:01:13 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>103</td>\n",
       "      <td>huge confusion varoufakisvideo zdfcomedian jan b√∂herman falsified middle finger</td>\n",
       "      <td>-0.3612</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 tweetId  \\\n",
       "279   591407369332076544   \n",
       "287   591419653278212096   \n",
       "321   591486673608781826   \n",
       "348   591381989539782656   \n",
       "399   590783181323776000   \n",
       "414   591287539245408256   \n",
       "600   595426185724723201   \n",
       "853   591922990859825152   \n",
       "855   591928405769216000   \n",
       "940   592026556337291265   \n",
       "966   592125524362727424   \n",
       "981   591987398969430016   \n",
       "984   591988802328993793   \n",
       "1150  591987270586007552   \n",
       "1192  592002355752603648   \n",
       "1202  591987544872615937   \n",
       "1296  591987589374050304   \n",
       "1325  591987795671003136   \n",
       "1540  592009167122104320   \n",
       "1830  592025470801055744   \n",
       "1926  532177410205450240   \n",
       "1933  531833568402485248   \n",
       "1947  531917772075585536   \n",
       "1973  531803194742280192   \n",
       "2010  532103045564207104   \n",
       "2012  532105500083494912   \n",
       "2022  532157941210558466   \n",
       "2090  532434757208928256   \n",
       "2674  531842683895947264   \n",
       "2749  531804219234209795   \n",
       "2761  531867359594561536   \n",
       "2826  532170378496331776   \n",
       "2856  532361786847019009   \n",
       "2970  532096973126795264   \n",
       "2971  532097026364669952   \n",
       "2974  532100565598081025   \n",
       "3006  532180249258127360   \n",
       "3053  532248987068551169   \n",
       "3142  531797675717300224   \n",
       "3329  531816269108879360   \n",
       "3361  531869500803526657   \n",
       "3463  531808719646896128   \n",
       "3466  531887994051915776   \n",
       "3509  532141698399232000   \n",
       "3526  532180699713785856   \n",
       "3559  532335699324710912   \n",
       "3694  578420939890786305   \n",
       "\n",
       "                                                                                                                   tweetText  \\\n",
       "279                                                              'Samurai Ghost': Photo Shows Mysterious Boots Behind Girl     \n",
       "287                                                               'Samurai Ghost': Photo Shows Mysterious Boots Behind Girl    \n",
       "321                                              'Samurai Ghost': Photo shows mysterious boots behind girl #samurai #ghost.    \n",
       "348                                           'Samurai Ghost': Photo Shows Mysterious Boots Behind Girl - ABC News  via @ABC   \n",
       "399                                  Samurai ghost photobombed little girl. Can you see it? --&gt;  #GhoulishWednesday #KTM    \n",
       "414                                                      'Samurai Ghost': Photo Shows Mysterious Boots Behind Girl  via @GMA   \n",
       "600                                                                                                        #NepalEarthquake    \n",
       "853                                           Dharahara Tower Then and Now: A History of Earthquakes in Nepal  via @WSJIndia   \n",
       "855                                                         Dharahara Tower then and now: A history of earthquakes in Nepal    \n",
       "940                                                           Nepal's historic Dharahara Tower collapses, trapping hundreds    \n",
       "966                            Saturday's earthquake collapsed Nepal's nine-story Dharahara Tower, which was built in 1832     \n",
       "981       Nepal's historic Dharahara Tower collapses in massive earthquake: The historic Dharahara tower, a landmark in ...    \n",
       "984   Nepal's historic Dharahara Tower collapses in massive earthquake  \\n\\nThe historic Dharahara tower, a landmark in Nep‚Ä¶   \n",
       "1150                                         Nepal's historic Dharahara Tower collapses in massive earthquake  via @mashable   \n",
       "1192                                         Nepal's historic Dharahara tower collapses in massive earthquake  via @mashable   \n",
       "1202                                                       Nepal's historic Dharahara Tower collapses in massive earthquake    \n",
       "1296                                                      Nepal's historic Dharahara Tower collapses in massive earthquake     \n",
       "1325                                                   Nepal's historic Dharahara Tower collapses in massive earthquake.  ‚Ä¶    \n",
       "1540                                                       Nepal's historic Dharahara tower collapses in massive earthquake    \n",
       "1830      Nepal's historic Dharahara tower collapses in massive earthquake: The historic Dharahara tower, a landmark in ...    \n",
       "1926                                          SYRIA! SYRIAN HERO BOY rescue girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ    \n",
       "1933                                          SYRIA! SYRIAN HERO BOY resque girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ    \n",
       "1947                                                      SYRIA! SYRIAN HERO BOY rescue girl in shootout ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ    \n",
       "1973                                                     SYRIA! SYRIAN HERO BOY rescue girl in shootout  ÿßŸÑÿ∑ŸÅŸÑ ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ    \n",
       "2010                                       SYRIA! SYRIAN HERO BOY rescue girl in shootout  ÿßŸÑÿ∑ŸÅŸÑ ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ:  v√≠a @YouTube   \n",
       "2012                                       SYRIA! SYRIAN HERO BOY rescue girl in shootout  ÿßŸÑÿ∑ŸÅŸÑ ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ:  ÿπÿ®ÿ± @YouTube   \n",
       "2022                                                      SYRIA! SYRIAN HERO BOY rescue girl in shootout ÿßŸÑÿ∑ŸÅŸÑ ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ    \n",
       "2090                                            SYRIA! SYRIAN HERO BOY rescue girl in shootout ÿßŸÑÿ∑ŸÅŸÑ ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ - YouTube    \n",
       "2674                                       SYRIA! SYRIAN HERO BOY rescue girl in shootout  ÿßŸÑÿ∑ŸÅŸÑ ÿßŸÑÿ≥Ÿàÿ±Ÿä ÿßŸÑÿ®ÿ∑ŸÑ:  via @YouTube   \n",
       "2749                                                           ÿ¥ÿ®ŸÉÿ© ÿ¥ÿßŸÖ ÿßŸÑÿ•ÿÆÿ®ÿßÿ±Ÿäÿ© - SYRIAN HERO BOY rescue girl in shootout    \n",
       "2761                                                                    SYRIA! SYRIAN HERO BOY rescue girl in shootout  ÿß‚Ä¶:    \n",
       "2826               Me ha gustado un v√≠deo de @YouTube de @snn_shaam_video ( - SYRIA! SYRIAN HERO BOY rescue girl in shootout   \n",
       "2856                   Gostei de um v√≠deo @YouTube de @snn_shaam_video  SYRIA! SYRIAN HERO BOY rescue girl in shootout ÿßŸÑÿ∑ŸÅŸÑ   \n",
       "2970      Watch: Syrian 'hero boy' appears to brave sniper fire to rescue terrified girl in dramatic video - via @Telegraph    \n",
       "2971            Watch: Syrian 'hero boy' appears to brave sniper fire to rescue terrified girl in dramatic video - Telegraph   \n",
       "2974                       Watch: Syrian 'hero boy' appears to brave sniper fire to rescue terrified girl in dramatic video    \n",
       "3006      Watch: Syrian 'hero boy' appears to brave sniper fire to rescue terrified girl in dramatic video: Amateur vide...    \n",
       "3053                           Syrian ‚ÄòHero Boy‚Äô Appears To Brave Sniper Fire To Rescue Terrified Girl In Dramatic Video -     \n",
       "3142                                                                    SYRIA! SYRIAN HERO BOY rescue girl in shootout. S‚Ä¶:    \n",
       "3329                              SYRIA! SYRIAN HERO BOY rescue girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä ...:  ÿπÿ®ÿ± @YouTube   \n",
       "3361                             ÿ£ÿπÿ¨ÿ®ŸÜŸä ŸÅŸäÿØŸäŸà @YouTube ÿπŸÑŸâ  SYRIA! SYRIAN HERO BOY rescue girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä   \n",
       "3463                       I liked a @YouTube video  SYRIA! SYRIAN HERO BOY rescue girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä   \n",
       "3466                              SYRIA! SYRIAN HERO BOY rescue girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä ...:  via @YouTube   \n",
       "3509                 Me ha gustado un v√≠deo de @YouTube ( - SYRIA! SYRIAN HERO BOY rescue girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä   \n",
       "3526                              SYRIA! SYRIAN HERO BOY rescue girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä ...:  v√≠a @YouTube   \n",
       "3559                           Gostei de um v√≠deo @YouTube  SYRIAN HERO BOY rescue girl in shootout. SEE THIS!! ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä   \n",
       "3694                 Riesen Verwirrung um Varoufakis-Video ZDF-Comedian Jan B√∂hmermann: Ich habe den Mittelfinger gef√§lscht    \n",
       "\n",
       "          userId             imageId(s)         username  \\\n",
       "279    626592883  samurai_01,samurai_02    anomalistnews   \n",
       "287    712620543  samurai_01,samurai_02          kazem5t   \n",
       "321    273031537             samurai_02         tragab77   \n",
       "348     73293640  samurai_01,samurai_02         rsun0525   \n",
       "399   2650465766  samurai_01,samurai_02    CaitlinBell97   \n",
       "414    467596457             samurai_02   MaryClayAuthor   \n",
       "600   2230961521              nepal_01        Cronopio60   \n",
       "853    976449991               nepal_24        PPatel108   \n",
       "855    124174094               nepal_24          Noatodo   \n",
       "940    603540902               nepal_25   NeedObamaAgain   \n",
       "966   2748784320               nepal_31     said_alfaruq   \n",
       "981   1467962250               nepal_25        Jkenton09   \n",
       "984   1480797793               nepal_25  lavonna_kaschel   \n",
       "1150   434423875               nepal_25       CooeeMedia   \n",
       "1192  1003393316               nepal_25          madoqin   \n",
       "1202   238501031               nepal_25         longvdit   \n",
       "1296   115981166               nepal_25          eddieqx   \n",
       "1325  2450372724               nepal_25      Ad3laTorres   \n",
       "1540    89829260               nepal_25  TechKnowledgeIt   \n",
       "1830   197874698               nepal_25  marketmobileapp   \n",
       "1926  1489520599            syrianboy_1   saadalkhaldi48   \n",
       "1933   224528604            syrianboy_1           alkh81   \n",
       "1947   179486518            syrianboy_1         bmaladin   \n",
       "1973   474702914            syrianboy_1        ratebduma   \n",
       "2010    98995688            syrianboy_1          SFerriz   \n",
       "2012   434708383            syrianboy_1  alaakanawatitah   \n",
       "2022    28564316            syrianboy_1           MFS002   \n",
       "2090  2883016028            syrianboy_1        KLapangan   \n",
       "2674    93410612            syrianboy_1    RonaldElzenga   \n",
       "2749  1898637019            syrianboy_1   basheermorroco   \n",
       "2761    92623517            syrianboy_1        aalsibaie   \n",
       "2826   357397143            syrianboy_1          Evilinx   \n",
       "2856    32640504            syrianboy_1       lu_Nephesh   \n",
       "2970   257042320            syrianboy_1         dormansi   \n",
       "2971   566884841            syrianboy_1      SyriaSketch   \n",
       "2974   152491277            syrianboy_1      TelurBalado   \n",
       "3006    21487168            syrianboy_1          jevives   \n",
       "3053   292777349            syrianboy_1           PzFeed   \n",
       "3142  2508630144            syrianboy_1       Rabe3Zahra   \n",
       "3329   389364879            syrianboy_1         ez_aldin   \n",
       "3361   361895079            syrianboy_1     pato7alqarni   \n",
       "3463   248646136            syrianboy_1    Fc__barce1ona   \n",
       "3466   539935564            syrianboy_1         elouarak   \n",
       "3509   128160716            syrianboy_1      anitamari31   \n",
       "3526   571962587            syrianboy_1       charlieerg   \n",
       "3559    40133004            syrianboy_1       tufoborges   \n",
       "3694  2785050622           varoufakis_1    trackballzero   \n",
       "\n",
       "                           timestamp label  length  \\\n",
       "279   Fri Apr 24 01:04:39 +0000 2015  fake      59   \n",
       "287   Fri Apr 24 01:53:28 +0000 2015  fake      58   \n",
       "321   Fri Apr 24 06:19:46 +0000 2015  fake      75   \n",
       "348   Thu Apr 23 23:23:48 +0000 2015  fake      78   \n",
       "399   Wed Apr 22 07:44:21 +0000 2015  fake      87   \n",
       "414   Thu Apr 23 17:08:29 +0000 2015  fake      67   \n",
       "600   Tue May 05 03:13:59 +0000 2015  fake      17   \n",
       "853   Sat Apr 25 11:13:33 +0000 2015  real      78   \n",
       "855   Sat Apr 25 11:35:04 +0000 2015  real      64   \n",
       "940   Sat Apr 25 18:05:05 +0000 2015  real      62   \n",
       "966   Sun Apr 26 00:38:20 +0000 2015  real      93   \n",
       "981   Sat Apr 25 15:29:29 +0000 2015  real     114   \n",
       "984   Sat Apr 25 15:35:03 +0000 2015  real     118   \n",
       "1150  Sat Apr 25 15:28:58 +0000 2015  real      79   \n",
       "1192  Sat Apr 25 16:28:55 +0000 2015  real      79   \n",
       "1202  Sat Apr 25 15:30:03 +0000 2015  real      65   \n",
       "1296  Sat Apr 25 15:30:14 +0000 2015  real      66   \n",
       "1325  Sat Apr 25 15:31:03 +0000 2015  real      69   \n",
       "1540  Sat Apr 25 16:55:59 +0000 2015  real      65   \n",
       "1830  Sat Apr 25 18:00:46 +0000 2015  real     114   \n",
       "1926  Tue Nov 11 14:25:56 +0000 2014  fake      78   \n",
       "1933  Mon Nov 10 15:39:38 +0000 2014  fake      78   \n",
       "1947  Mon Nov 10 21:14:14 +0000 2014  fake      66   \n",
       "1973  Mon Nov 10 13:38:56 +0000 2014  fake      67   \n",
       "2010  Tue Nov 11 09:30:26 +0000 2014  fake      81   \n",
       "2012  Tue Nov 11 09:40:11 +0000 2014  fake      81   \n",
       "2022  Tue Nov 11 13:08:34 +0000 2014  fake      66   \n",
       "2090  Wed Nov 12 07:28:32 +0000 2014  fake      76   \n",
       "2674  Mon Nov 10 16:15:51 +0000 2014  fake      81   \n",
       "2749  Mon Nov 10 13:43:00 +0000 2014  fake      61   \n",
       "2761  Mon Nov 10 17:53:54 +0000 2014  fake      52   \n",
       "2826  Tue Nov 11 13:58:00 +0000 2014  fake     105   \n",
       "2856  Wed Nov 12 02:38:35 +0000 2014  fake     101   \n",
       "2970  Tue Nov 11 09:06:18 +0000 2014  fake     114   \n",
       "2971  Tue Nov 11 09:06:31 +0000 2014  fake     109   \n",
       "2974  Tue Nov 11 09:20:35 +0000 2014  fake      97   \n",
       "3006  Tue Nov 11 14:37:13 +0000 2014  fake     114   \n",
       "3053  Tue Nov 11 19:10:21 +0000 2014  fake      93   \n",
       "3142  Mon Nov 10 13:17:00 +0000 2014  fake      52   \n",
       "3329  Mon Nov 10 14:30:53 +0000 2014  fake      90   \n",
       "3361  Mon Nov 10 18:02:25 +0000 2014  fake      91   \n",
       "3463  Mon Nov 10 14:00:53 +0000 2014  fake      97   \n",
       "3466  Mon Nov 10 19:15:54 +0000 2014  fake      90   \n",
       "3509  Tue Nov 11 12:04:02 +0000 2014  fake     103   \n",
       "3526  Tue Nov 11 14:39:00 +0000 2014  fake      90   \n",
       "3559  Wed Nov 12 00:54:55 +0000 2014  fake      93   \n",
       "3694  Thu Mar 19 05:01:13 +0000 2015  fake     103   \n",
       "\n",
       "                                                                                               cleanText  \\\n",
       "279                                               samurai ghost photo shows mysterious boots behind girl   \n",
       "287                                               samurai ghost photo shows mysterious boots behind girl   \n",
       "321                                 samurai ghost photo shows mysterious boots behind girl samurai ghost   \n",
       "348                              samurai ghost photo shows mysterious boots behind girl abc news via abc   \n",
       "399                                       samurai ghost photobombed little girl see ghoulishwednesday km   \n",
       "414                                       samurai ghost photo shows mysterious boots behind girl via gma   \n",
       "600                                                                                      nepalearthquake   \n",
       "853                                               dharahara tower history earthquakes nepal via wsjindia   \n",
       "855                                                            dharahara tower history earthquakes nepal   \n",
       "940                                           nepal historic dharahara tower collapses trapping hundreds   \n",
       "966                                  saturday earthquake collapsed nepal ninestory dharahara tower built   \n",
       "981        nepal historic dharahara tower collapses massive earthquake historic dharahara tower landmark   \n",
       "984    nepal historic dharahara tower collapses massive earthquake historic dharahara tower landmark nep   \n",
       "1150                            nepal historic dharahara tower collapses massive earthquake via mashable   \n",
       "1192                            nepal historic dharahara tower collapses massive earthquake via mashable   \n",
       "1202                                         nepal historic dharahara tower collapses massive earthquake   \n",
       "1296                                         nepal historic dharahara tower collapses massive earthquake   \n",
       "1325                                         nepal historic dharahara tower collapses massive earthquake   \n",
       "1540                                         nepal historic dharahara tower collapses massive earthquake   \n",
       "1830       nepal historic dharahara tower collapses massive earthquake historic dharahara tower landmark   \n",
       "1926                                      syria syrian hero boy rescue girl shootout see syrian hero boy   \n",
       "1933                                      syria syrian hero boy rescue girl shootout see syrian hero boy   \n",
       "1947                                                          syria syrian hero boy rescue girl shootout   \n",
       "1973                                                          syria syrian hero boy rescue girl shootout   \n",
       "2010                                              syria syrian hero boy rescue girl shootout v√≠a youtube   \n",
       "2012                                  syria syrian hero boy rescue girl shootout syrian hero via youtube   \n",
       "2022                                                          syria syrian hero boy rescue girl shootout   \n",
       "2090                                                  syria syrian hero boy rescue girl shootout youtube   \n",
       "2674                              syria syrian hero boy rescue girl shootout syrian hero boy via youtube   \n",
       "2749                                              Sham News Network syrian hero boy rescue girl shootout   \n",
       "2761                                                        syria syrian hero boy rescue girl shootout ÿß   \n",
       "2826                       liked youtube video son_sham_video syria syrian hero boy rescue girl shootout   \n",
       "2856               I liked youtube video son_sham_video syria syrian hero boy rescue girl shootout ÿßŸÑÿ∑ŸÅŸÑ   \n",
       "2970  watch syrian hero boy appears brave sniper fire rescue terrified girl dramatic video via telegraph   \n",
       "2971      watch syrian hero boy appears brave sniper fire rescue terrified girl dramatic video telegraph   \n",
       "2974                watch syrian hero boy appears brave sniper fire rescue terrified girl dramatic video   \n",
       "3006  watch syrian hero boy appears brave sniper fire rescue terrified girl dramatic video amateur video   \n",
       "3053                      syrian hero boy appears brave sniper fire rescue terrified girl dramatic video   \n",
       "3142                                                          syria syrian hero boy rescue girl shootout   \n",
       "3329                               syria syrian hero boy rescue girl shootout see syrian boy via youtube   \n",
       "3361                               Like youtube video syria syrian hero boy rescue girl shootout see boy   \n",
       "3463                     liked youtube video syria syrian hero boy rescue girl shootout see ÿßŸÑÿµÿ®Ÿä ÿßŸÑÿ≥Ÿàÿ±Ÿä   \n",
       "3466                               syria syrian hero boy rescue girl shootout see syrian boy via youtube   \n",
       "3509                  ha estado un v√≠deo de youtube syria syrian hero boy rescue girl shootout see ÿßŸÑÿµÿ®Ÿä   \n",
       "3526                               syria syrian hero boy rescue girl shootout see syrian boy v√≠a youtube   \n",
       "3559                      poster de um v√≠deo youtube syrian hero boy rescue girl shootout see syrian boy   \n",
       "3694                     huge confusion varoufakisvideo zdfcomedian jan b√∂herman falsified middle finger   \n",
       "\n",
       "      sentimentScore  frequency  \n",
       "279          -0.3182          8  \n",
       "287          -0.3182         11  \n",
       "321          -0.5574         11  \n",
       "348          -0.3182         25  \n",
       "399          -0.3182          7  \n",
       "414          -0.3182         50  \n",
       "600           0.0000          9  \n",
       "853           0.0000          6  \n",
       "855           0.0000          8  \n",
       "940          -0.2960         19  \n",
       "966          -0.2732          7  \n",
       "981          -0.2263        205  \n",
       "984          -0.2263          7  \n",
       "1150         -0.2960         10  \n",
       "1192         -0.2960          6  \n",
       "1202         -0.2960        245  \n",
       "1296         -0.2960         17  \n",
       "1325         -0.2960          6  \n",
       "1540         -0.2960         10  \n",
       "1830         -0.2263         11  \n",
       "1926          0.8885        199  \n",
       "1933          0.8885         63  \n",
       "1947          0.7845         36  \n",
       "1973          0.7845         30  \n",
       "2010          0.7845         24  \n",
       "2012          0.8885          7  \n",
       "2022          0.7845         18  \n",
       "2090          0.7845          6  \n",
       "2674          0.8885         53  \n",
       "2749          0.7845          6  \n",
       "2761          0.7845         24  \n",
       "2826          0.8658         19  \n",
       "2856          0.8658          7  \n",
       "2970          0.5994         14  \n",
       "2971          0.5994          8  \n",
       "2974          0.5994         12  \n",
       "3006          0.5994         18  \n",
       "3053          0.5994          6  \n",
       "3142          0.7845         84  \n",
       "3329          0.7845         21  \n",
       "3361          0.8555        100  \n",
       "3463          0.8658         40  \n",
       "3466          0.7845         35  \n",
       "3509          0.8519         10  \n",
       "3526          0.7845          7  \n",
       "3559          0.7845          6  \n",
       "3694         -0.3612         11  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extraction of feature - frequency count, then remove duplicates\n",
    "test_data['frequency'] = test_data['tweetText'].map(test_data['tweetText'].value_counts())\n",
    "test_data['frequency'].value_counts()\n",
    "test_data.drop_duplicates(subset=['tweetText'], keep='first', inplace=True)\n",
    "test_data[test_data['frequency'] > 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bed0aac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 11738 entries, 0 to 14261\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   tweetId         11738 non-null  int64  \n",
      " 1   tweetText       11737 non-null  object \n",
      " 2   userId          11738 non-null  int64  \n",
      " 3   imageId(s)      11738 non-null  object \n",
      " 4   username        11738 non-null  object \n",
      " 5   timestamp       11738 non-null  object \n",
      " 6   label           11738 non-null  object \n",
      " 7   length          11738 non-null  int64  \n",
      " 8   cleanText       11737 non-null  object \n",
      " 9   sentimentScore  11738 non-null  float64\n",
      " 10  frequency       11737 non-null  float64\n",
      "dtypes: float64(2), int64(3), object(6)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "608d9196",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete tweets with empty text\n",
    "train_data[train_data['length'] == 0]\n",
    "train_data.drop(train_data[train_data.length == 0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "87543fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete tweets with empty text\n",
    "test_data[test_data['length'] == 0]\n",
    "test_data.drop(test_data[test_data.length == 0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "84c50cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>sentimentScore</th>\n",
       "      <th>frequency</th>\n",
       "      <th>hashtagCount</th>\n",
       "      <th>mentionCount</th>\n",
       "      <th>exclamationMarkCount</th>\n",
       "      <th>numberCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263046056240115712</td>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy.</td>\n",
       "      <td>21226711</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>iAnnieM</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>114</td>\n",
       "      <td>remember movie day tomorrow reminds happening hurricane sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>262995061304852481</td>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\"  REAL! RT.</td>\n",
       "      <td>192378571</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>CarlosVerareal</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>113</td>\n",
       "      <td>milenagimon look sandy ny tremendous image hurricane looks like independence day real rt</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262979898002534400</td>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy</td>\n",
       "      <td>132303095</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>LucasPalape</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>96</td>\n",
       "      <td>good picture hurricane sandy reminds movie independence day id sandy</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262996108400271360</td>\n",
       "      <td>Scary shit #hurricane #NY</td>\n",
       "      <td>241995902</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Haaaaarryyy</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>26</td>\n",
       "      <td>scary shit hurricane ny</td>\n",
       "      <td>-0.7783</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263018881839411200</td>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ</td>\n",
       "      <td>250315890</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>princess__natt</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>69</td>\n",
       "      <td>have place world nyc hurricane sandy statueofliberty</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>263364439582060545</td>\n",
       "      <td>42nd #time #square #NYC #subway #hurricane</td>\n",
       "      <td>163674788</td>\n",
       "      <td>sandyA_fake_23</td>\n",
       "      <td>classycg</td>\n",
       "      <td>Tue Oct 30 19:39:10 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>43</td>\n",
       "      <td>nd time square nyc subway hurricane</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>262927032705490944</td>\n",
       "      <td>Just in time for #halloween a photo of #hurricane #sandy #frankenstorm</td>\n",
       "      <td>246153081</td>\n",
       "      <td>sandyA_fake_14</td>\n",
       "      <td>j_unit87</td>\n",
       "      <td>Mon Oct 29 14:41:04 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>71</td>\n",
       "      <td>time halloween photo hurricane sandy frankenstorm</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>263321078884077568</td>\n",
       "      <td>Crazy pic of #Hurricane #Sandy prayers go out to family and friends on the East Coast</td>\n",
       "      <td>199565482</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>MrBlakMagik</td>\n",
       "      <td>Tue Oct 30 16:46:52 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>86</td>\n",
       "      <td>crazy pic hurricane sandy prayers go family friends east coast</td>\n",
       "      <td>0.1779</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>263111677485142017</td>\n",
       "      <td>#sandy #newyork #hurricane #statueofliberty #USA</td>\n",
       "      <td>78475739</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>safi37</td>\n",
       "      <td>Tue Oct 30 02:54:46 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>49</td>\n",
       "      <td>sandy newyork hurricane statueofliberty usa</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>262977091983785985</td>\n",
       "      <td>#nyc #hurricane</td>\n",
       "      <td>869777653</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>kingmichael03</td>\n",
       "      <td>Mon Oct 29 17:59:59 +0000 2012</td>\n",
       "      <td>fake</td>\n",
       "      <td>16</td>\n",
       "      <td>nyc hurricane</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId  \\\n",
       "0  263046056240115712   \n",
       "1  262995061304852481   \n",
       "2  262979898002534400   \n",
       "3  262996108400271360   \n",
       "4  263018881839411200   \n",
       "5  263364439582060545   \n",
       "6  262927032705490944   \n",
       "7  263321078884077568   \n",
       "8  263111677485142017   \n",
       "9  262977091983785985   \n",
       "\n",
       "                                                                                                            tweetText  \\\n",
       "0  ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy.    \n",
       "1   @milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\"  REAL! RT.   \n",
       "2                    Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy     \n",
       "3                                                                                          Scary shit #hurricane #NY    \n",
       "4                                               My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ    \n",
       "5                                                                         42nd #time #square #NYC #subway #hurricane    \n",
       "6                                             Just in time for #halloween a photo of #hurricane #sandy #frankenstorm    \n",
       "7                              Crazy pic of #Hurricane #Sandy prayers go out to family and friends on the East Coast    \n",
       "8                                                                   #sandy #newyork #hurricane #statueofliberty #USA    \n",
       "9                                                                                                    #nyc #hurricane    \n",
       "\n",
       "      userId      imageId(s)        username                       timestamp  \\\n",
       "0   21226711  sandyA_fake_46         iAnnieM  Mon Oct 29 22:34:01 +0000 2012   \n",
       "1  192378571  sandyA_fake_09  CarlosVerareal  Mon Oct 29 19:11:23 +0000 2012   \n",
       "2  132303095  sandyA_fake_09     LucasPalape  Mon Oct 29 18:11:08 +0000 2012   \n",
       "3  241995902  sandyA_fake_29     Haaaaarryyy  Mon Oct 29 19:15:33 +0000 2012   \n",
       "4  250315890  sandyA_fake_15  princess__natt  Mon Oct 29 20:46:02 +0000 2012   \n",
       "5  163674788  sandyA_fake_23        classycg  Tue Oct 30 19:39:10 +0000 2012   \n",
       "6  246153081  sandyA_fake_14        j_unit87  Mon Oct 29 14:41:04 +0000 2012   \n",
       "7  199565482  sandyA_fake_29     MrBlakMagik  Tue Oct 30 16:46:52 +0000 2012   \n",
       "8   78475739  sandyA_fake_15          safi37  Tue Oct 30 02:54:46 +0000 2012   \n",
       "9  869777653  sandyA_fake_29   kingmichael03  Mon Oct 29 17:59:59 +0000 2012   \n",
       "\n",
       "  label  length  \\\n",
       "0  fake     114   \n",
       "1  fake     113   \n",
       "2  fake      96   \n",
       "3  fake      26   \n",
       "4  fake      69   \n",
       "5  fake      43   \n",
       "6  fake      71   \n",
       "7  fake      86   \n",
       "8  fake      49   \n",
       "9  fake      16   \n",
       "\n",
       "                                                                                  cleanText  \\\n",
       "0                             remember movie day tomorrow reminds happening hurricane sandy   \n",
       "1  milenagimon look sandy ny tremendous image hurricane looks like independence day real rt   \n",
       "2                      good picture hurricane sandy reminds movie independence day id sandy   \n",
       "3                                                                   scary shit hurricane ny   \n",
       "4                                      have place world nyc hurricane sandy statueofliberty   \n",
       "5                                                       nd time square nyc subway hurricane   \n",
       "6                                         time halloween photo hurricane sandy frankenstorm   \n",
       "7                            crazy pic hurricane sandy prayers go family friends east coast   \n",
       "8                                               sandy newyork hurricane statueofliberty usa   \n",
       "9                                                                             nyc hurricane   \n",
       "\n",
       "   sentimentScore  frequency  hashtagCount  mentionCount  \\\n",
       "0          0.0000        1.0             1             0   \n",
       "1          0.3612        1.0             0             1   \n",
       "2          0.4404        1.0             2             0   \n",
       "3         -0.7783        1.0             2             0   \n",
       "4          0.0000        1.0             4             0   \n",
       "5          0.0000        1.0             5             0   \n",
       "6          0.0000        1.0             4             0   \n",
       "7          0.1779        1.0             2             0   \n",
       "8          0.0000        1.0             5             0   \n",
       "9          0.0000        1.0             2             0   \n",
       "\n",
       "   exclamationMarkCount  numberCount  \n",
       "0                     0            0  \n",
       "1                     2            1  \n",
       "2                     0            1  \n",
       "3                     0            0  \n",
       "4                     0            0  \n",
       "5                     0            2  \n",
       "6                     0            0  \n",
       "7                     0            0  \n",
       "8                     0            0  \n",
       "9                     0            0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extraction of other features - number of #s, number of @s, number of numbers, number of !s\n",
    "def get_symbol_count(text,symbol):\n",
    "    return text.count(symbol)\n",
    "\n",
    "def get_num_numbers(text):\n",
    "    return len(re.sub(\"[^0-9]\", \"\", text))\n",
    "\n",
    "#string = '¬øSe a_cu4erdan de la pel!√≠cula: ‚ÄúEl #d√≠6a despu√©s d7e ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN'\n",
    "#get_symbol_count(string,'!')\n",
    "#print(get_num_numbers(string))\n",
    "\n",
    "train_data['hashtagCount']= train_data['tweetText'].apply(lambda x: get_symbol_count(x,'#'))\n",
    "train_data['mentionCount']= train_data['tweetText'].apply(lambda x: get_symbol_count(x,'@'))\n",
    "train_data['exclamationMarkCount']= train_data['tweetText'].apply(lambda x: get_symbol_count(x,'!'))\n",
    "train_data['numberCount']= train_data['tweetText'].apply(lambda x: get_num_numbers(x))\n",
    "\n",
    "train_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b05f33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0     10714\n",
       "2.0       688\n",
       "3.0       144\n",
       "4.0        49\n",
       "5.0        35\n",
       "6.0        22\n",
       "8.0        13\n",
       "7.0        12\n",
       "12.0        9\n",
       "11.0        8\n",
       "10.0        8\n",
       "13.0        6\n",
       "14.0        4\n",
       "17.0        4\n",
       "15.0        3\n",
       "36.0        2\n",
       "9.0         2\n",
       "32.0        2\n",
       "19.0        2\n",
       "27.0        2\n",
       "16.0        2\n",
       "23.0        1\n",
       "42.0        1\n",
       "33.0        1\n",
       "70.0        1\n",
       "29.0        1\n",
       "50.0        1\n",
       "Name: frequency, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['frequency'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5a2800dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>sentimentScore</th>\n",
       "      <th>frequency</th>\n",
       "      <th>hashtagCount</th>\n",
       "      <th>mentionCount</th>\n",
       "      <th>exclamationMarkCount</th>\n",
       "      <th>numberCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>578854927457349632</td>\n",
       "      <td>kereeen RT @Shyman33: Eclipse from ISS....</td>\n",
       "      <td>70824972</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>peay_s</td>\n",
       "      <td>Fri Mar 20 09:45:43 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>43</td>\n",
       "      <td>kereeen rt shaman eclipse iss</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578874632670953472</td>\n",
       "      <td>Absolutely beautiful! RT @Shyman33: Eclipse from ISS....</td>\n",
       "      <td>344707006</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>JaredUcanChange</td>\n",
       "      <td>Fri Mar 20 11:04:02 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>57</td>\n",
       "      <td>absolutely beautiful rt shaman eclipse iss</td>\n",
       "      <td>0.6361</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>578891261353984000</td>\n",
       "      <td>‚Äú@Shyman33: Eclipse from ISS....  Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!</td>\n",
       "      <td>224839607</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>tpjp1231</td>\n",
       "      <td>Fri Mar 20 12:10:06 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>61</td>\n",
       "      <td>shaman eclipse iss eclipse seen space wow amazing</td>\n",
       "      <td>0.8225</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578846612312748032</td>\n",
       "      <td>Eclipse from ISS....</td>\n",
       "      <td>134543073</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Shyman33</td>\n",
       "      <td>Fri Mar 20 09:12:41 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>21</td>\n",
       "      <td>eclipse iss</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>578975333841551360</td>\n",
       "      <td>@ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç</td>\n",
       "      <td>1150728872</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Epimethee_</td>\n",
       "      <td>Fri Mar 20 17:44:11 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>87</td>\n",
       "      <td>ebonfigli eclipse view list something else divine creation limit</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>579274670853226496</td>\n",
       "      <td>‚Äú@ebonfigli: √âclipse vue de l'ISS... Autre chose...</td>\n",
       "      <td>470889709</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>BusineMi</td>\n",
       "      <td>Sat Mar 21 13:33:38 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>52</td>\n",
       "      <td>ebonfigli eclipse view list something else</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>578861590482665472</td>\n",
       "      <td>√âclipse vue de l'ISS... Autre chose...</td>\n",
       "      <td>383831305</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>ebonfigli</td>\n",
       "      <td>Fri Mar 20 10:12:12 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>39</td>\n",
       "      <td>eclipse view list something else</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>578844275061981184</td>\n",
       "      <td>Dit dus \\0/ RT ‚Äú@News_Executive: The Solar eclipse seen from International Space Station. #SolarEclipse #ISS #Space</td>\n",
       "      <td>291020879</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>PatriciaKusters</td>\n",
       "      <td>Fri Mar 20 09:03:24 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>116</td>\n",
       "      <td>dit us rt news_executive solar eclipse seen international space station solareclipse iss space</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>578838737448235008</td>\n",
       "      <td>Photo: The Solar eclipse as seen from the International Space Station. #SolarEclipse #ISS #Space</td>\n",
       "      <td>364810202</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>News_Executive</td>\n",
       "      <td>Fri Mar 20 08:41:23 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>97</td>\n",
       "      <td>photo solar eclipse seen international space station solareclipse iss space</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>579130328339623936</td>\n",
       "      <td>‚Äú@planetepics: A solar eclipse, viewed from the I.S.S.  cool. Eclipse from ISS</td>\n",
       "      <td>2187345679</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>CowlesR</td>\n",
       "      <td>Sat Mar 21 04:00:04 +0000 2015</td>\n",
       "      <td>fake</td>\n",
       "      <td>78</td>\n",
       "      <td>planetepics solar eclipse viewed iss cool eclipse iss</td>\n",
       "      <td>0.3182</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               tweetId  \\\n",
       "0   578854927457349632   \n",
       "1   578874632670953472   \n",
       "2   578891261353984000   \n",
       "3   578846612312748032   \n",
       "4   578975333841551360   \n",
       "5   579274670853226496   \n",
       "6   578861590482665472   \n",
       "8   578844275061981184   \n",
       "9   578838737448235008   \n",
       "10  579130328339623936   \n",
       "\n",
       "                                                                                                               tweetText  \\\n",
       "0                                                                            kereeen RT @Shyman33: Eclipse from ISS....    \n",
       "1                                                              Absolutely beautiful! RT @Shyman33: Eclipse from ISS....    \n",
       "2                                                          ‚Äú@Shyman33: Eclipse from ISS....  Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!   \n",
       "3                                                                                                  Eclipse from ISS....    \n",
       "4                                @ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç   \n",
       "5                                                                   ‚Äú@ebonfigli: √âclipse vue de l'ISS... Autre chose...    \n",
       "6                                                                                √âclipse vue de l'ISS... Autre chose...    \n",
       "8   Dit dus \\0/ RT ‚Äú@News_Executive: The Solar eclipse seen from International Space Station. #SolarEclipse #ISS #Space    \n",
       "9                      Photo: The Solar eclipse as seen from the International Space Station. #SolarEclipse #ISS #Space    \n",
       "10                                        ‚Äú@planetepics: A solar eclipse, viewed from the I.S.S.  cool. Eclipse from ISS   \n",
       "\n",
       "        userId   imageId(s)         username                       timestamp  \\\n",
       "0     70824972  eclipse_01            peay_s  Fri Mar 20 09:45:43 +0000 2015   \n",
       "1    344707006  eclipse_01   JaredUcanChange  Fri Mar 20 11:04:02 +0000 2015   \n",
       "2    224839607  eclipse_01          tpjp1231  Fri Mar 20 12:10:06 +0000 2015   \n",
       "3    134543073  eclipse_01          Shyman33  Fri Mar 20 09:12:41 +0000 2015   \n",
       "4   1150728872   eclipse_01       Epimethee_  Fri Mar 20 17:44:11 +0000 2015   \n",
       "5    470889709  eclipse_01          BusineMi  Sat Mar 21 13:33:38 +0000 2015   \n",
       "6    383831305  eclipse_01         ebonfigli  Fri Mar 20 10:12:12 +0000 2015   \n",
       "8    291020879   eclipse_01  PatriciaKusters  Fri Mar 20 09:03:24 +0000 2015   \n",
       "9    364810202   eclipse_01   News_Executive  Fri Mar 20 08:41:23 +0000 2015   \n",
       "10  2187345679  eclipse_01           CowlesR  Sat Mar 21 04:00:04 +0000 2015   \n",
       "\n",
       "   label  length  \\\n",
       "0   fake      43   \n",
       "1   fake      57   \n",
       "2   fake      61   \n",
       "3   fake      21   \n",
       "4   fake      87   \n",
       "5   fake      52   \n",
       "6   fake      39   \n",
       "8   fake     116   \n",
       "9   fake      97   \n",
       "10  fake      78   \n",
       "\n",
       "                                                                                         cleanText  \\\n",
       "0                                                                    kereeen rt shaman eclipse iss   \n",
       "1                                                       absolutely beautiful rt shaman eclipse iss   \n",
       "2                                                shaman eclipse iss eclipse seen space wow amazing   \n",
       "3                                                                                      eclipse iss   \n",
       "4                                 ebonfigli eclipse view list something else divine creation limit   \n",
       "5                                                       ebonfigli eclipse view list something else   \n",
       "6                                                                 eclipse view list something else   \n",
       "8   dit us rt news_executive solar eclipse seen international space station solareclipse iss space   \n",
       "9                      photo solar eclipse seen international space station solareclipse iss space   \n",
       "10                                           planetepics solar eclipse viewed iss cool eclipse iss   \n",
       "\n",
       "    sentimentScore  frequency  hashtagCount  mentionCount  \\\n",
       "0           0.0000          1             0             1   \n",
       "1           0.6361          1             0             1   \n",
       "2           0.8225          1             0             1   \n",
       "3           0.0000          1             0             0   \n",
       "4           0.6908          2             0             1   \n",
       "5           0.0000          1             0             1   \n",
       "6           0.0000          1             0             0   \n",
       "8           0.0000          1             3             1   \n",
       "9           0.0000          1             3             0   \n",
       "10          0.3182          1             0             1   \n",
       "\n",
       "    exclamationMarkCount  numberCount  \n",
       "0                      0            2  \n",
       "1                      1            2  \n",
       "2                      2            5  \n",
       "3                      0            0  \n",
       "4                      0            0  \n",
       "5                      0            0  \n",
       "6                      0            0  \n",
       "8                      0            1  \n",
       "9                      0            0  \n",
       "10                     0            0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extraction of other features - number of #s, number of @s, number of numbers, number of !s\n",
    "def get_symbol_count(text,symbol):\n",
    "    return text.count(symbol)\n",
    "\n",
    "def get_num_numbers(text):\n",
    "    return len(re.sub(\"[^0-9]\", \"\", text))\n",
    "\n",
    "#string = '¬øSe a_cu4erdan de la pel!√≠cula: ‚ÄúEl #d√≠6a despu√©s d7e ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy. http://t.co/JQQeRPwN'\n",
    "#get_symbol_count(string,'!')\n",
    "#print(get_num_numbers(string))\n",
    "\n",
    "test_data['hashtagCount']= test_data['tweetText'].apply(lambda x: get_symbol_count(x,'#'))\n",
    "test_data['mentionCount']= test_data['tweetText'].apply(lambda x: get_symbol_count(x,'@'))\n",
    "test_data['exclamationMarkCount']= test_data['tweetText'].apply(lambda x: get_symbol_count(x,'!'))\n",
    "test_data['numberCount']= test_data['tweetText'].apply(lambda x: get_num_numbers(x))\n",
    "\n",
    "test_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afc0344c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>sentimentScore</th>\n",
       "      <th>frequency</th>\n",
       "      <th>hashtagCount</th>\n",
       "      <th>mentionCount</th>\n",
       "      <th>exclamationMarkCount</th>\n",
       "      <th>numberCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>263046056240115712</td>\n",
       "      <td>¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy.</td>\n",
       "      <td>21226711</td>\n",
       "      <td>sandyA_fake_46</td>\n",
       "      <td>iAnnieM</td>\n",
       "      <td>Mon Oct 29 22:34:01 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>remember movie day tomorrow reminds happening hurricane sandy</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>262995061304852481</td>\n",
       "      <td>@milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\"  REAL! RT.</td>\n",
       "      <td>192378571</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>CarlosVerareal</td>\n",
       "      <td>Mon Oct 29 19:11:23 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "      <td>113</td>\n",
       "      <td>milenagimon look sandy ny tremendous image hurricane looks like independence day real rt</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>262979898002534400</td>\n",
       "      <td>Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy</td>\n",
       "      <td>132303095</td>\n",
       "      <td>sandyA_fake_09</td>\n",
       "      <td>LucasPalape</td>\n",
       "      <td>Mon Oct 29 18:11:08 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>good picture hurricane sandy reminds movie independence day id sandy</td>\n",
       "      <td>0.4404</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>262996108400271360</td>\n",
       "      <td>Scary shit #hurricane #NY</td>\n",
       "      <td>241995902</td>\n",
       "      <td>sandyA_fake_29</td>\n",
       "      <td>Haaaaarryyy</td>\n",
       "      <td>Mon Oct 29 19:15:33 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>scary shit hurricane ny</td>\n",
       "      <td>-0.7783</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>263018881839411200</td>\n",
       "      <td>My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ</td>\n",
       "      <td>250315890</td>\n",
       "      <td>sandyA_fake_15</td>\n",
       "      <td>princess__natt</td>\n",
       "      <td>Mon Oct 29 20:46:02 +0000 2012</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>have place world nyc hurricane sandy statueofliberty</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId  \\\n",
       "0  263046056240115712   \n",
       "1  262995061304852481   \n",
       "2  262979898002534400   \n",
       "3  262996108400271360   \n",
       "4  263018881839411200   \n",
       "\n",
       "                                                                                                            tweetText  \\\n",
       "0  ¬øSe acuerdan de la pel√≠cula: ‚ÄúEl d√≠a despu√©s de ma√±ana‚Äù? Me recuerda a lo que est√° pasando con el hurac√°n #Sandy.    \n",
       "1   @milenagimon: Miren a Sandy en NY!  Tremenda imagen del hurac√°n. Parece el \"D√≠a de la Independencia 2\"  REAL! RT.   \n",
       "2                    Buena la foto del Hurac√°n Sandy, me recuerda a la pel√≠cula D√≠a de la Independencia #ID4 #Sandy     \n",
       "3                                                                                          Scary shit #hurricane #NY    \n",
       "4                                               My fave place in the world #nyc #hurricane #sandy #statueofliberty üóΩ    \n",
       "\n",
       "      userId      imageId(s)        username                       timestamp  \\\n",
       "0   21226711  sandyA_fake_46         iAnnieM  Mon Oct 29 22:34:01 +0000 2012   \n",
       "1  192378571  sandyA_fake_09  CarlosVerareal  Mon Oct 29 19:11:23 +0000 2012   \n",
       "2  132303095  sandyA_fake_09     LucasPalape  Mon Oct 29 18:11:08 +0000 2012   \n",
       "3  241995902  sandyA_fake_29     Haaaaarryyy  Mon Oct 29 19:15:33 +0000 2012   \n",
       "4  250315890  sandyA_fake_15  princess__natt  Mon Oct 29 20:46:02 +0000 2012   \n",
       "\n",
       "   label  length  \\\n",
       "0      0     114   \n",
       "1      0     113   \n",
       "2      0      96   \n",
       "3      0      26   \n",
       "4      0      69   \n",
       "\n",
       "                                                                                  cleanText  \\\n",
       "0                             remember movie day tomorrow reminds happening hurricane sandy   \n",
       "1  milenagimon look sandy ny tremendous image hurricane looks like independence day real rt   \n",
       "2                      good picture hurricane sandy reminds movie independence day id sandy   \n",
       "3                                                                   scary shit hurricane ny   \n",
       "4                                      have place world nyc hurricane sandy statueofliberty   \n",
       "\n",
       "   sentimentScore  frequency  hashtagCount  mentionCount  \\\n",
       "0          0.0000        1.0             1             0   \n",
       "1          0.3612        1.0             0             1   \n",
       "2          0.4404        1.0             2             0   \n",
       "3         -0.7783        1.0             2             0   \n",
       "4          0.0000        1.0             4             0   \n",
       "\n",
       "   exclamationMarkCount  numberCount  \n",
       "0                     0            0  \n",
       "1                     2            1  \n",
       "2                     0            1  \n",
       "3                     0            0  \n",
       "4                     0            0  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert label to 0 or 1\n",
    "def assign_label(label):\n",
    "    if label == 'real':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "train_data['label'] = train_data['label'].apply(lambda x: assign_label(x))\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f8d7627f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweetId</th>\n",
       "      <th>tweetText</th>\n",
       "      <th>userId</th>\n",
       "      <th>imageId(s)</th>\n",
       "      <th>username</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>label</th>\n",
       "      <th>length</th>\n",
       "      <th>cleanText</th>\n",
       "      <th>sentimentScore</th>\n",
       "      <th>frequency</th>\n",
       "      <th>hashtagCount</th>\n",
       "      <th>mentionCount</th>\n",
       "      <th>exclamationMarkCount</th>\n",
       "      <th>numberCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>578854927457349632</td>\n",
       "      <td>kereeen RT @Shyman33: Eclipse from ISS....</td>\n",
       "      <td>70824972</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>peay_s</td>\n",
       "      <td>Fri Mar 20 09:45:43 +0000 2015</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>kereeen rt shaman eclipse iss</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>578874632670953472</td>\n",
       "      <td>Absolutely beautiful! RT @Shyman33: Eclipse from ISS....</td>\n",
       "      <td>344707006</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>JaredUcanChange</td>\n",
       "      <td>Fri Mar 20 11:04:02 +0000 2015</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>absolutely beautiful rt shaman eclipse iss</td>\n",
       "      <td>0.6361</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>578891261353984000</td>\n",
       "      <td>‚Äú@Shyman33: Eclipse from ISS....  Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!</td>\n",
       "      <td>224839607</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>tpjp1231</td>\n",
       "      <td>Fri Mar 20 12:10:06 +0000 2015</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>shaman eclipse iss eclipse seen space wow amazing</td>\n",
       "      <td>0.8225</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>578846612312748032</td>\n",
       "      <td>Eclipse from ISS....</td>\n",
       "      <td>134543073</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Shyman33</td>\n",
       "      <td>Fri Mar 20 09:12:41 +0000 2015</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>eclipse iss</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>578975333841551360</td>\n",
       "      <td>@ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç</td>\n",
       "      <td>1150728872</td>\n",
       "      <td>eclipse_01</td>\n",
       "      <td>Epimethee_</td>\n",
       "      <td>Fri Mar 20 17:44:11 +0000 2015</td>\n",
       "      <td>0</td>\n",
       "      <td>87</td>\n",
       "      <td>ebonfigli eclipse view list something else divine creation limit</td>\n",
       "      <td>0.6908</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              tweetId  \\\n",
       "0  578854927457349632   \n",
       "1  578874632670953472   \n",
       "2  578891261353984000   \n",
       "3  578846612312748032   \n",
       "4  578975333841551360   \n",
       "\n",
       "                                                                                 tweetText  \\\n",
       "0                                              kereeen RT @Shyman33: Eclipse from ISS....    \n",
       "1                                Absolutely beautiful! RT @Shyman33: Eclipse from ISS....    \n",
       "2                            ‚Äú@Shyman33: Eclipse from ISS....  Ïö∞Ï£ºÏóêÏÑúÎ≥∏ 3.20 ÏùºÏãù Wow! amazing!   \n",
       "3                                                                    Eclipse from ISS....    \n",
       "4  @ebonfigli: √âclipse vue de l'ISS... Autre chose...  cr√©ation divine n'a pas de limite üòç   \n",
       "\n",
       "       userId   imageId(s)         username                       timestamp  \\\n",
       "0    70824972  eclipse_01            peay_s  Fri Mar 20 09:45:43 +0000 2015   \n",
       "1   344707006  eclipse_01   JaredUcanChange  Fri Mar 20 11:04:02 +0000 2015   \n",
       "2   224839607  eclipse_01          tpjp1231  Fri Mar 20 12:10:06 +0000 2015   \n",
       "3   134543073  eclipse_01          Shyman33  Fri Mar 20 09:12:41 +0000 2015   \n",
       "4  1150728872   eclipse_01       Epimethee_  Fri Mar 20 17:44:11 +0000 2015   \n",
       "\n",
       "   label  length  \\\n",
       "0      0      43   \n",
       "1      0      57   \n",
       "2      0      61   \n",
       "3      0      21   \n",
       "4      0      87   \n",
       "\n",
       "                                                          cleanText  \\\n",
       "0                                     kereeen rt shaman eclipse iss   \n",
       "1                        absolutely beautiful rt shaman eclipse iss   \n",
       "2                 shaman eclipse iss eclipse seen space wow amazing   \n",
       "3                                                       eclipse iss   \n",
       "4  ebonfigli eclipse view list something else divine creation limit   \n",
       "\n",
       "   sentimentScore  frequency  hashtagCount  mentionCount  \\\n",
       "0          0.0000          1             0             1   \n",
       "1          0.6361          1             0             1   \n",
       "2          0.8225          1             0             1   \n",
       "3          0.0000          1             0             0   \n",
       "4          0.6908          2             0             1   \n",
       "\n",
       "   exclamationMarkCount  numberCount  \n",
       "0                     0            2  \n",
       "1                     1            2  \n",
       "2                     2            5  \n",
       "3                     0            0  \n",
       "4                     0            0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert label to 0 or 1\n",
    "def assign_label(label):\n",
    "    if label == 'real':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "test_data['label'] = test_data['label'].apply(lambda x: assign_label(x))\n",
    "test_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3765d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c93923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d6d57b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6996996996996997\n"
     ]
    }
   ],
   "source": [
    "# initialise model and vectorizers\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "\n",
    "#basic svc classifier\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "svc_model = pipeline.fit(train_data['cleanText'],train_data['label'])\n",
    "svc_pred = svc_model.predict(test_data['cleanText'])\n",
    "svc_f1 = f1_score(test_data['label'], svc_pred, average=\"micro\")\n",
    "print(svc_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b3be2065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7932932932932933\n"
     ]
    }
   ],
   "source": [
    "#naive bayes\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "nb_model = pipeline.fit(train_data['cleanText'], train_data['label'])\n",
    "nb_pred = nb_model.predict(test_data['cleanText'])\n",
    "nb_f1 = f1_score(test_data['label'], nb_pred, average=\"micro\")\n",
    "print(nb_f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bbaec139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7127127127127126\n",
      "0.7132132132132132\n",
      "0.7017017017017017\n",
      "0.6796796796796797\n",
      "0.7137137137137137\n",
      "0.7127127127127126\n",
      "0.7127127127127126\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "#different gamma values for rbf kernel\n",
    "gammas = [0.001,0.01,0.1, 1, 10, 100]\n",
    "for gamma in gammas:\n",
    "    svc = svm.SVC(kernel='rbf', gamma=gamma)\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', svc)\n",
    "    ])\n",
    "\n",
    "\n",
    "    rbf_model = pipeline.fit(train_data['cleanText'], train_data['label'])\n",
    "    rbf_pred = rbf_model.predict(test_data['cleanText'])\n",
    "    rbf_f1 = f1_score(test_data['label'], rbf_pred, average=\"micro\")\n",
    "    print(rbf_f1)\n",
    "\n",
    "\n",
    "#using different variations of svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "69510631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C value of 0.1 0.7132132132132132\n",
      "C value of 1 0.6796796796796797\n",
      "C value of 10 0.7357357357357357\n",
      "C value of 100 0.7332332332332332\n",
      "C value of 1000 0.7332332332332332\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "#c values for rbf kernel\n",
    "C = [0.1, 1, 10, 100, 1000]\n",
    "for c in C:\n",
    "    svc = svm.SVC(kernel='rbf', C=c)\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', svc)\n",
    "    ])\n",
    "\n",
    "\n",
    "    rbf_model = pipeline.fit(train_data['cleanText'], train_data['label'])\n",
    "    rbf_pred = rbf_model.predict(test_data['cleanText'])\n",
    "    rbf_f1 = f1_score(test_data['label'], rbf_pred, average=\"micro\")\n",
    "    print('C value of ' + str(c) + ' ' + str(rbf_f1))\n",
    "\n",
    "\n",
    "#using different variations of svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "325881af",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-71-6d7b14aa0fc4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleanText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mgrid_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleanText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mgrid_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"micro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1286\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1288\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1290\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1044\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1045\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    857\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    775\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 777\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    778\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    220\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 222\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    591\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m             \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m'passthrough'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'i'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[1;31m# see comment on the other call to np.iinfo in this file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_sparse_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    299\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_support\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_probA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_probB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_status_\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m             libsvm_sparse.libsvm_sparse_train(\n\u001b[0m\u001b[0;32m    302\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindptr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msolver_type\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m                 \u001b[0mkernel_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdegree\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32msklearn\\svm\\_libsvm_sparse.pyx\u001b[0m in \u001b[0;36msklearn.svm._libsvm_sparse.libsvm_sparse_train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\compressed.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;34m\"\"\"base matrix class for compressed row- and column-oriented matrices\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0m_data_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#grid search for rbf\n",
    "# defining parameter range\n",
    "params={'clf__C':[0.1, 1, 10, 100, 1000],\n",
    "        'clf__gamma':[10,5,1, 0.1, 0.01, 0.001, 0.0001]}\n",
    "\n",
    "svc = svm.SVC(kernel='rbf')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', svc)\n",
    "])\n",
    "\n",
    "#grid = GridSearchCV(pipeline, cv=10,param_grid=params)\n",
    "#grid.fit(train_data['cleanText'], train_data['label'])\n",
    "#grid_pred = grid.predict(test_data['cleanText'])\n",
    "#grid_f1 = f1_score(test_data['label'], grid_pred, average=\"micro\")\n",
    "#print('grid search for rbf: ' + grid_f1)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "055d2fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7357357357357357\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#best performing rbf kernel SVM\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    " \n",
    "svc = svm.SVC(kernel='rbf',C=10,gamma=1)\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', svc)\n",
    "])\n",
    "\n",
    "best_rbf_model = pipeline.fit(train_data['cleanText'], train_data['label'])\n",
    "best_rbf_pred = best_rbf_model.predict(test_data['cleanText'])\n",
    "best_rbf_f1 = f1_score(test_data['label'], best_rbf_pred, average=\"micro\")\n",
    "print(best_rbf_f1)\n",
    "plot_confusion_matrix(best_rbf_model, test_data['cleanText'], test_data['label'])\n",
    "plt.title(\"SVM RBF Kernel\")\n",
    "\n",
    "plt.savefig('svmrbf.png')\n",
    "\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "caba00eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-120-1b625a391070>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m     pipe = Pipeline([\n\u001b[0;32m     21\u001b[0m                       \u001b[1;33m(\u001b[0m\u001b[1;34m'tfidf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_transformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m                       \u001b[1;33m(\u001b[0m\u001b[1;34m'classify'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m                     ])\n\u001b[0;32m     24\u001b[0m     \u001b[0mboth_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "#different poly kernel degrees for cleanText and length\n",
    "degrees = [0, 1, 2, 3, 4, 5, 6]\n",
    "for degree in degrees:\n",
    "    # Set X and y\n",
    "    X = train_data[['cleanText', 'length']]\n",
    "    y = train_data['label']\n",
    "\n",
    "    # initialise model and vectorizers\n",
    "    svc = svm.SVC(kernel='poly', degree=degree)\n",
    "    vectorizer1 = TfidfTransformer()\n",
    "\n",
    "    # construct the column transfomer\n",
    "    column_transformer = ColumnTransformer(\n",
    "        [('tfidf1', vectorizer1, 'cleanText'),remainder='passthrough')])\n",
    "\n",
    "    # fit the model\n",
    "    pipe = Pipeline([\n",
    "                      ('tfidf', column_transformer),\n",
    "                      ('classify', model)\n",
    "                    ])\n",
    "    both_model = pipe.fit(X,y)\n",
    "    both_pred = both_model.predict(test_data[['cleanText', 'length']])\n",
    "    both_f1 = f1_score(test_data['label'], both_pred, average=\"micro\")\n",
    "    print('degree = ' + str(degree) + ' f1 = ' + str(both_f1))\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8894e2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "degree = 0 f1 = 0.7127127127127126\n",
      "degree = 1 f1 = 0.7417417417417418\n",
      "degree = 2 f1 = 0.6711711711711712\n",
      "degree = 3 f1 = 0.7052052052052052\n",
      "degree = 4 f1 = 0.7117117117117117\n",
      "degree = 5 f1 = 0.7127127127127126\n",
      "degree = 6 f1 = 0.7127127127127126\n"
     ]
    }
   ],
   "source": [
    "#different poly kernel degrees\n",
    "degrees = [0, 1, 2, 3, 4, 5, 6]\n",
    "for degree in degrees:\n",
    "    svc = svm.SVC(kernel='poly', degree=degree)\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', svc)\n",
    "    ])\n",
    "\n",
    "\n",
    "    poly_model = pipeline.fit(train_data['cleanText'], train_data['label'])\n",
    "    poly_pred = poly_model.predict(test_data['cleanText'])\n",
    "    poly_f1 = f1_score(test_data['label'], poly_pred, average=\"micro\")\n",
    "    print('degree = ' + str(degree) + ' f1 = ' + str(poly_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8adfcc68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gamma value of 0.01 0.7127127127127126\n",
      "Gamma value of 0.8 0.7332332332332332\n",
      "Gamma value of 1 0.7412412412412412\n",
      "Gamma value of 1.5 0.7262262262262263\n",
      "Gamma value of 1.2 0.7382382382382382\n",
      "Gamma value of 2 0.7162162162162162\n"
     ]
    }
   ],
   "source": [
    "#different gamma values for poly kernel\n",
    "gammas = [0.01,0.8, 1,1.5,1.2 ,2]\n",
    "\n",
    "for gamma in gammas:\n",
    "    svc = svm.SVC(kernel='poly',degree=1, gamma=gamma)\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', svc)\n",
    "    ])\n",
    "\n",
    "\n",
    "    poly_model = pipeline.fit(train_data['cleanText'], train_data['label'])\n",
    "    poly_pred = poly_model.predict(test_data['cleanText'])\n",
    "    poly_f1 = f1_score(test_data['label'], poly_pred, average=\"micro\")\n",
    "    print('Gamma value of ' + str(gamma) + ' ' + str(poly_f1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "da4f2e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7417417417417418\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='poly',degree=1)\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', svc)\n",
    "])\n",
    "\n",
    "\n",
    "poly_model = pipeline.fit(train_data['cleanText'], train_data['label'])\n",
    "poly_pred = poly_model.predict(test_data['cleanText'])\n",
    "poly_f1 = f1_score(test_data['label'], poly_pred, average=\"micro\")\n",
    "print(poly_f1)\n",
    "plot_confusion_matrix(poly_model, test_data['cleanText'], test_data['label'])\n",
    "plt.title(\"SVM Polynomial\")\n",
    "\n",
    "plt.savefig('svmpoly.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "92ce366e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C value of 0.1 0.7102102102102102\n",
      "C value of 1 0.7412412412412412\n",
      "C value of 10 0.44844844844844844\n",
      "C value of 100 0.6306306306306306\n",
      "C value of 1000 0.6271271271271271\n"
     ]
    }
   ],
   "source": [
    "#different C values for linear\n",
    "C = [0.1, 1, 10, 100, 1000]\n",
    "for c in C:\n",
    "    svc = svm.SVC(kernel='linear', C=c)\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', svc)\n",
    "    ])\n",
    "\n",
    "\n",
    "    rbf_model = pipeline.fit(train_data['cleanText'], train_data['label'])\n",
    "    rbf_pred = rbf_model.predict(test_data['cleanText'])\n",
    "    rbf_f1 = f1_score(test_data['label'], rbf_pred, average=\"micro\")\n",
    "    print('C value of ' + str(c) + ' ' + str(rbf_f1))\n",
    "\n",
    "#untuned poly = 0.7052052052052052\n",
    "#untuned linear = 0.7412412412412412\n",
    "#untuned rbf = 0.6796796796796797\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0ae90347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7412412412412412\n",
      "0.7412412412412412\n",
      "0.7412412412412412\n",
      "0.7412412412412412\n",
      "0.7412412412412412\n",
      "0.7412412412412412\n"
     ]
    }
   ],
   "source": [
    "#different C values for linear\n",
    "gammas = [0.001,0.01,0.1, 1, 10, 100]\n",
    "for gamma in gammas:\n",
    "    svc = svm.SVC(kernel='linear', gamma=gamma)\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', svc)\n",
    "    ])\n",
    "\n",
    "\n",
    "    rbf_model = pipeline.fit(train_data['cleanText'], train_data['label'])\n",
    "    rbf_pred = rbf_model.predict(test_data['cleanText'])\n",
    "    rbf_f1 = f1_score(test_data['label'], rbf_pred, average=\"micro\")\n",
    "    print(rbf_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a03ade65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:610: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 593, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 341, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 303, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\joblib\\memory.py\", line 352, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 754, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1850, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1203, in fit_transform\n",
      "    vocabulary, X = self._count_vocab(raw_documents,\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1115, in _count_vocab\n",
      "    for feature in analyze(doc):\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 104, in _analyze\n",
      "    doc = preprocessor(doc)\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 69, in _preprocess\n",
      "    doc = doc.lower()\n",
      "  File \"C:\\Users\\44784\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\", line 687, in __getattr__\n",
      "    raise AttributeError(attr + \" not found\")\n",
      "AttributeError: lower not found\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\Users\\44784\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-101-9604d7ed82fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleanText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mgrid_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleanText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mgrid_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"micro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    339\u001b[0m         \"\"\"\n\u001b[0;32m    340\u001b[0m         \u001b[0mfit_params_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_fit_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 341\u001b[1;33m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    342\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    343\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    301\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[1;31m# Fit or load from cache the current transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[0;32m    304\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    752\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1848\u001b[0m         \"\"\"\n\u001b[0;32m   1849\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1850\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1851\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1201\u001b[0m         \u001b[0mmax_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[0m\u001b[0;32m   1204\u001b[0m                                           self.fixed_vocabulary_)\n\u001b[0;32m   1205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1115\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1116\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    102\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     67\u001b[0m     \"\"\"\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    685\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    686\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 687\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    689\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    " \n",
    "# defining parameter range\n",
    "params={'tfidf__max_df': [0.25, 0.5, 0.75],\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)]}\n",
    "\n",
    "svc = svm.SVC(kernel='linear')\n",
    "\n",
    "\n",
    "pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', svc)\n",
    "])\n",
    "\n",
    "grid = GridSearchCV(pipeline,param_grid=params)\n",
    "grid.fit(train_data['cleanText'], train_data['label'])\n",
    "grid_pred = grid.predict(test_data['cleanText'])\n",
    "grid_f1 = f1_score(test_data['label'], grid_pred, average=\"micro\")\n",
    "print('grid search for linear: ' + grid_f1)\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "216d4d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7412412412412412\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "svc = svm.SVC(kernel='linear', gamma=gamma)\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', svc)\n",
    "])\n",
    "\n",
    "\n",
    "lin_model = pipeline.fit(train_data['cleanText'], train_data['label'])\n",
    "lin_pred = lin_model.predict(test_data['cleanText'])\n",
    "lin_f1 = f1_score(test_data['label'], lin_pred, average=\"micro\")\n",
    "print(lin_f1)\n",
    "plot_confusion_matrix(lin_model, test_data['cleanText'], test_data['label'])\n",
    "plt.title(\"SVM Linear\")\n",
    "\n",
    "plt.savefig('svmlinear.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "0d633335",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "predict_proba is not available when  probability=False",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-127-0637e9ef42f5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mvoting_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmixed_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleanText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m \u001b[0mvoting_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvoting_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleanText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[0mvoting_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoting_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"micro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvoting_f1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    418\u001b[0m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 419\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    421\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'_final_estimator'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    308\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvoting\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'soft'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 309\u001b[1;33m             \u001b[0mmaj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# 'hard' voting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\u001b[0m in \u001b[0;36m_predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    327\u001b[0m         \u001b[1;34m\"\"\"Predict class probabilities for X in 'soft' voting.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         avg = np.average(self._collect_probas(X), axis=0,\n\u001b[0m\u001b[0;32m    330\u001b[0m                          weights=self._weights_not_none)\n\u001b[0;32m    331\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mavg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\u001b[0m in \u001b[0;36m_collect_probas\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_collect_probas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;34m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_predict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_voting.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_collect_probas\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[1;34m\"\"\"Collect results from clf.predict calls.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 324\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_predict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m__get__\u001b[1;34m(self, obj, type)\u001b[0m\n\u001b[0;32m    112\u001b[0m                     \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m                     \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattribute_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \"\"\"\n\u001b[1;32m--> 666\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    667\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_predict_proba\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\u001b[0m in \u001b[0;36m_check_proba\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 633\u001b[1;33m             raise AttributeError(\"predict_proba is not available when \"\n\u001b[0m\u001b[0;32m    634\u001b[0m                                  \" probability=False\")\n\u001b[0;32m    635\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_impl\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'c_svc'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'nu_svc'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: predict_proba is not available when  probability=False"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#naive bayes\n",
    "naive_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "svc = svm.SVC(kernel='linear', gamma=gamma)\n",
    "svc_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', svc)\n",
    "])\n",
    "\n",
    "random_pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "classifiers = [\n",
    "    (\"svm\", svc_pipeline),\n",
    "    (\"naive\", naive_pipeline),\n",
    "    (\"rand\",random_pipeline),\n",
    "]\n",
    " \n",
    "mixed_pipe = Pipeline([\n",
    "    (\"voting\", VotingClassifier(classifiers, voting=\"soft\"))\n",
    "])\n",
    "\n",
    "voting_model = mixed_pipe.fit(train_data['cleanText'], train_data['label'])\n",
    "voting_pred = voting_model.predict(test_data['cleanText'])\n",
    "voting_f1 = f1_score(test_data['label'], voting_pred, average=\"micro\")\n",
    "print(voting_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "78eee328",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-129-1d5e0d33bc4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mnb_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleanText'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'imageId(s)'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mnb_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnb_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cleanText'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'imageId(s)'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mnb_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnb_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"micro\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnb_f1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[1;31m# update the docstring of the returned function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m             \u001b[0mXt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfixed_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1256\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1115\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1116\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m             raise ValueError(\"np.nan is an invalid document, expected byte or \"\n\u001b[0m\u001b[0;32m    218\u001b[0m                              \"unicode string.\")\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "#naive bayes\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "nb_model = pipeline.fit(train_data['cleanText'] + ' ' + train_data['imageId(s)'], train_data['label'])\n",
    "nb_pred = nb_model.predict(test_data['cleanText'] + ' ' + train_data['imageId(s)'])\n",
    "nb_f1 = f1_score(test_data['label'], nb_pred, average=\"micro\")\n",
    "print(nb_f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "a4d0910c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7067067067067067\n"
     ]
    }
   ],
   "source": [
    "#random forest\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier())\n",
    "])\n",
    "\n",
    "\n",
    "rf_model = pipeline.fit(train_data['cleanText'], train_data['label'])\n",
    "rf_pred = rf_model.predict(test_data['cleanText'])\n",
    "rf_f1 = f1_score(test_data['label'], rf_pred, average=\"micro\")\n",
    "print(rf_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c8be45b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6996996996996997\n"
     ]
    }
   ],
   "source": [
    "#linear svc\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LinearSVC())\n",
    "])\n",
    "\n",
    "\n",
    "lin_svc_model = pipeline.fit(train_data['cleanText'], train_data['label'])\n",
    "lin_svc_pred = lin_svc_model.predict(test_data['cleanText'])\n",
    "lin_svc_f1 = f1_score(test_data['label'], lin_svc_pred, average=\"micro\")\n",
    "print(lin_svc_f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10358da6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
